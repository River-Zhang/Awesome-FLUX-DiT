[
  {
    "title": "Turbo3D: Ultra-fast Text-to-3D Generation",
    "authors": [
      "Hanzhe Hu",
      "Tianwei Yin",
      "Fujun Luan",
      "Yiwei Hu",
      "Hao Tan",
      "Zexiang Xu",
      "Sai Bi",
      "Shubham Tulsiani",
      "Kai Zhang"
    ],
    "abstract": "We present Turbo3D, an ultra-fast text-to-3D system capable of generating high-quality Gaussian splatting assets in under one second. Turbo3D employs a rapid 4-step, 4-view diffusion generator and an efficient feed-forward Gaussian reconstructor, both operating in latent space. The 4-step, 4-view generator is a student model distilled through a novel Dual-Teacher approach, which encourages the student to learn view consistency from a multi-view teacher and photo-realism from a single-view teacher. By shifting the Gaussian reconstructor's inputs from pixel space to latent space, we eliminate the extra image decoding time and halve the transformer sequence length for maximum efficiency. Our method demonstrates superior 3D generation results compared to previous baselines, while operating in a fraction of their runtime.",
    "arxiv_url": "http://arxiv.org/abs/2412.04470v1",
    "pdf_url": "http://arxiv.org/pdf/2412.04470v1",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos",
    "authors": [
      "Sharath Girish",
      "Tianye Li",
      "Amrita Mazumdar",
      "Abhinav Shrivastava",
      "David Luebke",
      "Shalini De Mello"
    ],
    "abstract": "Online free-viewpoint video (FVV) streaming is a challenging problem, which is relatively under-explored. It requires incremental on-the-fly updates to a volumetric representation, fast training and rendering to satisfy real-time constraints and a small memory footprint for efficient transmission. If achieved, it can enhance user experience by enabling novel applications, e.g., 3D video conferencing and live volumetric video broadcast, among others. In this work, we propose a novel framework for QUantized and Efficient ENcoding (QUEEN) for streaming FVV using 3D Gaussian Splatting (3D-GS). QUEEN directly learns Gaussian attribute residuals between consecutive frames at each time-step without imposing any structural constraints on them, allowing for high quality reconstruction and generalizability. To efficiently store the residuals, we further propose a quantization-sparsity framework, which contains a learned latent-decoder for effectively quantizing attribute residuals other than Gaussian positions and a learned gating module to sparsify position residuals. We propose to use the Gaussian viewspace gradient difference vector as a signal to separate the static and dynamic content of the scene. It acts as a guide for effective sparsity learning and speeds up training. On diverse FVV benchmarks, QUEEN outperforms the state-of-the-art online FVV methods on all metrics. Notably, for several highly dynamic scenes, it reduces the model size to just 0.7 MB per frame while training in under 5 sec and rendering at 350 FPS. Project website is at https://research.nvidia.com/labs/amri/projects/queen",
    "arxiv_url": "http://arxiv.org/abs/2412.04469v1",
    "pdf_url": "http://arxiv.org/pdf/2412.04469v1",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "gaussian splatting",
      "efficient",
      "ar",
      "high quality",
      "3d gaussian",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering",
    "authors": [
      "Cheng Sun",
      "Jaesung Choe",
      "Charles Loop",
      "Wei-Chiu Ma",
      "Yu-Chiang Frank Wang"
    ],
    "abstract": "We propose an efficient radiance field rendering algorithm that incorporates a rasterization process on sparse voxels without neural networks or 3D Gaussians. There are two key contributions coupled with the proposed system. The first is to render sparse voxels in the correct depth order along pixel rays by using dynamic Morton ordering. This avoids the well-known popping artifact found in Gaussian splatting. Second, we adaptively fit sparse voxels to different levels of detail within scenes, faithfully reproducing scene details while achieving high rendering frame rates. Our method improves the previous neural-free voxel grid representation by over 4db PSNR and more than 10x rendering FPS speedup, achieving state-of-the-art comparable novel-view synthesis results. Additionally, our neural-free sparse voxels are seamlessly compatible with grid-based 3D processing algorithms. We achieve promising mesh reconstruction accuracy by integrating TSDF-Fusion and Marching Cubes into our sparse grid system.",
    "arxiv_url": "http://arxiv.org/abs/2412.04459v1",
    "pdf_url": "http://arxiv.org/pdf/2412.04459v1",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "efficient",
      "ar",
      "3d gaussian",
      "high-fidelity",
      "4d",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Monocular Dynamic Gaussian Splatting is Fast and Brittle but Smooth Motion Helps",
    "authors": [
      "Yiqing Liang",
      "Mikhail Okunev",
      "Mikaela Angelina Uy",
      "Runfeng Li",
      "Leonidas Guibas",
      "James Tompkin",
      "Adam W. Harley"
    ],
    "abstract": "Gaussian splatting methods are emerging as a popular approach for converting multi-view image data into scene representations that allow view synthesis. In particular, there is interest in enabling view synthesis for dynamic scenes using only monocular input data -- an ill-posed and challenging problem. The fast pace of work in this area has produced multiple simultaneous papers that claim to work best, which cannot all be true. In this work, we organize, benchmark, and analyze many Gaussian-splatting-based methods, providing apples-to-apples comparisons that prior works have lacked. We use multiple existing datasets and a new instructive synthetic dataset designed to isolate factors that affect reconstruction quality. We systematically categorize Gaussian splatting methods into specific motion representation types and quantify how their differences impact performance. Empirically, we find that their rank order is well-defined in synthetic data, but the complexity of real-world data currently overwhelms the differences. Furthermore, the fast rendering speed of all Gaussian-based methods comes at the cost of brittleness in optimization. We summarize our experiments into a list of findings that can help to further progress in this lively problem setting. Project Webpage: https://lynl7130.github.io/MonoDyGauBench.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2412.04457v1",
    "pdf_url": "http://arxiv.org/pdf/2412.04457v1",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "gaussian splatting",
      "motion",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human Avatars",
    "authors": [
      "Shota Sasaki",
      "Jane Wu",
      "Ko Nishino"
    ],
    "abstract": "This paper introduces a novel clothed human model that can be learned from multiview RGB videos, with a particular emphasis on recovering physically accurate body and cloth movements. Our method, Position Based Dynamic Gaussians (PBDyG), realizes ``movement-dependent'' cloth deformation via physical simulation, rather than merely relying on ``pose-dependent'' rigid transformations. We model the clothed human holistically but with two distinct physical entities in contact: clothing modeled as 3D Gaussians, which are attached to a skinned SMPL body that follows the movement of the person in the input videos. The articulation of the SMPL body also drives physically-based simulation of the clothes' Gaussians to transform the avatar to novel poses. In order to run position based dynamics simulation, physical properties including mass and material stiffness are estimated from the RGB videos through Dynamic 3D Gaussian Splatting. Experiments demonstrate that our method not only accurately reproduces appearance but also enables the reconstruction of avatars wearing highly deformable garments, such as skirts or coats, which have been challenging to reconstruct using existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2412.04433v1",
    "pdf_url": "http://arxiv.org/pdf/2412.04433v1",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "motion",
      "human",
      "ar",
      "3d gaussian",
      "deformation",
      "avatar",
      "body",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding",
    "authors": [
      "Yuqi Wu",
      "Wenzhao Zheng",
      "Sicheng Zuo",
      "Yuanhui Huang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "3D occupancy prediction provides a comprehensive description of the surrounding scenes and has become an essential task for 3D perception. Most existing methods focus on offline perception from one or a few views and cannot be applied to embodied agents which demands to gradually perceive the scene through progressive embodied exploration. In this paper, we formulate an embodied 3D occupancy prediction task to target this practical scenario and propose a Gaussian-based EmbodiedOcc framework to accomplish it. We initialize the global scene with uniform 3D semantic Gaussians and progressively update local regions observed by the embodied agent. For each update, we extract semantic and structural features from the observed image and efficiently incorporate them via deformable cross-attention to refine the regional Gaussians. Finally, we employ Gaussian-to-voxel splatting to obtain the global 3D occupancy from the updated 3D Gaussians. Our EmbodiedOcc assumes an unknown (i.e., uniformly distributed) environment and maintains an explicit global memory of it with 3D Gaussians. It gradually gains knowledge through local refinement of regional Gaussians, which is consistent with how humans understand new scenes through embodied exploration. We reorganize an EmbodiedOcc-ScanNet benchmark based on local annotations to facilitate the evaluation of the embodied 3D occupancy prediction task. Experiments demonstrate that our EmbodiedOcc outperforms existing local prediction methods and accomplishes the embodied occupancy prediction with high accuracy and strong expandability. Our code is available at: https://github.com/YkiWu/EmbodiedOcc.",
    "arxiv_url": "http://arxiv.org/abs/2412.04380v1",
    "pdf_url": "http://arxiv.org/pdf/2412.04380v1",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "https://github.com/YkiWu/EmbodiedOcc",
    "keywords": [
      "human",
      "efficient",
      "ar",
      "3d gaussian",
      "semantic",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models",
    "authors": [
      "Yifan Lu",
      "Xuanchi Ren",
      "Jiawei Yang",
      "Tianchang Shen",
      "Zhangjie Wu",
      "Jun Gao",
      "Yue Wang",
      "Siheng Chen",
      "Mike Chen",
      "Sanja Fidler",
      "Jiahui Huang"
    ],
    "abstract": "We present InfiniCube, a scalable method for generating unbounded dynamic 3D driving scenes with high fidelity and controllability. Previous methods for scene generation either suffer from limited scales or lack geometric and appearance consistency along generated sequences. In contrast, we leverage the recent advancements in scalable 3D representation and video models to achieve large dynamic scene generation that allows flexible controls through HD maps, vehicle bounding boxes, and text descriptions. First, we construct a map-conditioned sparse-voxel-based 3D generative model to unleash its power for unbounded voxel world generation. Then, we re-purpose a video model and ground it on the voxel world through a set of carefully designed pixel-aligned guidance buffers, synthesizing a consistent appearance. Finally, we propose a fast feed-forward approach that employs both voxel and pixel branches to lift the dynamic videos to dynamic 3D Gaussians with controllable objects. Our method can generate controllable and realistic 3D driving scenes, and extensive experiments validate the effectiveness and superiority of our model.",
    "arxiv_url": "http://arxiv.org/abs/2412.03934v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03934v1",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "3d gaussian",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-View Pose-Agnostic Change Localization with Zero Labels",
    "authors": [
      "Chamuditha Jayanga Galappaththige",
      "Jason Lai",
      "Lloyd Windrim",
      "Donald Dansereau",
      "Niko Suenderhauf",
      "Dimity Miller"
    ],
    "abstract": "Autonomous agents often require accurate methods for detecting and localizing changes in their environment, particularly when observations are captured from unconstrained and inconsistent viewpoints. We propose a novel label-free, pose-agnostic change detection method that integrates information from multiple viewpoints to construct a change-aware 3D Gaussian Splatting (3DGS) representation of the scene. With as few as 5 images of the post-change scene, our approach can learn additional change channels in a 3DGS and produce change masks that outperform single-view techniques. Our change-aware 3D scene representation additionally enables the generation of accurate change masks for unseen viewpoints. Experimental results demonstrate state-of-the-art performance in complex multi-object scenes, achieving a 1.7$\\times$ and 1.6$\\times$ improvement in Mean Intersection Over Union and F1 score respectively over other baselines. We also contribute a new real-world dataset to benchmark change detection in diverse challenging scenes in the presence of lighting variations.",
    "arxiv_url": "http://arxiv.org/abs/2412.03911v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03911v1",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "localization",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for Monocular Dynamic 3D Reconstruction",
    "authors": [
      "Xuesong Li",
      "Jinguang Tong",
      "Jie Hong",
      "Vivien Rolland",
      "Lars Petersson"
    ],
    "abstract": "Dynamic scene reconstruction from monocular video is critical for real-world applications. This paper tackles the dual challenges of dynamic novel-view synthesis and 3D geometry reconstruction by introducing a hybrid framework: Deformable Gaussian Splatting and Dynamic Neural Surfaces (DGNS), in which both modules can leverage each other for both tasks. During training, depth maps generated by the deformable Gaussian splatting module guide the ray sampling for faster processing and provide depth supervision within the dynamic neural surface module to improve geometry reconstruction. Simultaneously, the dynamic neural surface directs the distribution of Gaussian primitives around the surface, enhancing rendering quality. To further refine depth supervision, we introduce a depth-filtering process on depth maps derived from Gaussian rasterization. Extensive experiments on public datasets demonstrate that DGNS achieves state-of-the-art performance in both novel-view synthesis and 3D reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2412.03910v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03910v1",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "gaussian splatting",
      "geometry",
      "ar",
      "3d reconstruction",
      "face",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting",
    "authors": [
      "Jingyu Lin",
      "Jiaqi Gu",
      "Lubin Fan",
      "Bojian Wu",
      "Yujing Lou",
      "Renjie Chen",
      "Ligang Liu",
      "Jieping Ye"
    ],
    "abstract": "Generating high-quality novel view renderings of 3D Gaussian Splatting (3DGS) in scenes featuring transient objects is challenging. We propose a novel hybrid representation, termed as HybridGS, using 2D Gaussians for transient objects per image and maintaining traditional 3D Gaussians for the whole static scenes. Note that, the 3DGS itself is better suited for modeling static scenes that assume multi-view consistency, but the transient objects appear occasionally and do not adhere to the assumption, thus we model them as planar objects from a single view, represented with 2D Gaussians. Our novel representation decomposes the scene from the perspective of fundamental viewpoint consistency, making it more reasonable. Additionally, we present a novel multi-view regulated supervision method for 3DGS that leverages information from co-visible regions, further enhancing the distinctions between the transients and statics. Then, we propose a straightforward yet effective multi-stage training strategy to ensure robust training and high-quality view synthesis across various settings. Experiments on benchmark datasets show our state-of-the-art performance of novel view synthesis in both indoor and outdoor scenes, even in the presence of distracting elements.",
    "arxiv_url": "http://arxiv.org/abs/2412.03844v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03844v1",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "outdoor",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos",
    "authors": [
      "Hanxue Liang",
      "Jiawei Ren",
      "Ashkan Mirzaei",
      "Antonio Torralba",
      "Ziwei Liu",
      "Igor Gilitschenski",
      "Sanja Fidler",
      "Cengiz Oztireli",
      "Huan Ling",
      "Zan Gojcic",
      "Jiahui Huang"
    ],
    "abstract": "Recent advancements in static feed-forward scene reconstruction have demonstrated significant progress in high-quality novel view synthesis. However, these models often struggle with generalizability across diverse environments and fail to effectively handle dynamic content. We present BTimer (short for BulletTimer), the first motion-aware feed-forward model for real-time reconstruction and novel view synthesis of dynamic scenes. Our approach reconstructs the full scene in a 3D Gaussian Splatting representation at a given target ('bullet') timestamp by aggregating information from all the context frames. Such a formulation allows BTimer to gain scalability and generalization by leveraging both static and dynamic scene datasets. Given a casual monocular dynamic video, BTimer reconstructs a bullet-time scene within 150ms while reaching state-of-the-art performance on both static and dynamic scene datasets, even compared with optimization-based approaches.",
    "arxiv_url": "http://arxiv.org/abs/2412.03526v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03526v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "motion",
      "ar",
      "3d gaussian",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dense Scene Reconstruction from Light-Field Images Affected by Rolling Shutter",
    "authors": [
      "Hermes McGriff",
      "Renato Martins",
      "Nicolas Andreff",
      "Cedric Demonceaux"
    ],
    "abstract": "This paper presents a dense depth estimation approach from light-field (LF) images that is able to compensate for strong rolling shutter (RS) effects. Our method estimates RS compensated views and dense RS compensated disparity maps. We present a two-stage method based on a 2D Gaussians Splatting that allows for a ``render and compare\" strategy with a point cloud formulation. In the first stage, a subset of sub-aperture images is used to estimate an RS agnostic 3D shape that is related to the scene target shape ``up to a motion\". In the second stage, the deformation of the 3D shape is computed by estimating an admissible camera motion. We demonstrate the effectiveness and advantages of this approach through several experiments conducted for different scenes and types of motions. Due to lack of suitable datasets for evaluation, we also present a new carefully designed synthetic dataset of RS LF images. The source code, trained models and dataset will be made publicly available at: https://github.com/ICB-Vision-AI/DenseRSLF",
    "arxiv_url": "http://arxiv.org/abs/2412.03518v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03518v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/ICB-Vision-AI/DenseRSLF",
    "keywords": [
      "deformation",
      "motion",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Urban4D: Semantic-Guided 4D Gaussian Splatting for Urban Scene Reconstruction",
    "authors": [
      "Ziwen Li",
      "Jiaxin Huang",
      "Runnan Chen",
      "Yunlong Che",
      "Yandong Guo",
      "Tongliang Liu",
      "Fakhri Karray",
      "Mingming Gong"
    ],
    "abstract": "Reconstructing dynamic urban scenes presents significant challenges due to their intrinsic geometric structures and spatiotemporal dynamics. Existing methods that attempt to model dynamic urban scenes without leveraging priors on potentially moving regions often produce suboptimal results. Meanwhile, approaches based on manual 3D annotations yield improved reconstruction quality but are impractical due to labor-intensive labeling. In this paper, we revisit the potential of 2D semantic maps for classifying dynamic and static Gaussians and integrating spatial and temporal dimensions for urban scene representation. We introduce Urban4D, a novel framework that employs a semantic-guided decomposition strategy inspired by advances in deep 2D semantic map generation. Our approach distinguishes potentially dynamic objects through reliable semantic Gaussians. To explicitly model dynamic objects, we propose an intuitive and effective 4D Gaussian splatting (4DGS) representation that aggregates temporal information through learnable time embeddings for each Gaussian, predicting their deformations at desired timestamps using a multilayer perceptron (MLP). For more accurate static reconstruction, we also design a k-nearest neighbor (KNN)-based consistency regularization to handle the ground surface due to its low-texture characteristic. Extensive experiments on real-world datasets demonstrate that Urban4D not only achieves comparable or better quality than previous state-of-the-art methods but also effectively captures dynamic objects while maintaining high visual fidelity for static elements.",
    "arxiv_url": "http://arxiv.org/abs/2412.03473v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03473v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "dynamic",
      "ar",
      "deformation",
      "semantic",
      "face",
      "4d",
      "urban scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction",
    "authors": [
      "Wanting Zhang",
      "Haodong Xiang",
      "Zhichao Liao",
      "Xiansong Lai",
      "Xinghui Li",
      "Long Zeng"
    ],
    "abstract": "The reconstruction of indoor scenes remains challenging due to the inherent complexity of spatial structures and the prevalence of textureless regions. Recent advancements in 3D Gaussian Splatting have improved novel view synthesis with accelerated processing but have yet to deliver comparable performance in surface reconstruction. In this paper, we introduce 2DGS-Room, a novel method leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction. Specifically, we employ a seed-guided mechanism to control the distribution of 2D Gaussians, with the density of seed points dynamically optimized through adaptive growth and pruning mechanisms. To further improve geometric accuracy, we incorporate monocular depth and normal priors to provide constraints for details and textureless regions respectively. Additionally, multi-view consistency constraints are employed to mitigate artifacts and further enhance reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets demonstrate that our method achieves state-of-the-art performance in indoor scene reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2412.03428v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03428v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "high-fidelity",
      "face",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Volumetrically Consistent 3D Gaussian Rasterization",
    "authors": [
      "Chinmay Talegaonkar",
      "Yash Belhe",
      "Ravi Ramamoorthi",
      "Nicholas Antipa"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view synthesis at high inference speeds. However, its splatting-based rendering model makes several approximations to the rendering equation, reducing physical accuracy. We show that splatting and its approximations are unnecessary, even within a rasterizer; we instead volumetrically integrate 3D Gaussians directly to compute the transmittance across them analytically. We use this analytic transmittance to derive more physically-accurate alpha values than 3DGS, which can directly be used within their framework. The result is a method that more closely follows the volume rendering equation (similar to ray-tracing) while enjoying the speed benefits of rasterization. Our method represents opaque surfaces with higher accuracy and fewer points than 3DGS. This enables it to outperform 3DGS for view synthesis (measured in SSIM and LPIPS). Being volumetrically consistent also enables our method to work out of the box for tomography. We match the state-of-the-art 3DGS-based tomography method with fewer points. Being volumetrically consistent also enables our method to work out of the box for tomography. We match the state-of-the-art 3DGS-based tomography method with fewer points.",
    "arxiv_url": "http://arxiv.org/abs/2412.03378v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03378v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "gaussian splatting",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SGSST: Scaling Gaussian Splatting StyleTransfer",
    "authors": [
      "Bruno Galerne",
      "Jianling Wang",
      "Lara Raad",
      "Jean-Michel Morel"
    ],
    "abstract": "Applying style transfer to a full 3D environment is a challenging task that has seen many developments since the advent of neural rendering. 3D Gaussian splatting (3DGS) has recently pushed further many limits of neural rendering in terms of training speed and reconstruction quality. This work introduces SGSST: Scaling Gaussian Splatting Style Transfer, an optimization-based method to apply style transfer to pretrained 3DGS scenes. We demonstrate that a new multiscale loss based on global neural statistics, that we name SOS for Simultaneously Optimized Scales, enables style transfer to ultra-high resolution 3D scenes. Not only SGSST pioneers 3D scene style transfer at such high image resolutions, it also produces superior visual quality as assessed by thorough qualitative, quantitative and perceptual comparisons.",
    "arxiv_url": "http://arxiv.org/abs/2412.03371v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03371v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV",
      "cs.GR",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NeRF and Gaussian Splatting SLAM in the Wild",
    "authors": [
      "Fabian Schmidt",
      "Markus Enzweiler",
      "Abhinav Valada"
    ],
    "abstract": "Navigating outdoor environments with visual Simultaneous Localization and Mapping (SLAM) systems poses significant challenges due to dynamic scenes, lighting variations, and seasonal changes, requiring robust solutions. While traditional SLAM methods struggle with adaptability, deep learning-based approaches and emerging neural radiance fields as well as Gaussian Splatting-based SLAM methods, offer promising alternatives. However, these methods have primarily been evaluated in controlled indoor environments with stable conditions, leaving a gap in understanding their performance in unstructured and variable outdoor settings. This study addresses this gap by evaluating these methods in natural outdoor environments, focusing on camera tracking accuracy, robustness to environmental factors, and computational efficiency, highlighting distinct trade-offs. Extensive evaluations demonstrate that neural SLAM methods achieve superior robustness, particularly under challenging conditions such as low light, but at a high computational cost. At the same time, traditional methods perform the best across seasons but are highly sensitive to variations in lighting conditions. The code of the benchmark is publicly available at https://github.com/iis-esslingen/nerf-3dgs-benchmark.",
    "arxiv_url": "http://arxiv.org/abs/2412.03263v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03263v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "https://github.com/iis-esslingen/nerf-3dgs-benchmark",
    "keywords": [
      "gaussian splatting",
      "slam",
      "localization",
      "outdoor",
      "ar",
      "nerf",
      "mapping",
      "understanding",
      "tracking",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splats in Splats: Embedding Invisible 3D Watermark within Gaussian Splatting",
    "authors": [
      "Yijia Guo",
      "Wenkai Huang",
      "Yang Li",
      "Gaolei Li",
      "Hang Zhang",
      "Liwen Hu",
      "Jianhua Li",
      "Tiejun Huang",
      "Lei Ma"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction performance with explicit scene representations. Given the widespread application of 3DGS in 3D reconstruction and generation tasks, there is an urgent need to protect the copyright of 3DGS assets. However, existing copyright protection techniques for 3DGS overlook the usability of 3D assets, posing challenges for practical deployment. Here we describe WaterGS, the first 3DGS watermarking framework that embeds 3D content in 3DGS itself without modifying any attributes of the vanilla 3DGS. To achieve this, we take a deep insight into spherical harmonics (SH) and devise an importance-graded SH coefficient encryption strategy to embed the hidden SH coefficients. Furthermore, we employ a convolutional autoencoder to establish a mapping between the original Gaussian primitives' opacity and the hidden Gaussian primitives' opacity. Extensive experiments indicate that WaterGS significantly outperforms existing 3D steganography techniques, with 5.31% higher scene fidelity and 3X faster rendering speed, while ensuring security, robustness, and user experience. Codes and data will be released at https://water-gs.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2412.03121v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03121v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "gaussian splatting",
      "efficient",
      "ar",
      "3d gaussian",
      "mapping",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos",
    "authors": [
      "Yoonwoo Jeong",
      "Junmyeong Lee",
      "Hoseung Choi",
      "Minsu Cho"
    ],
    "abstract": "Dynamic view synthesis (DVS) has advanced remarkably in recent years, achieving high-fidelity rendering while reducing computational costs. Despite the progress, optimizing dynamic neural fields from casual videos remains challenging, as these videos do not provide direct 3D information, such as camera trajectories or the underlying scene geometry. In this work, we present RoDyGS, an optimization pipeline for dynamic Gaussian Splatting from casual videos. It effectively learns motion and underlying geometry of scenes by separating dynamic and static primitives, and ensures that the learned motion and geometry are physically plausible by incorporating motion and geometric regularization terms. We also introduce a comprehensive benchmark, Kubric-MRig, that provides extensive camera and object motion along with simultaneous multi-view captures, features that are absent in previous benchmarks. Experimental results demonstrate that the proposed method significantly outperforms previous pose-free dynamic neural fields and achieves competitive rendering quality compared to existing pose-free static neural fields. The code and data are publicly available at https://rodygs.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2412.03077v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03077v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "geometry",
      "motion",
      "ar",
      "high-fidelity",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting Under Attack: Investigating Adversarial Noise in 3D Objects",
    "authors": [
      "Abdurrahman Zeybey",
      "Mehmet Ergezer",
      "Tommy Nguyen"
    ],
    "abstract": "3D Gaussian Splatting has advanced radiance field reconstruction, enabling high-quality view synthesis and fast rendering in 3D modeling. While adversarial attacks on object detection models are well-studied for 2D images, their impact on 3D models remains underexplored. This work introduces the Masked Iterative Fast Gradient Sign Method (M-IFGSM), designed to generate adversarial noise targeting the CLIP vision-language model. M-IFGSM specifically alters the object of interest by focusing perturbations on masked regions, degrading the performance of CLIP's zero-shot object detection capability when applied to 3D models. Using eight objects from the Common Objects 3D (CO3D) dataset, we demonstrate that our method effectively reduces the accuracy and confidence of the model, with adversarial noise being nearly imperceptible to human observers. The top-1 accuracy in original model renders drops from 95.4\\% to 12.5\\% for train images and from 91.2\\% to 35.4\\% for test images, with confidence levels reflecting this shift from true classification to misclassification, underscoring the risks of adversarial attacks on 3D models in applications such as autonomous driving, robotics, and surveillance. The significance of this research lies in its potential to expose vulnerabilities in modern 3D vision models, including radiance fields, prompting the development of more robust defenses and security measures in critical real-world applications.",
    "arxiv_url": "http://arxiv.org/abs/2412.02803v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02803v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "gaussian splatting",
      "human",
      "ar",
      "3d gaussian",
      "robotics",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction",
    "authors": [
      "Lingteng Qiu",
      "Shenhao Zhu",
      "Qi Zuo",
      "Xiaodong Gu",
      "Yuan Dong",
      "Junfei Zhang",
      "Chao Xu",
      "Zhe Li",
      "Weihao Yuan",
      "Liefeng Bo",
      "Guanying Chen",
      "Zilong Dong"
    ],
    "abstract": "Generating animatable human avatars from a single image is essential for various digital human modeling applications. Existing 3D reconstruction methods often struggle to capture fine details in animatable models, while generative approaches for controllable animation, though avoiding explicit 3D modeling, suffer from viewpoint inconsistencies in extreme poses and computational inefficiencies. In this paper, we address these challenges by leveraging the power of generative models to produce detailed multi-view canonical pose images, which help resolve ambiguities in animatable human reconstruction. We then propose a robust method for 3D reconstruction of inconsistent images, enabling real-time rendering during inference. Specifically, we adapt a transformer-based video generation model to generate multi-view canonical pose images and normal maps, pretraining on a large-scale video dataset to improve generalization. To handle view inconsistencies, we recast the reconstruction problem as a 4D task and introduce an efficient 3D modeling approach using 4D Gaussian Splatting. Experiments demonstrate that our method achieves photorealistic, real-time animation of 3D human avatars from in-the-wild images, showcasing its effectiveness and generalization capability.",
    "arxiv_url": "http://arxiv.org/abs/2412.02684v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02684v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "animation",
      "human",
      "efficient",
      "ar",
      "avatar",
      "3d reconstruction",
      "4d",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation Benchmark",
    "authors": [
      "Haidong Xu",
      "Meishan Zhang",
      "Hao Ju",
      "Zhedong Zheng",
      "Hongyuan Zhu",
      "Erik Cambria",
      "Min Zhang",
      "Hao Fei"
    ],
    "abstract": "Producing emotionally dynamic 3D facial avatars with text derived from spoken words (Emo3D) has been a pivotal research topic in 3D avatar generation. While progress has been made in general-purpose 3D avatar generation, the exploration of generating emotional 3D avatars remains scarce, primarily due to the complexities of identifying and rendering rich emotions from spoken words. This paper reexamines Emo3D generation and draws inspiration from human processes, breaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping (T3DEM) and 3D Avatar Rendering (3DAR). T3DEM is the most crucial step in determining the quality of Emo3D generation and encompasses three key challenges: Expression Diversity, Emotion-Content Consistency, and Expression Fluidity. To address these challenges, we introduce a novel benchmark to advance research in Emo3D generation. First, we present EmoAva, a large-scale, high-quality dataset for T3DEM, comprising 15,000 text-to-3D expression mappings that characterize the aforementioned three challenges in Emo3D generation. Furthermore, we develop various metrics to effectively evaluate models against these identified challenges. Next, to effectively model the consistency, diversity, and fluidity of human expressions in the T3DEM step, we propose the Continuous Text-to-Expression Generator, which employs an autoregressive Conditional Variational Autoencoder for expression code generation, enhanced with Latent Temporal Attention and Expression-wise Attention mechanisms. Finally, to further enhance the 3DAR step on rendering higher-quality subtle expressions, we present the Globally-informed Gaussian Avatar (GiGA) model. GiGA incorporates a global information mechanism into 3D Gaussian representations, enabling the capture of subtle micro-expressions and seamless transitions between emotional states.",
    "arxiv_url": "http://arxiv.org/abs/2412.02508v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02508v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "motion",
      "ar",
      "3d gaussian",
      "avatar",
      "mapping",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RelayGS: Reconstructing Dynamic Scenes with Large-Scale and Complex Motions via Relay Gaussians",
    "authors": [
      "Qiankun Gao",
      "Yanmin Wu",
      "Chengxiang Wen",
      "Jiarui Meng",
      "Luyang Tang",
      "Jie Chen",
      "Ronggang Wang",
      "Jian Zhang"
    ],
    "abstract": "Reconstructing dynamic scenes with large-scale and complex motions remains a significant challenge. Recent techniques like Neural Radiance Fields and 3D Gaussian Splatting (3DGS) have shown promise but still struggle with scenes involving substantial movement. This paper proposes RelayGS, a novel method based on 3DGS, specifically designed to represent and reconstruct highly dynamic scenes. Our RelayGS learns a complete 4D representation with canonical 3D Gaussians and a compact motion field, consisting of three stages. First, we learn a fundamental 3DGS from all frames, ignoring temporal scene variations, and use a learnable mask to separate the highly dynamic foreground from the minimally moving background. Second, we replicate multiple copies of the decoupled foreground Gaussians from the first stage, each corresponding to a temporal segment, and optimize them using pseudo-views constructed from multiple frames within each segment. These Gaussians, termed Relay Gaussians, act as explicit relay nodes, simplifying and breaking down large-scale motion trajectories into smaller, manageable segments. Finally, we jointly learn the scene's temporal motion and refine the canonical Gaussians learned from the first two stages. We conduct thorough experiments on two dynamic scene datasets featuring large and complex motions, where our RelayGS outperforms state-of-the-arts by more than 1 dB in PSNR, and successfully reconstructs real-world basketball game scenes in a much more complete and coherent manner, whereas previous methods usually struggle to capture the complex motion of players. Code will be publicly available at https://github.com/gqk/RelayGS",
    "arxiv_url": "http://arxiv.org/abs/2412.02493v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02493v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/gqk/RelayGS",
    "keywords": [
      "gaussian splatting",
      "motion",
      "compact",
      "3d gaussian",
      "ar",
      "4d",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TimeWalker: Personalized Neural Space for Lifelong Head Avatars",
    "authors": [
      "Dongwei Pan",
      "Yang Li",
      "Hongsheng Li",
      "Kwan-Yee Lin"
    ],
    "abstract": "We present TimeWalker, a novel framework that models realistic, full-scale 3D head avatars of a person on lifelong scale. Unlike current human head avatar pipelines that capture identity at the momentary level(e.g., instant photography or short videos), TimeWalker constructs a person's comprehensive identity from unstructured data collection over his/her various life stages, offering a paradigm to achieve full reconstruction and animation of that person at different moments of life. At the heart of TimeWalker's success is a novel neural parametric model that learns personalized representation with the disentanglement of shape, expression, and appearance across ages. Central to our methodology are the concepts of two aspects: (1) We track back to the principle of modeling a person's identity in an additive combination of average head representation in the canonical space, and moment-specific head attribute representations driven from a set of neural head basis. To learn the set of head basis that could represent the comprehensive head variations in a compact manner, we propose a Dynamic Neural Basis-Blending Module (Dynamo). It dynamically adjusts the number and blend weights of neural head bases, according to both shared and specific traits of the target person over ages. (2) Dynamic 2D Gaussian Splatting (DNA-2DGS), an extension of Gaussian splatting representation, to model head motion deformations like facial expressions without losing the realism of rendering and reconstruction. DNA-2DGS includes a set of controllable 2D oriented planar Gaussian disks that utilize the priors from parametric model, and move/rotate with the change of expression. Through extensive experimental evaluations, we show TimeWalker's ability to reconstruct and animate avatars across decoupled dimensions with realistic rendering effects, demonstrating a way to achieve personalized 'time traveling' in a breeze.",
    "arxiv_url": "http://arxiv.org/abs/2412.02421v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02421v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "animation",
      "motion",
      "human",
      "compact",
      "head",
      "ar",
      "deformation",
      "avatar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Realistic Surgical Simulation from Monocular Videos",
    "authors": [
      "Kailing Wang",
      "Chen Yang",
      "Keyang Zhao",
      "Xiaokang Yang",
      "Wei Shen"
    ],
    "abstract": "This paper tackles the challenge of automatically performing realistic surgical simulations from readily available surgical videos. Recent efforts have successfully integrated physically grounded dynamics within 3D Gaussians to perform high-fidelity simulations in well-reconstructed simulation environments from static scenes. However, they struggle with the geometric inconsistency in reconstructing simulation environments and unrealistic physical deformations in simulations of soft tissues when it comes to dynamic and complex surgical processes. In this paper, we propose SurgiSim, a novel automatic simulation system to overcome these limitations. To build a surgical simulation environment, we maintain a canonical 3D scene composed of 3D Gaussians coupled with a deformation field to represent a dynamic surgical scene. This process involves a multi-stage optimization with trajectory and anisotropic regularization, enhancing the geometry consistency of the canonical scene, which serves as the simulation environment. To achieve realistic physical simulations in this environment, we implement a Visco-Elastic deformation model based on the Maxwell model, effectively restoring the complex deformations of tissues. Additionally, we infer the physical parameters of tissues by minimizing the discrepancies between the input video and simulation results guided by estimated tissue motion, ensuring realistic simulation outcomes. Experiments on various surgical scenarios and interactions demonstrate SurgiSim's ability to perform realistic simulation of soft tissues among surgical procedures, showing its enormous potential for enhancing surgical training, planning, and robotic surgery systems. The project page is at https://namaenashibot.github.io/SurgiSim/.",
    "arxiv_url": "http://arxiv.org/abs/2412.02359v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02359v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "motion",
      "ar",
      "3d gaussian",
      "deformation",
      "high-fidelity",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSGTrack: Gaussian Splatting-Guided Object Pose Tracking from RGB Videos",
    "authors": [
      "Zhiyuan Chen",
      "Fan Lu",
      "Guo Yu",
      "Bin Li",
      "Sanqing Qu",
      "Yuan Huang",
      "Changhong Fu",
      "Guang Chen"
    ],
    "abstract": "Tracking the 6DoF pose of unknown objects in monocular RGB video sequences is crucial for robotic manipulation. However, existing approaches typically rely on accurate depth information, which is non-trivial to obtain in real-world scenarios. Although depth estimation algorithms can be employed, geometric inaccuracy can lead to failures in RGBD-based pose tracking methods. To address this challenge, we introduce GSGTrack, a novel RGB-based pose tracking framework that jointly optimizes geometry and pose. Specifically, we adopt 3D Gaussian Splatting to create an optimizable 3D representation, which is learned simultaneously with a graph-based geometry optimization to capture the object's appearance features and refine its geometry. However, the joint optimization process is susceptible to perturbations from noisy pose and geometry data. Thus, we propose an object silhouette loss to address the issue of pixel-wise loss being overly sensitive to pose noise during tracking. To mitigate the geometric ambiguities caused by inaccurate depth information, we propose a geometry-consistent image pair selection strategy, which filters out low-confidence pairs and ensures robust geometric optimization. Extensive experiments on the OnePose and HO3D datasets demonstrate the effectiveness of GSGTrack in both 6DoF pose tracking and object reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2412.02267v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02267v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "geometry",
      "ar",
      "3d gaussian",
      "tracking"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-robot autonomous 3D reconstruction using Gaussian splatting with Semantic guidance",
    "authors": [
      "Jing Zeng",
      "Qi Ye",
      "Tianle Liu",
      "Yang Xu",
      "Jin Li",
      "Jinming Xu",
      "Liang Li",
      "Jiming Chen"
    ],
    "abstract": "Implicit neural representations and 3D Gaussian splatting (3DGS) have shown great potential for scene reconstruction. Recent studies have expanded their applications in autonomous reconstruction through task assignment methods. However, these methods are mainly limited to single robot, and rapid reconstruction of large-scale scenes remains challenging. Additionally, task-driven planning based on surface uncertainty is prone to being trapped in local optima. To this end, we propose the first 3DGS-based centralized multi-robot autonomous 3D reconstruction framework. To further reduce time cost of task generation and improve reconstruction quality, we integrate online open-vocabulary semantic segmentation with surface uncertainty of 3DGS, focusing view sampling on regions with high instance uncertainty. Finally, we develop a multi-robot collaboration strategy with mode and task assignments improving reconstruction quality while ensuring planning efficiency. Our method demonstrates the highest reconstruction quality among all planning methods and superior planning efficiency compared to existing multi-robot methods. We deploy our method on multiple robots, and results show that it can effectively plan view paths and reconstruct scenes with high quality.",
    "arxiv_url": "http://arxiv.org/abs/2412.02249v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02249v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "high quality",
      "3d gaussian",
      "3d reconstruction",
      "semantic",
      "face",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SparseLGS: Sparse View Language Embedded Gaussian Splatting",
    "authors": [
      "Jun Hu",
      "Zhang Chen",
      "Zhong Li",
      "Yi Xu",
      "Juyong Zhang"
    ],
    "abstract": "Recently, several studies have combined Gaussian Splatting to obtain scene representations with language embeddings for open-vocabulary 3D scene understanding. While these methods perform well, they essentially require very dense multi-view inputs, limiting their applicability in real-world scenarios. In this work, we propose SparseLGS to address the challenge of 3D scene understanding with pose-free and sparse view input images. Our method leverages a learning-based dense stereo model to handle pose-free and sparse inputs, and a three-step region matching approach to address the multi-view semantic inconsistency problem, which is especially important for sparse inputs. Different from directly learning high-dimensional CLIP features, we extract low-dimensional information and build bijections to avoid excessive learning and storage costs. We introduce a reconstruction loss during semantic training to improve Gaussian positions and shapes. To the best of our knowledge, we are the first to address the 3D semantic field problem with sparse pose-free inputs. Experimental results show that SparseLGS achieves comparable quality when reconstructing semantic fields with fewer inputs (3-4 views) compared to previous SOTA methods with dense input. Besides, when using the same sparse input, SparseLGS leads significantly in quality and heavily improves the computation speed (5$\\times$speedup). Project page: https://ustc3dv.github.io/SparseLGS",
    "arxiv_url": "http://arxiv.org/abs/2412.02245v2",
    "pdf_url": "http://arxiv.org/pdf/2412.02245v2",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "sparse view",
      "ar",
      "semantic",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "How to Use Diffusion Priors under Sparse Views?",
    "authors": [
      "Qisen Wang",
      "Yifan Zhao",
      "Jiawei Ma",
      "Jia Li"
    ],
    "abstract": "Novel view synthesis under sparse views has been a long-term important challenge in 3D reconstruction. Existing works mainly rely on introducing external semantic or depth priors to supervise the optimization of 3D representations. However, the diffusion model, as an external prior that can directly provide visual supervision, has always underperformed in sparse-view 3D reconstruction using Score Distillation Sampling (SDS) due to the low information entropy of sparse views compared to text, leading to optimization challenges caused by mode deviation. To this end, we present a thorough analysis of SDS from the mode-seeking perspective and propose Inline Prior Guided Score Matching (IPSM), which leverages visual inline priors provided by pose relationships between viewpoints to rectify the rendered image distribution and decomposes the original optimization objective of SDS, thereby offering effective diffusion visual guidance without any fine-tuning or pre-training. Furthermore, we propose the IPSM-Gaussian pipeline, which adopts 3D Gaussian Splatting as the backbone and supplements depth and geometry consistency regularization based on IPSM to further improve inline priors and rectified distribution. Experimental results on different public datasets show that our method achieves state-of-the-art reconstruction quality. The code is released at https://github.com/iCVTEAM/IPSM.",
    "arxiv_url": "http://arxiv.org/abs/2412.02225v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02225v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/iCVTEAM/IPSM",
    "keywords": [
      "gaussian splatting",
      "geometry",
      "sparse-view",
      "sparse view",
      "ar",
      "3d gaussian",
      "3d reconstruction",
      "semantic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SparseGrasp: Robotic Grasping via 3D Semantic Gaussian Splatting from Sparse Multi-View RGB Images",
    "authors": [
      "Junqiu Yu",
      "Xinlin Ren",
      "Yongchong Gu",
      "Haitao Lin",
      "Tianyu Wang",
      "Yi Zhu",
      "Hang Xu",
      "Yu-Gang Jiang",
      "Xiangyang Xue",
      "Yanwei Fu"
    ],
    "abstract": "Language-guided robotic grasping is a rapidly advancing field where robots are instructed using human language to grasp specific objects. However, existing methods often depend on dense camera views and struggle to quickly update scenes, limiting their effectiveness in changeable environments.   In contrast, we propose SparseGrasp, a novel open-vocabulary robotic grasping system that operates efficiently with sparse-view RGB images and handles scene updates fastly. Our system builds upon and significantly enhances existing computer vision modules in robotic learning. Specifically, SparseGrasp utilizes DUSt3R to generate a dense point cloud as the initialization for 3D Gaussian Splatting (3DGS), maintaining high fidelity even under sparse supervision. Importantly, SparseGrasp incorporates semantic awareness from recent vision foundation models. To further improve processing efficiency, we repurpose Principal Component Analysis (PCA) to compress features from 2D models. Additionally, we introduce a novel render-and-compare strategy that ensures rapid scene updates, enabling multi-turn grasping in changeable environments.   Experimental results show that SparseGrasp significantly outperforms state-of-the-art methods in terms of both speed and adaptability, providing a robust solution for multi-turn grasping in changeable environment.",
    "arxiv_url": "http://arxiv.org/abs/2412.02140v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02140v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "gaussian splatting",
      "sparse-view",
      "human",
      "efficient",
      "ar",
      "3d gaussian",
      "semantic"
    ],
    "citations": 0,
    "semantic_url": ""
  }
]