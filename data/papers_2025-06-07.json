[
  {
    "title": "Astraea: A GPU-Oriented Token-wise Acceleration Framework for Video Diffusion Transformers",
    "authors": [
      "Haosong Liu",
      "Yuge Cheng",
      "Zihan Liu",
      "Aiyue Chen",
      "Yiwu Yao",
      "Chen Chen",
      "Jingwen Leng",
      "Yu Feng",
      "Minyi Guo"
    ],
    "abstract": "Video diffusion transformers (vDiTs) have made impressive progress in text-to-video generation, but their high computational demands present major challenges for practical deployment. While existing acceleration methods reduce workload at various granularities, they often rely on heuristics, limiting their applicability.   We introduce ASTRAEA, an automatic framework that searches for near-optimal configurations for vDiT-based video generation. At its core, ASTRAEA proposes a lightweight token selection mechanism and a memory-efficient, GPU-parallel sparse attention strategy, enabling linear reductions in execution time with minimal impact on generation quality. To determine optimal token reduction for different timesteps, we further design a search framework that leverages a classic evolutionary algorithm to automatically determine the distribution of the token budget effectively. Together, ASTRAEA achieves up to 2.4x inference speedup on a single GPU with great scalability (up to 13.2x speedup on 8 GPUs) while retaining better video quality compared to the state-of-the-art methods (<0.5% loss on the VBench score compared to the baseline vDiT models).",
    "arxiv_url": "http://arxiv.org/abs/2506.05096v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05096v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LayerFlow: A Unified Model for Layer-aware Video Generation",
    "authors": [
      "Sihui Ji",
      "Hao Luo",
      "Xi Chen",
      "Yuanpeng Tu",
      "Yiyang Wang",
      "Hengshuang Zhao"
    ],
    "abstract": "We present LayerFlow, a unified solution for layer-aware video generation. Given per-layer prompts, LayerFlow generates videos for the transparent foreground, clean background, and blended scene. It also supports versatile variants like decomposing a blended video or generating the background for the given foreground and vice versa. Starting from a text-to-video diffusion transformer, we organize the videos for different layers as sub-clips, and leverage layer embeddings to distinguish each clip and the corresponding layer-wise prompts. In this way, we seamlessly support the aforementioned variants in one unified framework. For the lack of high-quality layer-wise training videos, we design a multi-stage training strategy to accommodate static images with high-quality layer annotations. Specifically, we first train the model with low-quality video data. Then, we tune a motion LoRA to make the model compatible with static frames. Afterward, we train the content LoRA on the mixture of image data with high-quality layered images along with copy-pasted video data. During inference, we remove the motion LoRA thus generating smooth videos with desired layers.",
    "arxiv_url": "http://arxiv.org/abs/2506.04228v1",
    "pdf_url": "http://arxiv.org/pdf/2506.04228v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion Transformers",
    "authors": [
      "Xuanhua He",
      "Quande Liu",
      "Zixuan Ye",
      "Weicai Ye",
      "Qiulin Wang",
      "Xintao Wang",
      "Qifeng Chen",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai"
    ],
    "abstract": "Fine-grained and efficient controllability on video diffusion transformers has raised increasing desires for the applicability. Recently, In-context Conditioning emerged as a powerful paradigm for unified conditional video generation, which enables diverse controls by concatenating varying context conditioning signals with noisy video latents into a long unified token sequence and jointly processing them via full-attention, e.g., FullDiT. Despite their effectiveness, these methods face quadratic computation overhead as task complexity increases, hindering practical deployment. In this paper, we study the efficiency bottleneck neglected in original in-context conditioning video generation framework. We begin with systematic analysis to identify two key sources of the computation inefficiencies: the inherent redundancy within context condition tokens and the computational redundancy in context-latent interactions throughout the diffusion process. Based on these insights, we propose FullDiT2, an efficient in-context conditioning framework for general controllability in both video generation and editing tasks, which innovates from two key perspectives. Firstly, to address the token redundancy, FullDiT2 leverages a dynamic token selection mechanism to adaptively identify important context tokens, reducing the sequence length for unified full-attention. Additionally, a selective context caching mechanism is devised to minimize redundant interactions between condition tokens and video latents. Extensive experiments on six diverse conditional video editing and generation tasks demonstrate that FullDiT2 achieves significant computation reduction and 2-3 times speedup in averaged time cost per diffusion step, with minimal degradation or even higher performance in video generation quality. The project page is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}.",
    "arxiv_url": "http://arxiv.org/abs/2506.04213v2",
    "pdf_url": "http://arxiv.org/pdf/2506.04213v2",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "video editing",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Image Editing As Programs with Diffusion Models",
    "authors": [
      "Yujia Hu",
      "Songhua Liu",
      "Zhenxiong Tan",
      "Xingyi Yang",
      "Xinchao Wang"
    ],
    "abstract": "While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial layout changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a unified image editing framework built upon the Diffusion Transformer (DiT) architecture. At its core, IEAP approaches instructional editing through a reductionist lens, decomposing complex editing instructions into sequences of atomic operations. Each operation is implemented via a lightweight adapter sharing the same DiT backbone and is specialized for a specific type of edit. Programmed by a vision-language model (VLM)-based agent, these operations collaboratively support arbitrary and structurally inconsistent transformations. By modularizing and sequencing edits in this way, IEAP generalizes robustly across a wide range of editing tasks, from simple adjustments to substantial structural changes. Extensive experiments demonstrate that IEAP significantly outperforms state-of-the-art methods on standard benchmarks across various editing scenarios. In these evaluations, our framework delivers superior accuracy and semantic fidelity, particularly for complex, multi-step instructions. Codes are available at https://github.com/YujiaHu1109/IEAP.",
    "arxiv_url": "http://arxiv.org/abs/2506.04158v1",
    "pdf_url": "http://arxiv.org/pdf/2506.04158v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/YujiaHu1109/IEAP",
    "keywords": [
      "diffusion transformer",
      "text-to-image",
      "image editing",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EmoArt: A Multidimensional Dataset for Emotion-Aware Artistic Generation",
    "authors": [
      "Cheng Zhang",
      "Hongxia xie",
      "Bin Wen",
      "Songhan Zuo",
      "Ruoxuan Zhang",
      "Wen-huang Cheng"
    ],
    "abstract": "With the rapid advancement of diffusion models, text-to-image generation has achieved significant progress in image resolution, detail fidelity, and semantic alignment, particularly with models like Stable Diffusion 3.5, Stable Diffusion XL, and FLUX 1. However, generating emotionally expressive and abstract artistic images remains a major challenge, largely due to the lack of large-scale, fine-grained emotional datasets. To address this gap, we present the EmoArt Dataset -- one of the most comprehensive emotion-annotated art datasets to date. It contains 132,664 artworks across 56 painting styles (e.g., Impressionism, Expressionism, Abstract Art), offering rich stylistic and cultural diversity. Each image includes structured annotations: objective scene descriptions, five key visual attributes (brushwork, composition, color, line, light), binary arousal-valence labels, twelve emotion categories, and potential art therapy effects. Using EmoArt, we systematically evaluate popular text-to-image diffusion models for their ability to generate emotionally aligned images from text. Our work provides essential data and benchmarks for emotion-driven image synthesis and aims to advance fields such as affective computing, multimodal learning, and computational art, enabling applications in art therapy and creative design. The dataset and more details can be accessed via our project website.",
    "arxiv_url": "http://arxiv.org/abs/2506.03652v1",
    "pdf_url": "http://arxiv.org/pdf/2506.03652v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "FLUX",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas",
    "authors": [
      "Austin Silveria",
      "Soham V. Govande",
      "Daniel Y. Fu"
    ],
    "abstract": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in high-quality image and video generation but incur substantial compute cost at inference. A common observation is that DiT latent noise vectors change slowly across inference steps, which suggests that the DiT compute may be redundant across steps. In this paper, we aim to speed up inference by reducing this redundancy, without additional training. We first study how activations change between steps in two state-of-the-art open-source DiTs. We find that just 5-25% of the values in attention and MLP explain 70-90% of the change in activations across steps. This finding motivates our approach, Chipmunk, which uses dynamic sparsity at inference time to recompute only the fastest-changing intermediate activations, while caching the rest. Dynamic sparsity introduces two systems challenges: (1) sparse attention and MLP operations tend to underutilize GPU tensor cores; and (2) computing dynamic sparsity patterns at runtime and caching activations both introduce overhead. To address these challenges, Chipmunk first uses a voxel-based reordering of input tokens to introduce column-wise sparsity. We implement column-sparse kernels utilizing efficient sparse gathers from global to shared GPU memory, achieving a 9.3x speedup at 93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk overlaps the computation of sparsity patterns and cache updates with other parts of the computation (e.g., second layer of the MLP) to hide the extra latency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on FLUX.1-dev without compromising generation quality. Furthermore, we show that Chipmunk can be stacked on top of full step caching, achieving a 3.72x speedup on HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev with minimal quality impact.",
    "arxiv_url": "http://arxiv.org/abs/2506.03275v1",
    "pdf_url": "http://arxiv.org/pdf/2506.03275v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation",
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions",
    "authors": [
      "Di Chang",
      "Mingdeng Cao",
      "Yichun Shi",
      "Bo Liu",
      "Shengqu Cai",
      "Shijie Zhou",
      "Weilin Huang",
      "Gordon Wetzstein",
      "Mohammad Soleymani",
      "Peng Wang"
    ],
    "abstract": "Editing images with instructions to reflect non-rigid motions, camera viewpoint shifts, object deformations, human articulations, and complex interactions, poses a challenging yet underexplored problem in computer vision. Existing approaches and datasets predominantly focus on static scenes or rigid transformations, limiting their capacity to handle expressive edits involving dynamic motion. To address this gap, we introduce ByteMorph, a comprehensive framework for instruction-based image editing with an emphasis on non-rigid motions. ByteMorph comprises a large-scale dataset, ByteMorph-6M, and a strong baseline model built upon the Diffusion Transformer (DiT), named ByteMorpher. ByteMorph-6M includes over 6 million high-resolution image editing pairs for training, along with a carefully curated evaluation benchmark ByteMorph-Bench. Both capture a wide variety of non-rigid motion types across diverse environments, human figures, and object categories. The dataset is constructed using motion-guided data generation, layered compositing techniques, and automated captioning to ensure diversity, realism, and semantic coherence. We further conduct a comprehensive evaluation of recent instruction-based image editing methods from both academic and commercial domains.",
    "arxiv_url": "http://arxiv.org/abs/2506.03107v1",
    "pdf_url": "http://arxiv.org/pdf/2506.03107v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "image editing"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers",
    "authors": [
      "Pengtao Chen",
      "Xianfang Zeng",
      "Maosen Zhao",
      "Peng Ye",
      "Mingzhu Shen",
      "Wei Cheng",
      "Gang Yu",
      "Tao Chen"
    ],
    "abstract": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video generation, this long sequence generation task remains constrained by the quadratic complexity of attention mechanisms, resulting in significant inference latency. Through detailed analysis of attention maps in Video Diffusion Transformer (vDiT), we identify three recurring sparsity patterns: diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\% attention heads can be skipped. Crucially, these patterns exhibit strong layer-depth and head-position correlations but show limited dependence on the input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels that replace dense attention with computationally efficient implementations for each identified sparsity pattern. 2) An offline sparse diffusion search algorithm that selects the optimal sparse computation strategy per layer and head via hardware-aware cost modeling. After determining the optimal configuration, we fuse heads within the same layer that share the same attention strategy, enhancing inference efficiency. Integrated into state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1), Sparse-vDiT achieves 2.09$\\times$, 2.38$\\times$, and 1.67$\\times$ theoretical FLOP reduction, and actual inference speedups of 1.76$\\times$, 1.85$\\times$, and 1.58$\\times$, respectively, while maintaining high visual fidelity, with PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent structural sparsity in vDiTs can be systematically exploited for long video synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2506.03065v1",
    "pdf_url": "http://arxiv.org/pdf/2506.03065v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers",
    "authors": [
      "Yan Gong",
      "Yiren Song",
      "Yicheng Li",
      "Chenglin Li",
      "Yin Zhang"
    ],
    "abstract": "Inspired by the in-context learning mechanism of large language models (LLMs), a new paradigm of generalizable visual prompt-based image editing is emerging. Existing single-reference methods typically focus on style or appearance adjustments and struggle with non-rigid transformations. To address these limitations, we propose leveraging source-target image pairs to extract and transfer content-aware editing intent to novel query images. To this end, we introduce RelationAdapter, a lightweight module that enables Diffusion Transformer (DiT) based models to effectively capture and apply visual transformations from minimal examples. We also introduce Relation252K, a comprehensive dataset comprising 218 diverse editing tasks, to evaluate model generalization and adaptability in visual prompt-driven scenarios. Experiments on Relation252K show that RelationAdapter significantly improves the model's ability to understand and transfer editing intent, leading to notable gains in generation quality and overall editing performance.",
    "arxiv_url": "http://arxiv.org/abs/2506.02528v1",
    "pdf_url": "http://arxiv.org/pdf/2506.02528v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "image editing"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OmniV2V: Versatile Video Generation and Editing via Dynamic Content Manipulation",
    "authors": [
      "Sen Liang",
      "Zhentao Yu",
      "Zhengguang Zhou",
      "Teng Hu",
      "Hongmei Wang",
      "Yi Chen",
      "Qin Lin",
      "Yuan Zhou",
      "Xin Li",
      "Qinglin Lu",
      "Zhibo Chen"
    ],
    "abstract": "The emergence of Diffusion Transformers (DiT) has brought significant advancements to video generation, especially in text-to-video and image-to-video tasks. Although video generation is widely applied in various fields, most existing models are limited to single scenarios and cannot perform diverse video generation and editing through dynamic content manipulation. We propose OmniV2V, a video model capable of generating and editing videos across different scenarios based on various operations, including: object movement, object addition, mask-guided video edit, try-on, inpainting, outpainting, human animation, and controllable character video synthesis. We explore a unified dynamic content manipulation injection module, which effectively integrates the requirements of the above tasks. In addition, we design a visual-text instruction module based on LLaVA, enabling the model to effectively understand the correspondence between visual content and instructions. Furthermore, we build a comprehensive multi-task data processing system. Since there is data overlap among various tasks, this system can efficiently provide data augmentation. Using this system, we construct a multi-type, multi-scenario OmniV2V dataset and its corresponding OmniV2V-Test benchmark. Extensive experiments show that OmniV2V works as well as, and sometimes better than, the best existing open-source and commercial models for many video generation and editing tasks.",
    "arxiv_url": "http://arxiv.org/abs/2506.01801v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01801v1",
    "published_date": "2025-06-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "Controllable",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LongDWM: Cross-Granularity Distillation for Building a Long-Term Driving World Model",
    "authors": [
      "Xiaodong Wang",
      "Zhirong Wu",
      "Peixi Peng"
    ],
    "abstract": "Driving world models are used to simulate futures by video generation based on the condition of the current state and actions. However, current models often suffer serious error accumulations when predicting the long-term future, which limits the practical application. Recent studies utilize the Diffusion Transformer (DiT) as the backbone of driving world models to improve learning flexibility. However, these models are always trained on short video clips (high fps and short duration), and multiple roll-out generations struggle to produce consistent and reasonable long videos due to the training-inference gap. To this end, we propose several solutions to build a simple yet effective long-term driving world model. First, we hierarchically decouple world model learning into large motion learning and bidirectional continuous motion learning. Then, considering the continuity of driving scenes, we propose a simple distillation method where fine-grained video flows are self-supervised signals for coarse-grained flows. The distillation is designed to improve the coherence of infinite video generation. The coarse-grained and fine-grained modules are coordinated to generate long-term and temporally coherent videos. In the public benchmark NuScenes, compared with the state-of-the-art front-view model, our model improves FVD by $27\\%$ and reduces inference time by $85\\%$ for the video task of generating 110+ frames. More videos (including 90s duration) are available at https://Wang-Xiaodong1899.github.io/longdwm/.",
    "arxiv_url": "http://arxiv.org/abs/2506.01546v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01546v1",
    "published_date": "2025-06-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Playing with Transformer at 30+ FPS via Next-Frame Diffusion",
    "authors": [
      "Xinle Cheng",
      "Tianyu He",
      "Jiayi Xu",
      "Junliang Guo",
      "Di He",
      "Jiang Bian"
    ],
    "abstract": "Autoregressive video models offer distinct advantages over bidirectional diffusion models in creating interactive video content and supporting streaming applications with arbitrary duration. In this work, we present Next-Frame Diffusion (NFD), an autoregressive diffusion transformer that incorporates block-wise causal attention, enabling iterative sampling and efficient inference via parallel token generation within each frame. Nonetheless, achieving real-time video generation remains a significant challenge for such models, primarily due to the high computational cost associated with diffusion sampling and the hardware inefficiencies inherent to autoregressive generation. To address this, we introduce two innovations: (1) We extend consistency distillation to the video domain and adapt it specifically for video models, enabling efficient inference with few sampling steps; (2) To fully leverage parallel computation, motivated by the observation that adjacent frames often share the identical action input, we propose speculative sampling. In this approach, the model generates next few frames using current action input, and discard speculatively generated frames if the input action differs. Experiments on a large-scale action-conditioned video generation benchmark demonstrate that NFD beats autoregressive baselines in terms of both visual quality and sampling efficiency. We, for the first time, achieves autoregressive video generation at over 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.",
    "arxiv_url": "http://arxiv.org/abs/2506.01380v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01380v1",
    "published_date": "2025-06-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Evaluating Robot Policies in a World Model",
    "authors": [
      "Julian Quevedo",
      "Percy Liang",
      "Sherry Yang"
    ],
    "abstract": "Robotics has broad applications from automating house chores to taking care of patients. However, evaluating robot control policies is challenging, as real-world testing is expensive, while handcrafted simulations often fail to accurately reflect real-world conditions, resulting in poor correlation between simulated evaluation and real-world outcomes. In this work, we investigate World-model-based Policy Evaluation (WPE). We first train an action-conditioned video generation model as a proxy to real-world environments. To enable efficient rollouts of hundreds of interactive steps while mitigating error accumulation in the world model, we propose an inference scheme which we call Blockwise-Autoregressive Diffusion Transformer with adjustable context and decoding horizon lengths. To ensure that the world model indeed follows action input, we propose metrics based on the agreement between the ground truth video and generated video conditioned on the same sequence of actions to evaluate the world model. We then use the world model for policy evaluation by performing Monte Carlo rollouts in the world model while employing a vision-language model (VLM) as a reward function. Interestingly, we found that WPE tends to underestimate the policy values for in-distribution actions and overestimate policy values for out-of-distribution actions. Nevertheless, WPE preserves the relative rankings of different policies. In emulating real robot executions, WPE achieves high fidelity in mimicing robot arm movements as in real videos, while emulating highly realistic object interaction remains challenging. Despite this limitation, we show that a world model can serve as a starting point for evaluating robot policies before real-world deployment.",
    "arxiv_url": "http://arxiv.org/abs/2506.00613v1",
    "pdf_url": "http://arxiv.org/pdf/2506.00613v1",
    "published_date": "2025-05-31",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise Shape and Semantic Control",
    "authors": [
      "Danfeng li",
      "Hui Zhang",
      "Sheng Wang",
      "Jiacheng Li",
      "Zuxuan Wu"
    ],
    "abstract": "Despite recent advances in diffusion models, top-tier text-to-image (T2I) models still struggle to achieve precise spatial layout control, i.e. accurately generating entities with specified attributes and locations. Segmentation-mask-to-image (S2I) generation has emerged as a promising solution by incorporating pixel-level spatial guidance and regional text prompts. However, existing S2I methods fail to simultaneously ensure semantic consistency and shape consistency. To address these challenges, we propose Seg2Any, a novel S2I framework built upon advanced multimodal diffusion transformers (e.g. FLUX). First, to achieve both semantic and shape consistency, we decouple segmentation mask conditions into regional semantic and high-frequency shape components. The regional semantic condition is introduced by a Semantic Alignment Attention Mask, ensuring that generated entities adhere to their assigned text prompts. The high-frequency shape condition, representing entity boundaries, is encoded as an Entity Contour Map and then introduced as an additional modality via multi-modal attention to guide image spatial structure. Second, to prevent attribute leakage across entities in multi-entity scenarios, we introduce an Attribute Isolation Attention Mask mechanism, which constrains each entity's image tokens to attend exclusively to themselves during image self-attention. To support open-set S2I generation, we construct SACap-1M, a large-scale dataset containing 1 million images with 5.9 million segmented entities and detailed regional captions, along with a SACap-Eval benchmark for comprehensive S2I evaluation. Extensive experiments demonstrate that Seg2Any achieves state-of-the-art performance on both open-set and closed-set S2I benchmarks, particularly in fine-grained spatial and attribute control of entities.",
    "arxiv_url": "http://arxiv.org/abs/2506.00596v1",
    "pdf_url": "http://arxiv.org/pdf/2506.00596v1",
    "published_date": "2025-05-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "image generation",
      "text-to-image",
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation",
    "authors": [
      "Muhammad Adnan",
      "Nithesh Kurella",
      "Akhil Arunkumar",
      "Prashant J. Nair"
    ],
    "abstract": "Diffusion Transformers (DiTs) achieve state-of-the-art results in text-to-image, text-to-video generation, and editing. However, their large model size and the quadratic cost of spatial-temporal attention over multiple denoising steps make video generation computationally expensive. Static caching mitigates this by reusing features across fixed steps but fails to adapt to generation dynamics, leading to suboptimal trade-offs between speed and quality.   We propose Foresight, an adaptive layer-reuse technique that reduces computational redundancy across denoising steps while preserving baseline performance. Foresight dynamically identifies and reuses DiT block outputs for all layers across steps, adapting to generation parameters such as resolution and denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and CogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining video quality. The source code of Foresight is available at \\texttt{https://github.com/STAR-Laboratory/foresight}.",
    "arxiv_url": "http://arxiv.org/abs/2506.00329v1",
    "pdf_url": "http://arxiv.org/pdf/2506.00329v1",
    "published_date": "2025-05-31",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "https://github.com/STAR-Laboratory/foresight",
    "keywords": [
      "diffusion transformer",
      "video generation",
      "text-to-image"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning",
    "authors": [
      "Stepan Shabalin",
      "Ayush Panda",
      "Dmitrii Kharlapenko",
      "Abdur Raheem Ali",
      "Yixiong Hao",
      "Arthur Conmy"
    ],
    "abstract": "Sparse autoencoders are a promising new approach for decomposing language model activations for interpretation and control. They have been applied successfully to vision transformer image encoders and to small-scale diffusion models. Inference-Time Decomposition of Activations (ITDA) is a recently proposed variant of dictionary learning that takes the dictionary to be a set of data points from the activation distribution and reconstructs them with gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large text-to-image diffusion model, Flux 1, and consider the interpretability of embeddings of both by introducing a visual automated interpretation pipeline. We find that SAEs accurately reconstruct residual stream embeddings and beat MLP neurons on interpretability. We are able to use SAE features to steer image generation through activation addition. We find that ITDA has comparable interpretability to SAEs.",
    "arxiv_url": "http://arxiv.org/abs/2505.24360v2",
    "pdf_url": "http://arxiv.org/pdf/2505.24360v2",
    "published_date": "2025-05-30",
    "categories": [
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "FLUX",
      "image generation",
      "Control"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow Matching Models",
    "authors": [
      "Zheng Tan",
      "Weizhen Wang",
      "Andrea L. Bertozzi",
      "Ernest K. Ryu"
    ],
    "abstract": "Diffusion models (DMs) have demonstrated remarkable performance in high-fidelity image and video generation. Because high-quality generations with DMs typically require a large number of function evaluations (NFEs), resulting in slow sampling, there has been extensive research successfully reducing the NFE to a small range (<10) while maintaining acceptable image quality. However, many practical applications, such as those involving Stable Diffusion 3.5, FLUX, and SANA, commonly operate in the mid-NFE regime (20-50 NFE) to achieve superior results, and, despite the practical relevance, research on the effective sampling within this mid-NFE regime remains underexplored. In this work, we propose a novel, training-free, and structure-independent DM ODE solver called the Stabilized Taylor Orthogonal Runge--Kutta (STORK) method, based on a class of stiff ODE solvers with a Taylor expansion adaptation. Unlike prior work such as DPM-Solver, which is dependent on the semi-linear structure of the DM ODE, STORK is applicable to any DM sampling, including noise-based and flow matching-based models. Within the 20-50 NFE range, STORK achieves improved generation quality, as measured by FID scores, across unconditional pixel-level generation and conditional latent-space generation tasks using models like Stable Diffusion 3.5 and SANA. Code is available at https://github.com/ZT220501/STORK.",
    "arxiv_url": "http://arxiv.org/abs/2505.24210v1",
    "pdf_url": "http://arxiv.org/pdf/2505.24210v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CV",
      "cs.NA",
      "math.NA"
    ],
    "github_url": "https://github.com/ZT220501/STORK",
    "keywords": [
      "video generation",
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers",
    "authors": [
      "Yusuf Dalva",
      "Hidir Yesiltepe",
      "Pinar Yanardag"
    ],
    "abstract": "We introduce LoRAShop, the first framework for multi-concept image editing with LoRA models. LoRAShop builds on a key observation about the feature interaction patterns inside Flux-style diffusion transformers: concept-specific transformer features activate spatially coherent regions early in the denoising process. We harness this observation to derive a disentangled latent mask for each concept in a prior forward pass and blend the corresponding LoRA weights only within regions bounding the concepts to be personalized. The resulting edits seamlessly integrate multiple subjects or styles into the original scene while preserving global context, lighting, and fine details. Our experiments demonstrate that LoRAShop delivers better identity preservation compared to baselines. By eliminating retraining and external constraints, LoRAShop turns personalized diffusion models into a practical `photoshop-with-LoRAs' tool and opens new avenues for compositional visual storytelling and rapid creative iteration.",
    "arxiv_url": "http://arxiv.org/abs/2505.23758v1",
    "pdf_url": "http://arxiv.org/pdf/2505.23758v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "rectified flow",
      "image editing",
      "image generation",
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model",
    "authors": [
      "Qingyu Shi",
      "Jinbin Bai",
      "Zhuoran Zhao",
      "Wenhao Chai",
      "Kaidong Yu",
      "Jianzong Wu",
      "Shuangyong Song",
      "Yunhai Tong",
      "Xiangtai Li",
      "Xuelong Li",
      "Shuicheng Yan"
    ],
    "abstract": "Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm. Autoregressive unified models suffer from slow inference due to sequential decoding, and non-autoregressive unified models suffer from weak generalization due to limited pretrained backbones. We introduce Muddit, a unified discrete diffusion transformer that enables fast and parallel generation across both text and image modalities. Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture. Empirical results show that Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency. The work highlights the potential of purely discrete diffusion, when equipped with strong visual priors, as a scalable and effective backbone for unified generation.",
    "arxiv_url": "http://arxiv.org/abs/2505.23606v1",
    "pdf_url": "http://arxiv.org/pdf/2505.23606v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer",
    "authors": [
      "Qi Cai",
      "Jingwen Chen",
      "Yang Chen",
      "Yehao Li",
      "Fuchen Long",
      "Yingwei Pan",
      "Zhaofan Qiu",
      "Yiheng Zhang",
      "Fengbin Gao",
      "Peihan Xu",
      "Yimeng Wang",
      "Kai Yu",
      "Wenxuan Chen",
      "Ziwei Feng",
      "Zijian Gong",
      "Jianzhuang Pan",
      "Yi Peng",
      "Rui Tian",
      "Siyu Wang",
      "Bo Zhao",
      "Ting Yao",
      "Tao Mei"
    ],
    "abstract": "Recent advancements in image generative foundation models have prioritized quality improvements but often at the cost of increased computational complexity and inference latency. To address this critical trade-off, we introduce HiDream-I1, a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds. HiDream-I1 is constructed with a new sparse Diffusion Transformer (DiT) structure. Specifically, it starts with a dual-stream decoupled design of sparse DiT with dynamic Mixture-of-Experts (MoE) architecture, in which two separate encoders are first involved to independently process image and text tokens. Then, a single-stream sparse DiT structure with dynamic MoE architecture is adopted to trigger multi-model interaction for image generation in a cost-efficient manner. To support flexiable accessibility with varied model capabilities, we provide HiDream-I1 in three variants: HiDream-I1-Full, HiDream-I1-Dev, and HiDream-I1-Fast.   Furthermore, we go beyond the typical text-to-image generation and remould HiDream-I1 with additional image conditions to perform precise, instruction-based editing on given images, yielding a new instruction-based image editing model namely HiDream-E1. Ultimately, by integrating text-to-image generation and instruction-based image editing, HiDream-I1 evolves to form a comprehensive image agent (HiDream-A1) capable of fully interactive image creation and refinement. To accelerate multi-modal AIGC research, we have open-sourced all the codes and model weights of HiDream-I1-Full, HiDream-I1-Dev, HiDream-I1-Fast, HiDream-E1 through our project websites: https://github.com/HiDream-ai/HiDream-I1 and https://github.com/HiDream-ai/HiDream-E1. All features can be directly experienced via https://vivago.ai/studio.",
    "arxiv_url": "http://arxiv.org/abs/2505.22705v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22705v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "https://github.com/HiDream-ai/HiDream-I1",
    "keywords": [
      "diffusion transformer",
      "text-to-image",
      "image editing",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers",
    "authors": [
      "Weilun Feng",
      "Chuanguang Yang",
      "Haotong Qin",
      "Xiangqi Li",
      "Yu Wang",
      "Zhulin An",
      "Libo Huang",
      "Boyu Diao",
      "Zixiang Zhao",
      "Yongjun Xu",
      "Michele Magno"
    ],
    "abstract": "Diffusion transformers (DiT) have demonstrated exceptional performance in video generation. However, their large number of parameters and high computational complexity limit their deployment on edge devices. Quantization can reduce storage requirements and accelerate inference by lowering the bit-width of model parameters. Yet, existing quantization methods for image generation models do not generalize well to video generation tasks. We identify two primary challenges: the loss of information during quantization and the misalignment between optimization objectives and the unique requirements of video generation. To address these challenges, we present Q-VDiT, a quantization framework specifically designed for video DiT models. From the quantization perspective, we propose the Token-aware Quantization Estimator (TQE), which compensates for quantization errors in both the token and feature dimensions. From the optimization perspective, we introduce Temporal Maintenance Distillation (TMD), which preserves the spatiotemporal correlations between frames and enables the optimization of each frame with respect to the overall video context. Our W3A6 Q-VDiT achieves a scene consistency of 23.40, setting a new benchmark and outperforming current state-of-the-art quantization methods by 1.9$\\times$. Code will be available at https://github.com/cantbebetter2/Q-VDiT.",
    "arxiv_url": "http://arxiv.org/abs/2505.22167v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22167v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/cantbebetter2/Q-VDiT",
    "keywords": [
      "diffusion transformer",
      "video generation",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AlignGen: Boosting Personalized Image Generation with Cross-Modality Prior Alignment",
    "authors": [
      "Yiheng Lin",
      "Shifang Zhao",
      "Ting Liu",
      "Xiaochao Qu",
      "Luoqi Liu",
      "Yao Zhao",
      "Yunchao Wei"
    ],
    "abstract": "Personalized image generation aims to integrate user-provided concepts into text-to-image models, enabling the generation of customized content based on a given prompt. Recent zero-shot approaches, particularly those leveraging diffusion transformers, incorporate reference image information through multi-modal attention mechanism. This integration allows the generated output to be influenced by both the textual prior from the prompt and the visual prior from the reference image. However, we observe that when the prompt and reference image are misaligned, the generated results exhibit a stronger bias toward the textual prior, leading to a significant loss of reference content. To address this issue, we propose AlignGen, a Cross-Modality Prior Alignment mechanism that enhances personalized image generation by: 1) introducing a learnable token to bridge the gap between the textual and visual priors, 2) incorporating a robust training strategy to ensure proper prior alignment, and 3) employing a selective cross-modal attention mask within the multi-modal attention mechanism to further align the priors. Experimental results demonstrate that AlignGen outperforms existing zero-shot methods and even surpasses popular test-time optimization approaches.",
    "arxiv_url": "http://arxiv.org/abs/2505.21911v1",
    "pdf_url": "http://arxiv.org/pdf/2505.21911v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Frame In-N-Out: Unbounded Controllable Image-to-Video Generation",
    "authors": [
      "Boyang Wang",
      "Xuweiyi Chen",
      "Matheus Gadelha",
      "Zezhou Cheng"
    ],
    "abstract": "Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can control the objects in the image to naturally leave the scene or provide breaking new identity references to enter the scene, guided by user-specified motion trajectory. To support this task, we introduce a new dataset curated semi-automatically, a comprehensive evaluation protocol targeting this setting, and an efficient identity-preserving motion-controllable video Diffusion Transformer architecture. Our evaluation shows that our proposed approach significantly outperforms existing baselines.",
    "arxiv_url": "http://arxiv.org/abs/2505.21491v1",
    "pdf_url": "http://arxiv.org/pdf/2505.21491v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "Controllable",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Minute-Long Videos with Dual Parallelisms",
    "authors": [
      "Zeqing Wang",
      "Bowen Zheng",
      "Xingyi Yang",
      "Zhenxiong Tan",
      "Yuecong Xu",
      "Xinchao Wang"
    ],
    "abstract": "Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating an entire video on a single GPU, we parallelize both temporal frames and model layers across GPUs. However, a naive implementation of this division faces a key limitation: since diffusion models require synchronized noise levels across frames, this implementation leads to the serialization of original parallelisms. We leverage a block-wise denoising scheme to handle this. Namely, we process a sequence of frame blocks through the pipeline with progressively decreasing noise levels. Each GPU handles a specific block and layer subset while passing previous results to the next GPU, enabling asynchronous computation and communication. To further optimize performance, we incorporate two key enhancements. Firstly, a feature cache is implemented on each GPU to store and reuse features from the prior block as context, minimizing inter-GPU communication and redundant computation. Secondly, we employ a coordinated noise initialization strategy, ensuring globally consistent temporal dynamics by sharing initial noise patterns across GPUs without extra resource costs. Together, these enable fast, artifact-free, and infinitely long video generation. Applied to the latest diffusion transformer video generator, our method efficiently produces 1,025-frame videos with up to 6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX 4090 GPUs.",
    "arxiv_url": "http://arxiv.org/abs/2505.21070v2",
    "pdf_url": "http://arxiv.org/pdf/2505.21070v2",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy",
    "authors": [
      "Aiyue Chen",
      "Bin Dong",
      "Jingru Li",
      "Jing Lin",
      "Yiwu Yao",
      "Gongyi Wang"
    ],
    "abstract": "Video generation using diffusion models is highly computationally intensive, with 3D attention in Diffusion Transformer (DiT) models accounting for over 80\\% of the total computational resources. In this work, we introduce {\\bf RainFusion}, a novel training-free sparse attention method that exploits inherent sparsity nature in visual data to accelerate attention computation while preserving video quality. Specifically, we identify three unique sparse patterns in video generation attention calculations--Spatial Pattern, Temporal Pattern and Textural Pattern. The sparse pattern for each attention head is determined online with negligible overhead (\\textasciitilde\\,0.2\\%) with our proposed {\\bf ARM} (Adaptive Recognition Module) during inference. Our proposed {\\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated into state-of-the-art 3D-attention video generation models without additional training or calibration. We evaluate our method on leading open-sourced models including HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its broad applicability and effectiveness. Experimental results show that RainFusion achieves over {\\bf 2\\(\\times\\)} speedup in attention computation while maintaining video quality, with only a minimal impact on VBench scores (-0.2\\%).",
    "arxiv_url": "http://arxiv.org/abs/2505.21036v1",
    "pdf_url": "http://arxiv.org/pdf/2505.21036v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hierarchical Masked Autoregressive Models with Low-Resolution Token Pivots",
    "authors": [
      "Guangting Zheng",
      "Yehao Li",
      "Yingwei Pan",
      "Jiajun Deng",
      "Ting Yao",
      "Yanyong Zhang",
      "Tao Mei"
    ],
    "abstract": "Autoregressive models have emerged as a powerful generative paradigm for visual generation. The current de-facto standard of next token prediction commonly operates over a single-scale sequence of dense image tokens, and is incapable of utilizing global context especially for early tokens prediction. In this paper, we introduce a new autoregressive design to model a hierarchy from a few low-resolution image tokens to the typical dense image tokens, and delve into a thorough hierarchical dependency across multi-scale image tokens. Technically, we present a Hierarchical Masked Autoregressive models (Hi-MAR) that pivot on low-resolution image tokens to trigger hierarchical autoregressive modeling in a multi-phase manner. Hi-MAR learns to predict a few image tokens in low resolution, functioning as intermediary pivots to reflect global structure, in the first phase. Such pivots act as the additional guidance to strengthen the next autoregressive modeling phase by shaping global structural awareness of typical dense image tokens. A new Diffusion Transformer head is further devised to amplify the global context among all tokens for mask token prediction. Extensive evaluations on both class-conditional and text-to-image generation tasks demonstrate that Hi-MAR outperforms typical AR baselines, while requiring fewer computational costs. Code is available at https://github.com/HiDream-ai/himar.",
    "arxiv_url": "http://arxiv.org/abs/2505.20288v1",
    "pdf_url": "http://arxiv.org/pdf/2505.20288v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "https://github.com/HiDream-ai/himar",
    "keywords": [
      "diffusion transformer",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM",
    "authors": [
      "Peng Liu",
      "Xiaoming Ren",
      "Fengkai Liu",
      "Qingsong Xie",
      "Quanlong Zheng",
      "Yanhao Zhang",
      "Haonan Lu",
      "Yujiu Yang"
    ],
    "abstract": "Recent advancements in image-to-video (I2V) generation have shown promising performance in conventional scenarios. However, these methods still encounter significant challenges when dealing with complex scenes that require a deep understanding of nuanced motion and intricate object-action relationships. To address these challenges, we present Dynamic-I2V, an innovative framework that integrates Multimodal Large Language Models (MLLMs) to jointly encode visual and textual conditions for a diffusion transformer (DiT) architecture. By leveraging the advanced multimodal understanding capabilities of MLLMs, our model significantly improves motion controllability and temporal coherence in synthesized videos. The inherent multimodality of Dynamic-I2V further enables flexible support for diverse conditional inputs, extending its applicability to various downstream generation tasks. Through systematic analysis, we identify a critical limitation in current I2V benchmarks: a significant bias towards favoring low-dynamic videos, stemming from an inadequate balance between motion complexity and visual quality metrics. To resolve this evaluation gap, we propose DIVE - a novel assessment benchmark specifically designed for comprehensive dynamic quality measurement in I2V generation. In conclusion, extensive quantitative and qualitative experiments confirm that Dynamic-I2V attains state-of-the-art performance in image-to-video generation, particularly revealing significant improvements of 42.5%, 7.9%, and 11.8% in dynamic range, controllability, and quality, respectively, as assessed by the DIVE metric in comparison to existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2505.19901v3",
    "pdf_url": "http://arxiv.org/pdf/2505.19901v3",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "The Role of Video Generation in Enhancing Data-Limited Action Understanding",
    "authors": [
      "Wei Li",
      "Dezhao Luo",
      "Dongbao Yang",
      "Zhenhang Li",
      "Weiping Wang",
      "Yu Zhou"
    ],
    "abstract": "Video action understanding tasks in real-world scenarios always suffer data limitations. In this paper, we address the data-limited action understanding problem by bridging data scarcity. We propose a novel method that employs a text-to-video diffusion transformer to generate annotated data for model training. This paradigm enables the generation of realistic annotated data on an infinite scale without human intervention. We proposed the information enhancement strategy and the uncertainty-based label smoothing tailored to generate sample training. Through quantitative and qualitative analysis, we observed that real samples generally contain a richer level of information than generated samples. Based on this observation, the information enhancement strategy is proposed to enhance the informative content of the generated samples from two aspects: the environments and the characters. Furthermore, we observed that some low-quality generated samples might negatively affect model training. To address this, we devised the uncertainty-based label smoothing strategy to increase the smoothing of these samples, thus reducing their impact. We demonstrate the effectiveness of the proposed method on four datasets across five tasks and achieve state-of-the-art performance for zero-shot action recognition.",
    "arxiv_url": "http://arxiv.org/abs/2505.19495v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19495v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models",
    "authors": [
      "Hang Hua",
      "Ziyun Zeng",
      "Yizhi Song",
      "Yunlong Tang",
      "Liu He",
      "Daniel Aliaga",
      "Wei Xiong",
      "Jiebo Luo"
    ],
    "abstract": "Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and Gemini 2.5 Pro excel at following complex instructions, editing images and maintaining concept consistency. However, they are still evaluated by disjoint toolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning, and customized image generation benchmarks that overlook compositional semantics and common knowledge. We propose MMIG-Bench, a comprehensive Multi-Modal Image Generation Benchmark that unifies these tasks by pairing 4,850 richly annotated text prompts with 1,750 multi-view reference images across 380 subjects, spanning humans, animals, objects, and artistic styles. MMIG-Bench is equipped with a three-level evaluation framework: (1) low-level metrics for visual artifacts and identity preservation of objects; (2) novel Aspect Matching Score (AMS): a VQA-based mid-level metric that delivers fine-grained prompt-image alignment and shows strong correlation with human judgments; and (3) high-level metrics for aesthetics and human preference. Using MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5 Pro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human ratings, yielding in-depth insights into architecture and data design.",
    "arxiv_url": "http://arxiv.org/abs/2505.19415v2",
    "pdf_url": "http://arxiv.org/pdf/2505.19415v2",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "FLUX",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation",
    "authors": [
      "Shenggan Cheng",
      "Yuanxin Wei",
      "Lansong Diao",
      "Yong Liu",
      "Bujiao Chen",
      "Lianghua Huang",
      "Yu Liu",
      "Wenyuan Yu",
      "Jiangsu Du",
      "Wei Lin",
      "Yang You"
    ],
    "abstract": "Leveraging the diffusion transformer (DiT) architecture, models like Sora, CogVideoX and Wan have achieved remarkable progress in text-to-video, image-to-video, and video editing tasks. Despite these advances, diffusion-based video generation remains computationally intensive, especially for high-resolution, long-duration videos. Prior work accelerates its inference by skipping computation, usually at the cost of severe quality degradation. In this paper, we propose SRDiffusion, a novel framework that leverages collaboration between large and small models to reduce inference cost. The large model handles high-noise steps to ensure semantic and motion fidelity (Sketching), while the smaller model refines visual details in low-noise steps (Rendering). Experimental results demonstrate that our method outperforms existing approaches, over 3$\\times$ speedup for Wan with nearly no quality loss for VBench, and 2$\\times$ speedup for CogVideoX. Our method is introduced as a new direction orthogonal to existing acceleration strategies, offering a practical solution for scalable video generation.",
    "arxiv_url": "http://arxiv.org/abs/2505.19151v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19151v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation",
      "video editing"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation",
    "authors": [
      "Shuo Yang",
      "Haocheng Xi",
      "Yilong Zhao",
      "Muyang Li",
      "Jintao Zhang",
      "Han Cai",
      "Yujun Lin",
      "Xiuyu Li",
      "Chenfeng Xu",
      "Kelly Peng",
      "Jianfei Chen",
      "Song Han",
      "Kurt Keutzer",
      "Ion Stoica"
    ],
    "abstract": "Diffusion Transformers (DiTs) are essential for video generation but suffer from significant latency due to the quadratic complexity of attention. By computing only critical tokens, sparse attention reduces computational costs and offers a promising acceleration approach. However, we identify that existing methods fail to approach optimal generation quality under the same computation budget for two reasons: (1) Inaccurate critical token identification: current methods cluster tokens based on position rather than semantics, leading to imprecise aggregated representations. (2) Excessive computation waste: critical tokens are scattered among non-critical ones, leading to wasted computation on GPUs, which are optimized for processing contiguous tokens. In this paper, we propose SVG2, a training-free framework that maximizes identification accuracy and minimizes computation waste, achieving a Pareto frontier trade-off between generation quality and efficiency. The core of SVG2 is semantic-aware permutation, which clusters and reorders tokens based on semantic similarity using k-means. This approach ensures both a precise cluster representation, improving identification accuracy, and a densified layout of critical tokens, enabling efficient computation without padding. Additionally, SVG2 integrates top-p dynamic budget control and customized kernel implementations, achieving up to 2.30x and 1.89x speedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan 2.1, respectively.",
    "arxiv_url": "http://arxiv.org/abs/2505.18875v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18875v1",
    "published_date": "2025-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
    "authors": [
      "Wenhao Sun",
      "Rong-Cheng Tu",
      "Yifu Ding",
      "Zhao Jin",
      "Jingyi Liao",
      "Shunyu Liu",
      "Dacheng Tao"
    ],
    "abstract": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in high-quality video generation, but remain computationally expensive due to the quadratic complexity of attention over high-dimensional video sequences. Recent attention acceleration methods leverage the sparsity of attention patterns to improve efficiency; however, they often overlook inefficiencies of redundant long-range interactions. To address this problem, we propose \\textbf{VORTA}, an acceleration framework with two novel components: 1) a sparse attention mechanism that efficiently captures long-range dependencies, and 2) a routing strategy that adaptively replaces full 3D attention with specialized sparse attention variants throughout the sampling process. It achieves a $1.76\\times$ end-to-end speedup without quality loss on VBench. Furthermore, VORTA can seamlessly integrate with various other acceleration methods, such as caching and step distillation, reaching up to $14.41\\times$ speedup with negligible performance degradation. VORTA demonstrates its efficiency and enhances the practicality of VDiTs in real-world settings.",
    "arxiv_url": "http://arxiv.org/abs/2505.18809v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18809v1",
    "published_date": "2025-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DVD-Quant: Data-free Video Diffusion Transformers Quantization",
    "authors": [
      "Zhiteng Li",
      "Hanxuan Li",
      "Junyi Wu",
      "Kai Liu",
      "Linghe Kong",
      "Guihai Chen",
      "Yulun Zhang",
      "Xiaokang Yang"
    ],
    "abstract": "Diffusion Transformers (DiTs) have emerged as the state-of-the-art architecture for video generation, yet their computational and memory demands hinder practical deployment. While post-training quantization (PTQ) presents a promising approach to accelerate Video DiT models, existing methods suffer from two critical limitations: (1) dependence on lengthy, computation-heavy calibration procedures, and (2) considerable performance deterioration after quantization. To address these challenges, we propose DVD-Quant, a novel Data-free quantization framework for Video DiTs. Our approach integrates three key innovations: (1) Progressive Bounded Quantization (PBQ) and (2) Auto-scaling Rotated Quantization (ARQ) for calibration data-free quantization error reduction, as well as (3) $\\delta$-Guided Bit Switching ($\\delta$-GBS) for adaptive bit-width allocation. Extensive experiments across multiple video generation benchmarks demonstrate that DVD-Quant achieves an approximately 2$\\times$ speedup over full-precision baselines on HunyuanVideo while maintaining visual fidelity. Notably, DVD-Quant is the first to enable W4A4 PTQ for Video DiTs without compromising video quality. Code and models will be available at https://github.com/lhxcs/DVD-Quant.",
    "arxiv_url": "http://arxiv.org/abs/2505.18663v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18663v1",
    "published_date": "2025-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/lhxcs/DVD-Quant",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mod-Adapter: Tuning-Free and Versatile Multi-concept Personalization via Modulation Adapter",
    "authors": [
      "Weizhi Zhong",
      "Huan Yang",
      "Zheng Liu",
      "Huiguo He",
      "Zijian He",
      "Xuesong Niu",
      "Di Zhang",
      "Guanbin Li"
    ],
    "abstract": "Personalized text-to-image generation aims to synthesize images of user-provided concepts in diverse contexts. Despite recent progress in multi-concept personalization, most are limited to object concepts and struggle to customize abstract concepts (e.g., pose, lighting). Some methods have begun exploring multi-concept personalization supporting abstract concepts, but they require test-time fine-tuning for each new concept, which is time-consuming and prone to overfitting on limited training images. In this work, we propose a novel tuning-free method for multi-concept personalization that can effectively customize both object and abstract concepts without test-time fine-tuning. Our method builds upon the modulation mechanism in pretrained Diffusion Transformers (DiTs) model, leveraging the localized and semantically meaningful properties of the modulation space. Specifically, we propose a novel module, Mod-Adapter, to predict concept-specific modulation direction for the modulation process of concept-related text tokens. It incorporates vision-language cross-attention for extracting concept visual features, and Mixture-of-Experts (MoE) layers that adaptively map the concept features into the modulation space. Furthermore, to mitigate the training difficulty caused by the large gap between the concept image space and the modulation space, we introduce a VLM-guided pretraining strategy that leverages the strong image understanding capabilities of vision-language models to provide semantic supervision signals. For a comprehensive comparison, we extend a standard benchmark by incorporating abstract concepts. Our method achieves state-of-the-art performance in multi-concept personalization, supported by quantitative, qualitative, and human evaluations.",
    "arxiv_url": "http://arxiv.org/abs/2505.18612v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18612v1",
    "published_date": "2025-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration",
    "authors": [
      "Sudarshan Rajagopalan",
      "Kartik Narayan",
      "Vishal M. Patel"
    ],
    "abstract": "The use of latent diffusion models (LDMs) such as Stable Diffusion has significantly improved the perceptual quality of All-in-One image Restoration (AiOR) methods, while also enhancing their generalization capabilities. However, these LDM-based frameworks suffer from slow inference due to their iterative denoising process, rendering them impractical for time-sensitive applications. To address this, we propose RestoreVAR, a novel generative approach for AiOR that significantly outperforms LDM-based models in restoration performance while achieving over $\\mathbf{10\\times}$ faster inference. RestoreVAR leverages visual autoregressive modeling (VAR), a recently introduced approach which performs scale-space autoregression for image generation. VAR achieves comparable performance to that of state-of-the-art diffusion transformers with drastically reduced computational costs. To optimally exploit these advantages of VAR for AiOR, we propose architectural modifications and improvements, including intricately designed cross-attention mechanisms and a latent-space refinement module, tailored for the AiOR task. Extensive experiments show that RestoreVAR achieves state-of-the-art performance among generative AiOR methods, while also exhibiting strong generalization capabilities.",
    "arxiv_url": "http://arxiv.org/abs/2505.18047v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18047v1",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Training-Free Efficient Video Generation via Dynamic Token Carving",
    "authors": [
      "Yuechen Zhang",
      "Jinbo Xing",
      "Bin Xia",
      "Shaoteng Liu",
      "Bohao Peng",
      "Xin Tao",
      "Pengfei Wan",
      "Eric Lo",
      "Jiaya Jia"
    ],
    "abstract": "Despite the remarkable generation quality of video Diffusion Transformer (DiT) models, their practical deployment is severely hindered by extensive computational requirements. This inefficiency stems from two key challenges: the quadratic complexity of self-attention with respect to token length and the multi-step nature of diffusion models. To address these limitations, we present Jenga, a novel inference pipeline that combines dynamic attention carving with progressive resolution generation. Our approach leverages two key insights: (1) early denoising steps do not require high-resolution latents, and (2) later steps do not require dense attention. Jenga introduces a block-wise attention mechanism that dynamically selects relevant token interactions using 3D space-filling curves, alongside a progressive resolution strategy that gradually increases latent resolution during generation. Experimental results demonstrate that Jenga achieves substantial speedups across multiple state-of-the-art video diffusion models while maintaining comparable generation quality (8.83$\\times$ speedup with 0.01\\% performance drop on VBench). As a plug-and-play solution, Jenga enables practical, high-quality video generation on modern hardware by reducing inference time from minutes to seconds -- without requiring model retraining. Code: https://github.com/dvlab-research/Jenga",
    "arxiv_url": "http://arxiv.org/abs/2505.16864v1",
    "pdf_url": "http://arxiv.org/pdf/2505.16864v1",
    "published_date": "2025-05-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/dvlab-research/Jenga",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Interspatial Attention for Efficient 4D Human Video Generation",
    "authors": [
      "Ruizhi Shao",
      "Yinghao Xu",
      "Yujun Shen",
      "Ceyuan Yang",
      "Yang Zheng",
      "Changan Chen",
      "Yebin Liu",
      "Gordon Wetzstein"
    ],
    "abstract": "Generating photorealistic videos of digital humans in a controllable manner is crucial for a plethora of applications. Existing approaches either build on methods that employ template-based 3D representations or emerging video generation models but suffer from poor quality or limited consistency and identity preservation when generating individual or multiple digital humans. In this paper, we introduce a new interspatial attention (ISA) mechanism as a scalable building block for modern diffusion transformer (DiT)--based video generation models. ISA is a new type of cross attention that uses relative positional encodings tailored for the generation of human videos. Leveraging a custom-developed video variation autoencoder, we train a latent ISA-based diffusion model on a large corpus of video data. Our model achieves state-of-the-art performance for 4D human video synthesis, demonstrating remarkable motion consistency and identity preservation while providing precise control of the camera and body poses. Our code and model are publicly released at https://dsaurus.github.io/isa4d/.",
    "arxiv_url": "http://arxiv.org/abs/2505.15800v2",
    "pdf_url": "http://arxiv.org/pdf/2505.15800v2",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "Controllable",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scaling Diffusion Transformers Efficiently via $$P",
    "authors": [
      "Chenyu Zheng",
      "Xinyu Zhang",
      "Rongzhen Wang",
      "Wei Huang",
      "Zhi Tian",
      "Weilin Huang",
      "Jun Zhu",
      "Chongxuan Li"
    ],
    "abstract": "Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization ($\\mu$P) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether $\\mu$P of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard $\\mu$P to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that $\\mu$P of mainstream diffusion Transformers, including DiT, U-ViT, PixArt-$\\alpha$, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing $\\mu$P methodologies. Leveraging this result, we systematically demonstrate that DiT-$\\mu$P enjoys robust HP transferability. Notably, DiT-XL-2-$\\mu$P with transferred learning rate achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of $\\mu$P on text-to-image generation by scaling PixArt-$\\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under $\\mu$P outperform their respective baselines while requiring small tuning cost, only 5.5% of one training run for PixArt-$\\alpha$ and 3% of consumption by human experts for MMDiT-18B. These results establish $\\mu$P as a principled and efficient framework for scaling diffusion Transformers.",
    "arxiv_url": "http://arxiv.org/abs/2505.15270v1",
    "pdf_url": "http://arxiv.org/pdf/2505.15270v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Grouping First, Attending Smartly: Training-Free Acceleration for Diffusion Transformers",
    "authors": [
      "Sucheng Ren",
      "Qihang Yu",
      "Ju He",
      "Alan Yuille",
      "Liang-Chieh Chen"
    ],
    "abstract": "Diffusion-based Transformers have demonstrated impressive generative capabilities, but their high computational costs hinder practical deployment, for example, generating an $8192\\times 8192$ image can take over an hour on an A100 GPU. In this work, we propose GRAT (\\textbf{GR}ouping first, \\textbf{AT}tending smartly), a training-free attention acceleration strategy for fast image and video generation without compromising output quality. The key insight is to exploit the inherent sparsity in learned attention maps (which tend to be locally focused) in pretrained Diffusion Transformers and leverage better GPU parallelism. Specifically, GRAT first partitions contiguous tokens into non-overlapping groups, aligning both with GPU execution patterns and the local attention structures learned in pretrained generative Transformers. It then accelerates attention by having all query tokens within the same group share a common set of attendable key and value tokens. These key and value tokens are further restricted to structured regions, such as surrounding blocks or criss-cross regions, significantly reducing computational overhead (e.g., attaining a \\textbf{35.8$\\times$} speedup over full attention when generating $8192\\times 8192$ images) while preserving essential attention patterns and long-range context. We validate GRAT on pretrained Flux and HunyuanVideo for image and video generation, respectively. In both cases, GRAT achieves substantially faster inference without any fine-tuning, while maintaining the performance of full attention. We hope GRAT will inspire future research on accelerating Diffusion Transformers for scalable visual generation.",
    "arxiv_url": "http://arxiv.org/abs/2505.14687v1",
    "pdf_url": "http://arxiv.org/pdf/2505.14687v1",
    "published_date": "2025-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation",
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LMP: Leveraging Motion Prior in Zero-Shot Video Generation with Diffusion Transformer",
    "authors": [
      "Changgu Chen",
      "Xiaoyan Yang",
      "Junwei Shu",
      "Changbo Wang",
      "Yang Li"
    ],
    "abstract": "In recent years, large-scale pre-trained diffusion transformer models have made significant progress in video generation. While current DiT models can produce high-definition, high-frame-rate, and highly diverse videos, there is a lack of fine-grained control over the video content. Controlling the motion of subjects in videos using only prompts is challenging, especially when it comes to describing complex movements. Further, existing methods fail to control the motion in image-to-video generation, as the subject in the reference image often differs from the subject in the reference video in terms of initial position, size, and shape. To address this, we propose the Leveraging Motion Prior (LMP) framework for zero-shot video generation. Our framework harnesses the powerful generative capabilities of pre-trained diffusion transformers to enable motion in the generated videos to reference user-provided motion videos in both text-to-video and image-to-video generation. To this end, we first introduce a foreground-background disentangle module to distinguish between moving subjects and backgrounds in the reference video, preventing interference in the target video generation. A reweighted motion transfer module is designed to allow the target video to reference the motion from the reference video. To avoid interference from the subject in the reference video, we propose an appearance separation module to suppress the appearance of the reference subject in the target video. We annotate the DAVIS dataset with detailed prompts for our experiments and design evaluation metrics to validate the effectiveness of our method. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in generation quality, prompt-video consistency, and control capability. Our homepage is available at https://vpx-ecnu.github.io/LMP-Website/",
    "arxiv_url": "http://arxiv.org/abs/2505.14167v1",
    "pdf_url": "http://arxiv.org/pdf/2505.14167v1",
    "published_date": "2025-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Swin DiT: Diffusion Transformer using Pseudo Shifted Windows",
    "authors": [
      "Jiafu Wu",
      "Yabiao Wang",
      "Jian Li",
      "Jinlong Peng",
      "Yun Cao",
      "Chengjie Wang",
      "Jiangning Zhang"
    ],
    "abstract": "Diffusion Transformers (DiTs) achieve remarkable performance within the domain of image generation through the incorporation of the transformer architecture. Conventionally, DiTs are constructed by stacking serial isotropic global information modeling transformers, which face significant computational cost when processing high-resolution images. We empirically analyze that latent space image generation does not exhibit a strong dependence on global information as traditionally assumed. Most of the layers in the model demonstrate redundancy in global computation. In addition, conventional attention mechanisms exhibit low-frequency inertia issues. To address these issues, we propose \\textbf{P}seudo \\textbf{S}hifted \\textbf{W}indow \\textbf{A}ttention (PSWA), which fundamentally mitigates global model redundancy. PSWA achieves intermediate global-local information interaction through window attention, while employing a high-frequency bridging branch to simulate shifted window operations, supplementing appropriate global and high-frequency information. Furthermore, we propose the Progressive Coverage Channel Allocation(PCCA) strategy that captures high-order attention similarity without additional computational cost. Building upon all of them, we propose a series of Pseudo \\textbf{S}hifted \\textbf{Win}dow DiTs (\\textbf{Swin DiT}), accompanied by extensive experiments demonstrating their superior performance. For example, our proposed Swin-DiT-L achieves a 54%$\\uparrow$ FID improvement over DiT-XL/2 while requiring less computational. https://github.com/wujiafu007/Swin-DiT",
    "arxiv_url": "http://arxiv.org/abs/2505.13219v2",
    "pdf_url": "http://arxiv.org/pdf/2505.13219v2",
    "published_date": "2025-05-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/wujiafu007/Swin-DiT",
    "keywords": [
      "diffusion transformer",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SounDiT: Geo-Contextual Soundscape-to-Landscape Generation",
    "authors": [
      "Junbo Wang",
      "Haofeng Tan",
      "Bowen Liao",
      "Albert Jiang",
      "Teng Fei",
      "Qixing Huang",
      "Zhengzhong Tu",
      "Shan Ye",
      "Yuhao Kang"
    ],
    "abstract": "We present a novel and practically significant problem-Geo-Contextual Soundscape-to-Landscape (GeoS2L) generation-which aims to synthesize geographically realistic landscape images from environmental soundscapes. Prior audio-to-image generation methods typically rely on general-purpose datasets and overlook geographic and environmental contexts, resulting in unrealistic images that are misaligned with real-world environmental settings. To address this limitation, we introduce a novel geo-contextual computational framework that explicitly integrates geographic knowledge into multimodal generative modeling. We construct two large-scale geo-contextual multimodal datasets, SoundingSVI and SonicUrban, pairing diverse soundscapes with real-world landscape images. We propose SounDiT, a novel Diffusion Transformer (DiT)-based model that incorporates geo-contextual scene conditioning to synthesize geographically coherent landscape images. Furthermore, we propose a practically-informed geo-contextual evaluation framework, the Place Similarity Score (PSS), across element-, scene-, and human perception-levels to measure consistency between input soundscapes and generated landscape images. Extensive experiments demonstrate that SounDiT outperforms existing baselines in both visual fidelity and geographic settings. Our work not only establishes foundational benchmarks for GeoS2L generation but also highlights the importance of incorporating geographic domain knowledge in advancing multimodal generative models, opening new directions at the intersection of generative AI, geography, urban planning, and environmental sciences.",
    "arxiv_url": "http://arxiv.org/abs/2505.12734v1",
    "pdf_url": "http://arxiv.org/pdf/2505.12734v1",
    "published_date": "2025-05-19",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.GR",
      "cs.HC",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance",
    "authors": [
      "Xuan Shen",
      "Chenxia Han",
      "Yufa Zhou",
      "Yanyue Xie",
      "Yifan Gong",
      "Quanyi Wang",
      "Yiwei Wang",
      "Yanzhi Wang",
      "Pu Zhao",
      "Jiuxiang Gu"
    ],
    "abstract": "Diffusion transformer-based video generation models (DiTs) have recently attracted widespread attention for their excellent generation quality. However, their computational cost remains a major bottleneck-attention alone accounts for over 80% of total latency, and generating just 8 seconds of 720p video takes tens of minutes-posing serious challenges to practical application and scalability. To address this, we propose the DraftAttention, a training-free framework for the acceleration of video diffusion transformers with dynamic sparse attention on GPUs. We apply down-sampling to each feature map across frames in the compressed latent space, enabling a higher-level receptive field over the latent composed of hundreds of thousands of tokens. The low-resolution draft attention map, derived from draft query and key, exposes redundancy both spatially within each feature map and temporally across frames. We reorder the query, key, and value based on the draft attention map to guide the sparse attention computation in full resolution, and subsequently restore their original order after the attention computation. This reordering enables structured sparsity that aligns with hardware-optimized execution. Our theoretical analysis demonstrates that the low-resolution draft attention closely approximates the full attention, providing reliable guidance for constructing accurate sparse attention. Experimental results show that our method outperforms existing sparse attention approaches in video generation quality and achieves up to 1.75x end-to-end speedup on GPUs. Code: https://github.com/shawnricecake/draft-attention",
    "arxiv_url": "http://arxiv.org/abs/2505.14708v1",
    "pdf_url": "http://arxiv.org/pdf/2505.14708v1",
    "published_date": "2025-05-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/shawnricecake/draft-attention",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis",
    "authors": [
      "Bingda Tang",
      "Boyang Zheng",
      "Xichen Pan",
      "Sayak Paul",
      "Saining Xie"
    ],
    "abstract": "This paper does not describe a new method; instead, it provides a thorough exploration of an important yet understudied design space related to recent advances in text-to-image synthesis -- specifically, the deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for multi-modal generation. Previous studies mainly focused on overall system performance rather than detailed comparisons with alternative methods, and key design details and training recipes were often left undisclosed. These gaps create uncertainty about the real potential of this approach. To fill these gaps, we conduct an empirical study on text-to-image generation, performing controlled comparisons with established baselines, analyzing important design choices, and providing a clear, reproducible recipe for training at scale. We hope this work offers meaningful data points and practical guidelines for future research in multi-modal generation.",
    "arxiv_url": "http://arxiv.org/abs/2505.10046v1",
    "pdf_url": "http://arxiv.org/pdf/2505.10046v1",
    "published_date": "2025-05-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset",
    "authors": [
      "Jiuhai Chen",
      "Zhiyang Xu",
      "Xichen Pan",
      "Yushi Hu",
      "Can Qin",
      "Tom Goldstein",
      "Lifu Huang",
      "Tianyi Zhou",
      "Saining Xie",
      "Silvio Savarese",
      "Le Xue",
      "Caiming Xiong",
      "Ran Xu"
    ],
    "abstract": "Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.",
    "arxiv_url": "http://arxiv.org/abs/2505.09568v1",
    "pdf_url": "http://arxiv.org/pdf/2505.09568v1",
    "published_date": "2025-05-14",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mini Diffuser: Fast Multi-task Diffusion Policy Training Using Two-level Mini-batches",
    "authors": [
      "Yutong Hu",
      "Pinhao Song",
      "Kehan Wen",
      "Renaud Detry"
    ],
    "abstract": "We present a method that reduces, by an order of magnitude, the time and memory needed to train multi-task vision-language robotic diffusion policies. This improvement arises from a previously underexplored distinction between action diffusion and the image diffusion techniques that inspired it: In image generation, the target is high-dimensional. By contrast, in action generation, the dimensionality of the target is comparatively small, and only the image condition is high-dimensional. Our approach, \\emph{Mini Diffuser}, exploits this asymmetry by introducing \\emph{two-level minibatching}, which pairs multiple noised action samples with each vision-language condition, instead of the conventional one-to-one sampling strategy. To support this batching scheme, we introduce architectural adaptations to the diffusion transformer that prevent information leakage across samples while maintaining full conditioning access. In RLBench simulations, Mini-Diffuser achieves 95\\% of the performance of state-of-the-art multi-task diffusion policies, while using only 5\\% of the training time and 7\\% of the memory. Real-world experiments further validate that Mini-Diffuser preserves the key strengths of diffusion-based policies, including the ability to model multimodal action distributions and produce behavior conditioned on diverse perceptual inputs. Code available at mini-diffuse-actor.github.io",
    "arxiv_url": "http://arxiv.org/abs/2505.09430v2",
    "pdf_url": "http://arxiv.org/pdf/2505.09430v2",
    "published_date": "2025-05-14",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DanceGRPO: Unleashing GRPO on Visual Generation",
    "authors": [
      "Zeyue Xue",
      "Jie Wu",
      "Yu Gao",
      "Fangyuan Kong",
      "Lingting Zhu",
      "Mengzhao Chen",
      "Zhiheng Liu",
      "Wei Liu",
      "Qiushan Guo",
      "Weilin Huang",
      "Ping Luo"
    ],
    "abstract": "Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2505.07818v1",
    "pdf_url": "http://arxiv.org/pdf/2505.07818v1",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "rectified flow",
      "FLUX",
      "video generation",
      "text-to-image"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generative Pre-trained Autoregressive Diffusion Transformer",
    "authors": [
      "Yuan Zhang",
      "Jiacheng Jiang",
      "Guoqing Ma",
      "Zhiying Lu",
      "Haoyang Huang",
      "Jianlong Yuan",
      "Nan Duan"
    ],
    "abstract": "In this work, we present GPDiT, a Generative Pre-trained Autoregressive Diffusion Transformer that unifies the strengths of diffusion and autoregressive modeling for long-range video synthesis, within a continuous latent space. Instead of predicting discrete tokens, GPDiT autoregressively predicts future latent frames using a diffusion loss, enabling natural modeling of motion dynamics and semantic consistency across frames. This continuous autoregressive framework not only enhances generation quality but also endows the model with representation capabilities. Additionally, we introduce a lightweight causal attention variant and a parameter-free rotation-based time-conditioning mechanism, improving both the training and inference efficiency. Extensive experiments demonstrate that GPDiT achieves strong performance in video generation quality, video representation ability, and few-shot learning tasks, highlighting its potential as an effective framework for video modeling in continuous space.",
    "arxiv_url": "http://arxiv.org/abs/2505.07344v4",
    "pdf_url": "http://arxiv.org/pdf/2505.07344v4",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition",
    "authors": [
      "Zhiyuan Chen",
      "Keyi Li",
      "Yifan Jia",
      "Le Ye",
      "Yufei Ma"
    ],
    "abstract": "Diffusion transformer (DiT) models have achieved remarkable success in image generation, thanks for their exceptional generative capabilities and scalability. Nonetheless, the iterative nature of diffusion models (DMs) results in high computation complexity, posing challenges for deployment. Although existing cache-based acceleration methods try to utilize the inherent temporal similarity to skip redundant computations of DiT, the lack of correction may induce potential quality degradation. In this paper, we propose increment-calibrated caching, a training-free method for DiT acceleration, where the calibration parameters are generated from the pre-trained model itself with low-rank approximation. To deal with the possible correction failure arising from outlier activations, we introduce channel-aware Singular Value Decomposition (SVD), which further strengthens the calibration effect. Experimental results show that our method always achieve better performance than existing naive caching methods with a similar computation resource budget. When compared with 35-step DDIM, our method eliminates more than 45% computation and improves IS by 12 at the cost of less than 0.06 FID increase. Code is available at https://github.com/ccccczzy/icc.",
    "arxiv_url": "http://arxiv.org/abs/2505.05829v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05829v1",
    "published_date": "2025-05-09",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "https://github.com/ccccczzy/icc",
    "keywords": [
      "diffusion transformer",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers",
    "authors": [
      "Divyansh Srivastava",
      "Xiang Zhang",
      "He Wen",
      "Chenru Wen",
      "Zhuowen Tu"
    ],
    "abstract": "We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout generation pipeline for natural scenes. Prior scene layout generation methods are either closed-vocabulary or use proprietary large language models for open-vocabulary generation, limiting their modeling capabilities and broader applicability in controllable image generation. In this work, we propose to use lightweight open-source language models to obtain scene elements from text prompts and a novel aspect-aware diffusion Transformer architecture trained in an open-vocabulary manner for conditional layout generation. Extensive experiments demonstrate that LayouSyn outperforms existing methods and achieves state-of-the-art performance on challenging spatial and numerical reasoning benchmarks. Additionally, we present two applications of LayouSyn. First, we show that coarse initialization from large language models can be seamlessly combined with our method to achieve better results. Second, we present a pipeline for adding objects to images, demonstrating the potential of LayouSyn in image editing applications.",
    "arxiv_url": "http://arxiv.org/abs/2505.04718v1",
    "pdf_url": "http://arxiv.org/pdf/2505.04718v1",
    "published_date": "2025-05-07",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Controllable",
      "image editing",
      "Control",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deepfakes on Demand: the rise of accessible non-consensual deepfake image generators",
    "authors": [
      "Will Hawkins",
      "Chris Russell",
      "Brent Mittelstadt"
    ],
    "abstract": "Advances in multimodal machine learning have made text-to-image (T2I) models increasingly accessible and popular. However, T2I models introduce risks such as the generation of non-consensual depictions of identifiable individuals, otherwise known as deepfakes. This paper presents an empirical study exploring the accessibility of deepfake model variants online. Through a metadata analysis of thousands of publicly downloadable model variants on two popular repositories, Hugging Face and Civitai, we demonstrate a huge rise in easily accessible deepfake models. Almost 35,000 examples of publicly downloadable deepfake model variants are identified, primarily hosted on Civitai. These deepfake models have been downloaded almost 15 million times since November 2022, with the models targeting a range of individuals from global celebrities to Instagram users with under 10,000 followers. Both Stable Diffusion and Flux models are used for the creation of deepfake models, with 96% of these targeting women and many signalling intent to generate non-consensual intimate imagery (NCII). Deepfake model variants are often created via the parameter-efficient fine-tuning technique known as low rank adaptation (LoRA), requiring as few as 20 images, 24GB VRAM, and 15 minutes of time, making this process widely accessible via consumer-grade computers. Despite these models violating the Terms of Service of hosting platforms, and regulation seeking to prevent dissemination, these results emphasise the pressing need for greater action to be taken against the creation of deepfakes and NCII.",
    "arxiv_url": "http://arxiv.org/abs/2505.03859v1",
    "pdf_url": "http://arxiv.org/pdf/2505.03859v1",
    "published_date": "2025-05-06",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "68T01"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization",
    "authors": [
      "Wenchuan Wang",
      "Mengqi Huang",
      "Yijing Tu",
      "Zhendong Mao"
    ],
    "abstract": "Customized text-to-video generation with pre-trained large-scale models has recently garnered significant attention through focusing on identity and motion consistency. Existing works typically follow the isolated customized paradigm, where the subject identity or motion dynamics are customized exclusively. However, this paradigm completely ignores the intrinsic mutual constraints and synergistic interdependencies between identity and motion, resulting in identity-motion conflicts throughout the generation process that systematically degrades. To address this, we introduce DualReal, a novel framework that, employs adaptive joint training to collaboratively construct interdependencies between dimensions. Specifically, DualReal is composed of two units: (1) Dual-aware Adaptation dynamically selects a training phase (i.e., identity or motion), learns the current information guided by the frozen dimension prior, and employs a regularization strategy to avoid knowledge leakage; (2) StageBlender Controller leverages the denoising stages and Diffusion Transformer depths to guide different dimensions with adaptive granularity, avoiding conflicts at various stages and ultimately achieving lossless fusion of identity and motion patterns. We constructed a more comprehensive benchmark than existing methods. The experimental results show that DualReal improves CLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top performance on nearly all motion quality metrics.",
    "arxiv_url": "http://arxiv.org/abs/2505.02192v1",
    "pdf_url": "http://arxiv.org/pdf/2505.02192v1",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers",
    "authors": [
      "Kwon Byung-Ki",
      "Qi Dai",
      "Lee Hyoseok",
      "Chong Luo",
      "Tae-Hyun Oh"
    ],
    "abstract": "We present JointDiT, a diffusion transformer that models the joint distribution of RGB and depth. By leveraging the architectural benefit and outstanding image prior of the state-of-the-art diffusion transformer, JointDiT not only generates high-fidelity images but also produces geometrically plausible and accurate depth maps. This solid joint distribution modeling is achieved through two simple yet effective techniques that we propose, i.e., adaptive scheduling weights, which depend on the noise levels of each modality, and the unbalanced timestep sampling strategy. With these techniques, we train our model across all noise levels for each modality, enabling JointDiT to naturally handle various combinatorial generation tasks, including joint generation, depth estimation, and depth-conditioned image generation by simply controlling the timestep of each branch. JointDiT demonstrates outstanding joint generation performance. Furthermore, it achieves comparable results in depth estimation and depth-conditioned image generation, suggesting that joint distribution modeling can serve as a replaceable alternative to conditional generation. The project page is available at https://byungki-k.github.io/JointDiT/.",
    "arxiv_url": "http://arxiv.org/abs/2505.00482v1",
    "pdf_url": "http://arxiv.org/pdf/2505.00482v1",
    "published_date": "2025-05-01",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Can We Achieve Efficient Diffusion without Self-Attention? Distilling Self-Attention into Convolutions",
    "authors": [
      "ZiYi Dong",
      "Chengxing Zhou",
      "Weijian Deng",
      "Pengxu Wei",
      "Xiangyang Ji",
      "Liang Lin"
    ],
    "abstract": "Contemporary diffusion models built upon U-Net or Diffusion Transformer (DiT) architectures have revolutionized image generation through transformer-based attention mechanisms. The prevailing paradigm has commonly employed self-attention with quadratic computational complexity to handle global spatial relationships in complex images, thereby synthesizing high-fidelity images with coherent visual semantics.Contrary to conventional wisdom, our systematic layer-wise analysis reveals an interesting discrepancy: self-attention in pre-trained diffusion models predominantly exhibits localized attention patterns, closely resembling convolutional inductive biases. This suggests that global interactions in self-attention may be less critical than commonly assumed.Driven by this, we propose \\(\\Delta\\)ConvFusion to replace conventional self-attention modules with Pyramid Convolution Blocks (\\(\\Delta\\)ConvBlocks).By distilling attention patterns into localized convolutional operations while keeping other components frozen, \\(\\Delta\\)ConvFusion achieves performance comparable to transformer-based counterparts while reducing computational cost by 6929$\\times$ and surpassing LinFusion by 5.42$\\times$ in efficiency--all without compromising generative fidelity.",
    "arxiv_url": "http://arxiv.org/abs/2504.21292v1",
    "pdf_url": "http://arxiv.org/pdf/2504.21292v1",
    "published_date": "2025-04-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer",
    "authors": [
      "Zechuan Zhang",
      "Ji Xie",
      "Yu Lu",
      "Zongxin Yang",
      "Yi Yang"
    ],
    "abstract": "Instruction-based image editing enables robust image modification via natural language prompts, yet current methods face a precision-efficiency tradeoff. Fine-tuning methods demand significant computational resources and large datasets, while training-free techniques struggle with instruction comprehension and edit quality. We resolve this dilemma by leveraging large-scale Diffusion Transformer (DiT)' enhanced generation capacity and native contextual awareness. Our solution introduces three contributions: (1) an in-context editing framework for zero-shot instruction compliance using in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning strategy that enhances flexibility with efficient adaptation and dynamic expert routing, without extensive retraining; and (3) an early filter inference-time scaling method using vision-language models (VLMs) to select better initial noise early, improving edit quality. Extensive evaluations demonstrate our method's superiority: it outperforms state-of-the-art approaches while requiring only 0.5% training data and 1% trainable parameters compared to conventional baselines. This work establishes a new paradigm that enables high-precision yet efficient instruction-guided editing. Codes and demos can be found in https://river-zhang.github.io/ICEdit-gh-pages/.",
    "arxiv_url": "http://arxiv.org/abs/2504.20690v1",
    "pdf_url": "http://arxiv.org/pdf/2504.20690v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "image editing"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer",
    "authors": [
      "Junpeng Jiang",
      "Gangyi Hong",
      "Miao Zhang",
      "Hengtong Hu",
      "Kun Zhan",
      "Rui Shao",
      "Liqiang Nie"
    ],
    "abstract": "Collecting multi-view driving scenario videos to enhance the performance of 3D visual perception tasks presents significant challenges and incurs substantial costs, making generative models for realistic data an appealing alternative. Yet, the videos generated by recent works suffer from poor quality and spatiotemporal consistency, undermining their utility in advancing perception tasks under driving scenarios. To address this gap, we propose DiVE, a diffusion transformer-based generative framework meticulously engineered to produce high-fidelity, temporally coherent, and cross-view consistent multi-view videos, aligning seamlessly with bird's-eye view layouts and textual descriptions. DiVE leverages a unified cross-attention and a SketchFormer to exert precise control over multimodal data, while incorporating a view-inflated attention mechanism that adds no extra parameters, thereby guaranteeing consistency across views. Despite these advancements, synthesizing high-resolution videos under multimodal constraints introduces dual challenges: investigating the optimal classifier-free guidance coniguration under intricate multi-condition inputs and mitigating excessive computational latency in high-resolution rendering--both of which remain underexplored in prior researches. To resolve these limitations, we introduce two innovations: Multi-Control Auxiliary Branch Distillation, which streamlines multi-condition CFG selection while circumventing high computational overhead, and Resolution Progressive Sampling, a training-free acceleration strategy that staggers resolution scaling to reduce high latency due to high resolution. These innovations collectively achieve a 2.62x speedup with minimal quality degradation. Evaluated on the nuScenes dataset, DiVE achieves SOTA performance in multi-view video generation, yielding photorealistic outputs with exceptional temporal and cross-view coherence.",
    "arxiv_url": "http://arxiv.org/abs/2504.19614v1",
    "pdf_url": "http://arxiv.org/pdf/2504.19614v1",
    "published_date": "2025-04-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Physics-based super-resolved simulation of 3D elastic wave propagation adopting scalable Diffusion Transformer",
    "authors": [
      "Hugo Gabrielidis",
      "Filippo Gatti",
      "Stphane Vialle"
    ],
    "abstract": "In this study, we develop a Diffusion Transformer (referred as to DiT1D) for synthesizing realistic earthquake time histories. The DiT1D generates realistic broadband accelerograms (0-30 Hz resolution), constrained at low frequency by 3-dimensional (3D) elastodynamics numerical simulations, ensuring the fulfillment of the minimum observable physics. The DiT1D architecture, successfully adopted in super-resolution image generation, is trained on recorded single-station 3-components (3C) accelerograms. Thanks to Multi-Head Cross-Attention (MHCA) layers, we guide the DiT1D inference by enforcing the low-frequency part of the accelerogram spectrum into it. The DiT1D learns the low-to-high frequency map from the recorded accelerograms, duly normalized, and successfully transfer it to synthetic time histories. The latter are low-frequency by nature, because of the lack of knowledge on the underground structure of the Earth, demanded to fully calibrate the numerical model. We developed a CNN-LSTM lightweight network in conjunction with the DiT1D, so to predict the peak amplitude of the broadband signal from its low-pass-filtered counterpart, and rescale the normalized accelerograms rendered by the DiT1D. Despite the DiT1D being agnostic to any earthquake event peculiarities (magnitude, site conditions, etc.), it showcases remarkable zero-shot prediction realism when applied to the output of validated earthquake simulations. The generated time histories are viable input accelerograms for earthquake-resistant structural design and the pre-trained DiT1D holds a huge potential to integrate full-scale fault-to-structure digital twins of earthquake-prone regions.",
    "arxiv_url": "http://arxiv.org/abs/2504.17308v1",
    "pdf_url": "http://arxiv.org/pdf/2504.17308v1",
    "published_date": "2025-04-24",
    "categories": [
      "physics.geo-ph"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
    "authors": [
      "Theodoros Kouzelis",
      "Efstathios Karypidis",
      "Ioannis Kakogeorgiou",
      "Spyros Gidaris",
      "Nikos Komodakis"
    ],
    "abstract": "Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-level image latents (from a variational autoencoder) and high-level semantic features (from a pretrained self-supervised encoder like DINO). Our latent-semantic diffusion approach learns to generate coherent image-feature pairs from pure noise, significantly enhancing both generative quality and training efficiency, all while requiring only minimal modifications to standard Diffusion Transformer architectures. By eliminating the need for complex distillation objectives, our unified design simplifies training and unlocks a powerful new inference strategy: Representation Guidance, which leverages learned semantics to steer and refine image generation. Evaluated in both conditional and unconditional settings, our method delivers substantial improvements in image quality and training convergence speed, establishing a new direction for representation-aware generative modeling.",
    "arxiv_url": "http://arxiv.org/abs/2504.16064v1",
    "pdf_url": "http://arxiv.org/pdf/2504.16064v1",
    "published_date": "2025-04-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiTPainter: Efficient Video Inpainting with Diffusion Transformers",
    "authors": [
      "Xian Wu",
      "Chang Liu"
    ],
    "abstract": "Many existing video inpainting algorithms utilize optical flows to construct the corresponding maps and then propagate pixels from adjacent frames to missing areas by mapping. Despite the effectiveness of the propagation mechanism, they might encounter blurry and inconsistencies when dealing with inaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT) has emerged as a revolutionary technique for video generation tasks. However, pretrained DiT models for video generation all contain a large amount of parameters, which makes it very time consuming to apply to video inpainting tasks. In this paper, we present DiTPainter, an end-to-end video inpainting model based on Diffusion Transformer (DiT). DiTPainter uses an efficient transformer network designed for video inpainting, which is trained from scratch instead of initializing from any large pretrained models. DiTPainter can address videos with arbitrary lengths and can be applied to video decaptioning and video completion tasks with an acceptable time cost. Experiments show that DiTPainter outperforms existing video inpainting algorithms with higher quality and better spatial-temporal consistency.",
    "arxiv_url": "http://arxiv.org/abs/2504.15661v3",
    "pdf_url": "http://arxiv.org/pdf/2504.15661v3",
    "published_date": "2025-04-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation",
      "video inpainting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Acquire and then Adapt: Squeezing out Text-to-Image Model for Image Restoration",
    "authors": [
      "Junyuan Deng",
      "Xinyi Wu",
      "Yongxing Yang",
      "Congchao Zhu",
      "Song Wang",
      "Zhenyao Wu"
    ],
    "abstract": "Recently, pre-trained text-to-image (T2I) models have been extensively adopted for real-world image restoration because of their powerful generative prior. However, controlling these large models for image restoration usually requires a large number of high-quality images and immense computational resources for training, which is costly and not privacy-friendly. In this paper, we find that the well-trained large T2I model (i.e., Flux) is able to produce a variety of high-quality images aligned with real-world distributions, offering an unlimited supply of training samples to mitigate the above issue. Specifically, we proposed a training data construction pipeline for image restoration, namely FluxGen, which includes unconditional image generation, image selection, and degraded image simulation. A novel light-weighted adapter (FluxIR) with squeeze-and-excitation layers is also carefully designed to control the large Diffusion Transformer (DiT)-based T2I model so that reasonable details can be restored. Experiments demonstrate that our proposed method enables the Flux model to adapt effectively to real-world image restoration tasks, achieving superior scores and visual quality on both synthetic and real-world degradation datasets - at only about 8.5\\% of the training cost compared to current approaches.",
    "arxiv_url": "http://arxiv.org/abs/2504.15159v1",
    "pdf_url": "http://arxiv.org/pdf/2504.15159v1",
    "published_date": "2025-04-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "image generation",
      "text-to-image",
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Turbo2K: Towards Ultra-Efficient and High-Quality 2K Video Synthesis",
    "authors": [
      "Jingjing Ren",
      "Wenbo Li",
      "Zhongdao Wang",
      "Haoze Sun",
      "Bangzhen Liu",
      "Haoyu Chen",
      "Jiaqi Xu",
      "Aoxue Li",
      "Shifeng Zhang",
      "Bin Shao",
      "Yong Guo",
      "Lei Zhu"
    ],
    "abstract": "Demand for 2K video synthesis is rising with increasing consumer expectations for ultra-clear visuals. While diffusion transformers (DiTs) have demonstrated remarkable capabilities in high-quality video generation, scaling them to 2K resolution remains computationally prohibitive due to quadratic growth in memory and processing costs. In this work, we propose Turbo2K, an efficient and practical framework for generating detail-rich 2K videos while significantly improving training and inference efficiency. First, Turbo2K operates in a highly compressed latent space, reducing computational complexity and memory footprint, making high-resolution video synthesis feasible. However, the high compression ratio of the VAE and limited model size impose constraints on generative quality. To mitigate this, we introduce a knowledge distillation strategy that enables a smaller student model to inherit the generative capacity of a larger, more powerful teacher model. Our analysis reveals that, despite differences in latent spaces and architectures, DiTs exhibit structural similarities in their internal representations, facilitating effective knowledge transfer. Second, we design a hierarchical two-stage synthesis framework that first generates multi-level feature at lower resolutions before guiding high-resolution video generation. This approach ensures structural coherence and fine-grained detail refinement while eliminating redundant encoding-decoding overhead, further enhancing computational efficiency.Turbo2K achieves state-of-the-art efficiency, generating 5-second, 24fps, 2K videos with significantly reduced computational cost. Compared to existing methods, Turbo2K is up to 20$\\times$ faster for inference, making high-resolution video generation more scalable and practical for real-world applications.",
    "arxiv_url": "http://arxiv.org/abs/2504.14470v1",
    "pdf_url": "http://arxiv.org/pdf/2504.14470v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Text-Audio-Visual-conditioned Diffusion Model for Video Saliency Prediction",
    "authors": [
      "Li Yu",
      "Xuanzhe Sun",
      "Wei Zhou",
      "Moncef Gabbouj"
    ],
    "abstract": "Video saliency prediction is crucial for downstream applications, such as video compression and human-computer interaction. With the flourishing of multimodal learning, researchers started to explore multimodal video saliency prediction, including audio-visual and text-visual approaches. Auditory cues guide the gaze of viewers to sound sources, while textual cues provide semantic guidance for understanding video content. Integrating these complementary cues can improve the accuracy of saliency prediction. Therefore, we attempt to simultaneously analyze visual, auditory, and textual modalities in this paper, and propose TAVDiff, a Text-Audio-Visual-conditioned Diffusion Model for video saliency prediction. TAVDiff treats video saliency prediction as an image generation task conditioned on textual, audio, and visual inputs, and predicts saliency maps through stepwise denoising. To effectively utilize text, a large multimodal model is used to generate textual descriptions for video frames and introduce a saliency-oriented image-text response (SITR) mechanism to generate image-text response maps. It is used as conditional information to guide the model to localize the visual regions that are semantically related to the textual description. Regarding the auditory modality, it is used as another conditional information for directing the model to focus on salient regions indicated by sounds. At the same time, since the diffusion transformer (DiT) directly concatenates the conditional information with the timestep, which may affect the estimation of the noise level. To achieve effective conditional guidance, we propose Saliency-DiT, which decouples the conditional information from the timestep. Experimental results show that TAVDiff outperforms existing methods, improving 1.03\\%, 2.35\\%, 2.71\\% and 0.33\\% on SIM, CC, NSS and AUC-J metrics, respectively.",
    "arxiv_url": "http://arxiv.org/abs/2504.14267v1",
    "pdf_url": "http://arxiv.org/pdf/2504.14267v1",
    "published_date": "2025-04-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Entropy Rectifying Guidance for Diffusion and Flow Models",
    "authors": [
      "Tariq Berrada Ifriqi",
      "Adriana Romero-Soriano",
      "Michal Drozdzal",
      "Jakob Verbeek",
      "Karteek Alahari"
    ],
    "abstract": "Guidance techniques are commonly used in diffusion and flow models to improve image quality and consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) -- the most widely adopted guidance technique -- contrasts conditional and unconditional predictions to improve the generated images. This results, however, in trade-offs across quality, diversity and consistency, improving some at the expense of others. While recent work has shown that it is possible to disentangle these factors to some extent, such methods come with an overhead of requiring an additional (weaker) model, or require more forward passes per sampling step. In this paper, we propose Entropy Rectifying Guidance (ERG), a simple and effective guidance mechanism based on inference-time changes in the attention mechanism of state-of-the-art diffusion transformer architectures, which allows for simultaneous improvements over image quality, diversity and prompt consistency. ERG is more general than CFG and similar guidance techniques, as it extends to unconditional sampling. ERG results in significant improvements in various generation tasks such as text-to-image, class-conditional and unconditional image generation. We also show that ERG can be seamlessly combined with other recent guidance methods such as CADS and APG, further boosting generation performance.",
    "arxiv_url": "http://arxiv.org/abs/2504.13987v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13987v1",
    "published_date": "2025-04-18",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InstantCharacter: Personalize Any Characters with a Scalable Diffusion Transformer Framework",
    "authors": [
      "Jiale Tao",
      "Yanbing Zhang",
      "Qixun Wang",
      "Yiji Cheng",
      "Haofan Wang",
      "Xu Bai",
      "Zhengguang Zhou",
      "Ruihuang Li",
      "Linqing Wang",
      "Chunyu Wang",
      "Qin Lin",
      "Qinglin Lu"
    ],
    "abstract": "Current learning-based subject customization approaches, predominantly relying on U-Net architectures, suffer from limited generalization ability and compromised image quality. Meanwhile, optimization-based methods require subject-specific fine-tuning, which inevitably degrades textual controllability. To address these challenges, we propose InstantCharacter, a scalable framework for character customization built upon a foundation diffusion transformer. InstantCharacter demonstrates three fundamental advantages: first, it achieves open-domain personalization across diverse character appearances, poses, and styles while maintaining high-fidelity results. Second, the framework introduces a scalable adapter with stacked transformer encoders, which effectively processes open-domain character features and seamlessly interacts with the latent space of modern diffusion transformers. Third, to effectively train the framework, we construct a large-scale character dataset containing 10-million-level samples. The dataset is systematically organized into paired (multi-view character) and unpaired (text-image combinations) subsets. This dual-data structure enables simultaneous optimization of identity consistency and textual editability through distinct learning pathways. Qualitative experiments demonstrate the advanced capabilities of InstantCharacter in generating high-fidelity, text-controllable, and character-consistent images, setting a new benchmark for character-driven image generation. Our source code is available at https://github.com/Tencent/InstantCharacter.",
    "arxiv_url": "http://arxiv.org/abs/2504.12395v1",
    "pdf_url": "http://arxiv.org/pdf/2504.12395v1",
    "published_date": "2025-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Tencent/InstantCharacter",
    "keywords": [
      "diffusion transformer",
      "Control",
      "Controllable",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VGDFR: Diffusion-based Video Generation with Dynamic Latent Frame Rate",
    "authors": [
      "Zhihang Yuan",
      "Rui Xie",
      "Yuzhang Shang",
      "Hanling Zhang",
      "Siyuan Wang",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ],
    "abstract": "Diffusion Transformer(DiT)-based generation models have achieved remarkable success in video generation. However, their inherent computational demands pose significant efficiency challenges. In this paper, we exploit the inherent temporal non-uniformity of real-world videos and observe that videos exhibit dynamic information density, with high-motion segments demanding greater detail preservation than static scenes. Inspired by this temporal non-uniformity, we propose VGDFR, a training-free approach for Diffusion-based Video Generation with Dynamic Latent Frame Rate. VGDFR adaptively adjusts the number of elements in latent space based on the motion frequency of the latent space content, using fewer tokens for low-frequency segments while preserving detail in high-frequency segments. Specifically, our key contributions are: (1) A dynamic frame rate scheduler for DiT video generation that adaptively assigns frame rates for video segments. (2) A novel latent-space frame merging method to align latent representations with their denoised counterparts before merging those redundant in low-resolution space. (3) A preference analysis of Rotary Positional Embeddings (RoPE) across DiT layers, informing a tailored RoPE strategy optimized for semantic and local information capture. Experiments show that VGDFR can achieve a speedup up to 3x for video generation with minimal quality degradation.",
    "arxiv_url": "http://arxiv.org/abs/2504.12259v1",
    "pdf_url": "http://arxiv.org/pdf/2504.12259v1",
    "published_date": "2025-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Using LLMs as prompt modifier to avoid biases in AI image generators",
    "authors": [
      "Ren Peinl"
    ],
    "abstract": "This study examines how Large Language Models (LLMs) can reduce biases in text-to-image generation systems by modifying user prompts. We define bias as a model's unfair deviation from population statistics given neutral prompts. Our experiments with Stable Diffusion XL, 3.5 and Flux demonstrate that LLM-modified prompts significantly increase image diversity and reduce bias without the need to change the image generators themselves. While occasionally producing results that diverge from original user intent for elaborate prompts, this approach generally provides more varied interpretations of underspecified requests rather than superficial variations. The method works particularly well for less advanced image generators, though limitations persist for certain contexts like disability representation. All prompts and generated images are available at https://iisys-hof.github.io/llm-prompt-img-gen/",
    "arxiv_url": "http://arxiv.org/abs/2504.11104v1",
    "pdf_url": "http://arxiv.org/pdf/2504.11104v1",
    "published_date": "2025-04-15",
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.CY"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "FLUX",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Analysis of Attention in Video Diffusion Transformers",
    "authors": [
      "Yuxin Wen",
      "Jim Wu",
      "Ajay Jain",
      "Tom Goldstein",
      "Ashwinee Panda"
    ],
    "abstract": "We conduct an in-depth analysis of attention in video diffusion transformers (VDiTs) and report a number of novel findings. We identify three key properties of attention in VDiTs: Structure, Sparsity, and Sinks. Structure: We observe that attention patterns across different VDiTs exhibit similar structure across different prompts, and that we can make use of the similarity of attention patterns to unlock video editing via self-attention map transfer. Sparse: We study attention sparsity in VDiTs, finding that proposed sparsity methods do not work for all VDiTs, because some layers that are seemingly sparse cannot be sparsified. Sinks: We make the first study of attention sinks in VDiTs, comparing and contrasting them to attention sinks in language models. We propose a number of future directions that can make use of our insights to improve the efficiency-quality Pareto frontier for VDiTs.",
    "arxiv_url": "http://arxiv.org/abs/2504.10317v1",
    "pdf_url": "http://arxiv.org/pdf/2504.10317v1",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video editing"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D$^2$iT: Dynamic Diffusion Transformer for Accurate Image Generation",
    "authors": [
      "Weinan Jia",
      "Mengqi Huang",
      "Nan Chen",
      "Lei Zhang",
      "Zhendong Mao"
    ],
    "abstract": "Diffusion models are widely recognized for their ability to generate high-fidelity images. Despite the excellent performance and scalability of the Diffusion Transformer (DiT) architecture, it applies fixed compression across different image regions during the diffusion process, disregarding the naturally varying information densities present in these regions. However, large compression leads to limited local realism, while small compression increases computational complexity and compromises global consistency, ultimately impacting the quality of generated images. To address these limitations, we propose dynamically compressing different image regions by recognizing the importance of different regions, and introduce a novel two-stage framework designed to enhance the effectiveness and efficiency of image generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical encoder to encode different image regions at different downsampling rates, tailored to their specific information densities, thereby providing more accurate and natural latent codes for the diffusion process. (2) Dynamic Diffusion Transformer (D$^2$iT) at second stage generates images by predicting multi-grained noise, consisting of coarse-grained (less latent code in smooth regions) and fine-grained (more latent codes in detailed regions), through an novel combination of the Dynamic Grain Transformer and the Dynamic Content Transformer. The strategy of combining rough prediction of noise with detailed regions correction achieves a unification of global consistency and local realism. Comprehensive experiments on various generation tasks validate the effectiveness of our approach. Code will be released at https://github.com/jiawn-creator/Dynamic-DiT.",
    "arxiv_url": "http://arxiv.org/abs/2504.09454v1",
    "pdf_url": "http://arxiv.org/pdf/2504.09454v1",
    "published_date": "2025-04-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/jiawn-creator/Dynamic-DiT",
    "keywords": [
      "diffusion transformer",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Flux Already Knows -- Activating Subject-Driven Image Generation without Training",
    "authors": [
      "Hao Kang",
      "Stathi Fotiadis",
      "Liming Jiang",
      "Qing Yan",
      "Yumin Jia",
      "Zichuan Liu",
      "Min Jin Chong",
      "Xin Lu"
    ],
    "abstract": "We propose a simple yet effective zero-shot framework for subject-driven image generation using a vanilla Flux model. By framing the task as grid-based image completion and simply replicating the subject image(s) in a mosaic layout, we activate strong identity-preserving capabilities without any additional data, training, or inference-time fine-tuning. This \"free lunch\" approach is further strengthened by a novel cascade attention design and meta prompting technique, boosting fidelity and versatility. Experimental results show that our method outperforms baselines across multiple key metrics in benchmarks and human preference studies, with trade-offs in certain aspects. Additionally, it supports diverse edits, including logo insertion, virtual try-on, and subject replacement or insertion. These results demonstrate that a pre-trained foundational text-to-image model can enable high-quality, resource-efficient subject-driven generation, opening new possibilities for lightweight customization in downstream applications.",
    "arxiv_url": "http://arxiv.org/abs/2504.11478v2",
    "pdf_url": "http://arxiv.org/pdf/2504.11478v2",
    "published_date": "2025-04-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "FLUX",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MixDiT: Accelerating Image Diffusion Transformer Inference with Mixed-Precision MX Quantization",
    "authors": [
      "Daeun Kim",
      "Jinwoo Hwang",
      "Changhun Oh",
      "Jongse Park"
    ],
    "abstract": "Diffusion Transformer (DiT) has driven significant progress in image generation tasks. However, DiT inferencing is notoriously compute-intensive and incurs long latency even on datacenter-scale GPUs, primarily due to its iterative nature and heavy reliance on GEMM operations inherent to its encoder-based structure. To address the challenge, prior work has explored quantization, but achieving low-precision quantization for DiT inferencing with both high accuracy and substantial speedup remains an open problem. To this end, this paper proposes MixDiT, an algorithm-hardware co-designed acceleration solution that exploits mixed Microscaling (MX) formats to quantize DiT activation values. MixDiT quantizes the DiT activation tensors by selectively applying higher precision to magnitude-based outliers, which produce mixed-precision GEMM operations. To achieve tangible speedup from the mixed-precision arithmetic, we design a MixDiT accelerator that enables precision-flexible multiplications and efficient MX precision conversions. Our experimental results show that MixDiT delivers a speedup of 2.10-5.32 times over RTX 3090, with no loss in FID.",
    "arxiv_url": "http://arxiv.org/abs/2504.08398v1",
    "pdf_url": "http://arxiv.org/pdf/2504.08398v1",
    "published_date": "2025-04-11",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion Transformers for Tabular Data Time Series Generation",
    "authors": [
      "Fabrizio Garuti",
      "Enver Sangineto",
      "Simone Luetto",
      "Lorenzo Forni",
      "Rita Cucchiara"
    ],
    "abstract": "Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, generating time series of tabular data, where each element of the series depends on the others, remains a largely unexplored domain. This gap is probably due to the difficulty of jointly solving different problems, the main of which are the heterogeneity of tabular data (a problem common to non-time-dependent approaches) and the variable length of a time series. In this paper, we propose a Diffusion Transformers (DiTs) based approach for tabular data series generation. Inspired by the recent success of DiTs in image and video generation, we extend this framework to deal with heterogeneous data and variable-length sequences. Using extensive experiments on six datasets, we show that the proposed approach outperforms previous work by a large margin.",
    "arxiv_url": "http://arxiv.org/abs/2504.07566v2",
    "pdf_url": "http://arxiv.org/pdf/2504.07566v2",
    "published_date": "2025-04-10",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation",
    "authors": [
      "Wangbo Zhao",
      "Yizeng Han",
      "Jiasheng Tang",
      "Kai Wang",
      "Hao Luo",
      "Yibing Song",
      "Gao Huang",
      "Fan Wang",
      "Yang You"
    ],
    "abstract": "Diffusion Transformer (DiT), an emerging diffusion model for visual generation, has demonstrated superior performance but suffers from substantial computational costs. Our investigations reveal that these costs primarily stem from the \\emph{static} inference paradigm, which inevitably introduces redundant computation in certain \\emph{diffusion timesteps} and \\emph{spatial regions}. To overcome this inefficiency, we propose \\textbf{Dy}namic \\textbf{Di}ffusion \\textbf{T}ransformer (DyDiT), an architecture that \\emph{dynamically} adjusts its computation along both \\emph{timestep} and \\emph{spatial} dimensions. Specifically, we introduce a \\emph{Timestep-wise Dynamic Width} (TDW) approach that adapts model width conditioned on the generation timesteps. In addition, we design a \\emph{Spatial-wise Dynamic Token} (SDT) strategy to avoid redundant computation at unnecessary spatial locations. TDW and SDT can be seamlessly integrated into DiT and significantly accelerates the generation process. Building on these designs, we further enhance DyDiT in three key aspects. First, DyDiT is integrated seamlessly with flow matching-based generation, enhancing its versatility. Furthermore, we enhance DyDiT to tackle more complex visual generation tasks, including video generation and text-to-image generation, thereby broadening its real-world applications. Finally, to address the high cost of full fine-tuning and democratize technology access, we investigate the feasibility of training DyDiT in a parameter-efficient manner and introduce timestep-based dynamic LoRA (TD-LoRA). Extensive experiments on diverse visual generation models, including DiT, SiT, Latte, and FLUX, demonstrate the effectiveness of DyDiT.",
    "arxiv_url": "http://arxiv.org/abs/2504.06803v2",
    "pdf_url": "http://arxiv.org/pdf/2504.06803v2",
    "published_date": "2025-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation",
      "image generation",
      "text-to-image",
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Disentangling Instruction Influence in Diffusion Transformers for Parallel Multi-Instruction-Guided Image Editing",
    "authors": [
      "Hui Liu",
      "Bin Zou",
      "Suiyun Zhang",
      "Kecheng Chen",
      "Rui Liu",
      "Haoliang Li"
    ],
    "abstract": "Instruction-guided image editing enables users to specify modifications using natural language, offering more flexibility and control. Among existing frameworks, Diffusion Transformers (DiTs) outperform U-Net-based diffusion models in scalability and performance. However, while real-world scenarios often require concurrent execution of multiple instructions, step-by-step editing suffers from accumulated errors and degraded quality, and integrating multiple instructions with a single prompt usually results in incomplete edits due to instruction conflicts. We propose Instruction Influence Disentanglement (IID), a novel framework enabling parallel execution of multiple instructions in a single denoising process, designed for DiT-based models. By analyzing self-attention mechanisms in DiTs, we identify distinctive attention patterns in multi-instruction settings and derive instruction-specific attention masks to disentangle each instruction's influence. These masks guide the editing process to ensure localized modifications while preserving consistency in non-edited regions. Extensive experiments on open-source and custom datasets demonstrate that IID reduces diffusion steps while improving fidelity and instruction completion compared to existing baselines. The codes will be publicly released upon the acceptance of the paper.",
    "arxiv_url": "http://arxiv.org/abs/2504.04784v1",
    "pdf_url": "http://arxiv.org/pdf/2504.04784v1",
    "published_date": "2025-04-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "image editing"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion",
    "authors": [
      "Maksim Siniukov",
      "Di Chang",
      "Minh Tran",
      "Hongkun Gong",
      "Ashutosh Chaubey",
      "Mohammad Soleymani"
    ],
    "abstract": "Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address these challenges, we introduce DiTaiListener, powered by a video diffusion model with multimodal conditions. Our approach first generates short segments of listener responses conditioned on the speaker's speech and facial motions with DiTaiListener-Gen. It then refines the transitional frames via DiTaiListener-Edit for a seamless transition. Specifically, DiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener head portrait generation by introducing a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter integrates speakers' input in a causal manner into the video generation process to ensure temporally coherent listener responses. For long-form video generation, we introduce DiTaiListener-Edit, a transition refinement video-to-video diffusion model. The model fuses video segments into smooth and continuous videos, ensuring temporal consistency in facial expressions and image quality when merging short video segments produced by DiTaiListener-Gen. Quantitatively, DiTaiListener achieves the state-of-the-art performance on benchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO) spaces. User studies confirm the superior performance of DiTaiListener, with the model being the clear preference in terms of feedback, diversity, and smoothness, outperforming competitors by a significant margin.",
    "arxiv_url": "http://arxiv.org/abs/2504.04010v1",
    "pdf_url": "http://arxiv.org/pdf/2504.04010v1",
    "published_date": "2025-04-05",
    "categories": [
      "cs.CV",
      "cs.LG",
      "I.4.9"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "Controllable",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Detection Limits and Statistical Separability of Tree Ring Watermarks in Rectified Flow-based Text-to-Image Generation Models",
    "authors": [
      "Ved Umrajkar",
      "Aakash Kumar Singh"
    ],
    "abstract": "Tree-Ring Watermarking is a significant technique for authenticating AI-generated images. However, its effectiveness in rectified flow-based models remains unexplored, particularly given the inherent challenges of these models with noise latent inversion. Through extensive experimentation, we evaluated and compared the detection and separability of watermarks between SD 2.1 and FLUX.1-dev models. By analyzing various text guidance configurations and augmentation attacks, we demonstrate how inversion limitations affect both watermark recovery and the statistical separation between watermarked and unwatermarked images. Our findings provide valuable insights into the current limitations of Tree-Ring Watermarking in the current SOTA models and highlight the critical need for improved inversion methods to achieve reliable watermark detection and separability. The official implementation, dataset release and all experimental results are available at this \\href{https://github.com/dsgiitr/flux-watermarking}{\\textbf{link}}.",
    "arxiv_url": "http://arxiv.org/abs/2504.03850v1",
    "pdf_url": "http://arxiv.org/pdf/2504.03850v1",
    "published_date": "2025-04-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ],
    "github_url": "https://github.com/dsgiitr/flux-watermarking",
    "keywords": [
      "rectified flow",
      "image generation",
      "FLUX",
      "inversion",
      "text-to-image"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
    "authors": [
      "Zhengcong Fei",
      "Debang Li",
      "Di Qiu",
      "Jiahua Wang",
      "Yikun Dou",
      "Rui Wang",
      "Jingtao Xu",
      "Mingyuan Fan",
      "Guibin Chen",
      "Yang Li",
      "Yahui Zhou"
    ],
    "abstract": "This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.",
    "arxiv_url": "http://arxiv.org/abs/2504.02436v1",
    "pdf_url": "http://arxiv.org/pdf/2504.02436v1",
    "published_date": "2025-04-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "Controllable",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OmniTalker: One-shot Real-time Text-Driven Talking Audio-Video Generation With Multimodal Style Mimicking",
    "authors": [
      "Zhongjian Wang",
      "Peng Zhang",
      "Jinwei Qi",
      "Guangyuan Wang",
      "Chaonan Ji",
      "Sheng Xu",
      "Bang Zhang",
      "Liefeng Bo"
    ],
    "abstract": "Although significant progress has been made in audio-driven talking head generation, text-driven methods remain underexplored. In this work, we present OmniTalker, a unified framework that jointly generates synchronized talking audio-video content from input text while emulating the speaking and facial movement styles of the target identity, including speech characteristics, head motion, and facial dynamics. Our framework adopts a dual-branch diffusion transformer (DiT) architecture, with one branch dedicated to audio generation and the other to video synthesis. At the shallow layers, cross-modal fusion modules are introduced to integrate information between the two modalities. In deeper layers, each modality is processed independently, with the generated audio decoded by a vocoder and the video rendered using a GAN-based high-quality visual renderer. Leveraging the in-context learning capability of DiT through a masked-infilling strategy, our model can simultaneously capture both audio and visual styles without requiring explicit style extraction modules. Thanks to the efficiency of the DiT backbone and the optimized visual renderer, OmniTalker achieves real-time inference at 25 FPS. To the best of our knowledge, OmniTalker is the first one-shot framework capable of jointly modeling speech and facial styles in real time. Extensive experiments demonstrate its superiority over existing methods in terms of generation quality, particularly in preserving style consistency and ensuring precise audio-video synchronization, all while maintaining efficient inference.",
    "arxiv_url": "http://arxiv.org/abs/2504.02433v2",
    "pdf_url": "http://arxiv.org/pdf/2504.02433v2",
    "published_date": "2025-04-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Less-to-More Generalization: Unlocking More Controllability by In-Context Generation",
    "authors": [
      "Shaojin Wu",
      "Mengqi Huang",
      "Wenxu Wu",
      "Yufeng Cheng",
      "Fei Ding",
      "Qian He"
    ],
    "abstract": "Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation.",
    "arxiv_url": "http://arxiv.org/abs/2504.02160v1",
    "pdf_url": "http://arxiv.org/pdf/2504.02160v1",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiT4SR: Taming Diffusion Transformer for Real-World Image Super-Resolution",
    "authors": [
      "Zheng-Peng Duan",
      "Jiawei Zhang",
      "Xin Jin",
      "Ziheng Zhang",
      "Zheng Xiong",
      "Dongqing Zou",
      "Jimmy Ren",
      "Chun-Le Guo",
      "Chongyi Li"
    ],
    "abstract": "Large-scale pre-trained diffusion models are becoming increasingly popular in solving the Real-World Image Super-Resolution (Real-ISR) problem because of their rich generative priors. The recent development of diffusion transformer (DiT) has witnessed overwhelming performance over the traditional UNet-based architecture in image generation, which also raises the question: Can we adopt the advanced DiT-based diffusion model for Real-ISR? To this end, we propose our DiT4SR, one of the pioneering works to tame the large-scale DiT model for Real-ISR. Instead of directly injecting embeddings extracted from low-resolution (LR) images like ControlNet, we integrate the LR embeddings into the original attention mechanism of DiT, allowing for the bidirectional flow of information between the LR latent and the generated latent. The sufficient interaction of these two streams allows the LR stream to evolve with the diffusion process, producing progressively refined guidance that better aligns with the generated latent at each diffusion step. Additionally, the LR guidance is injected into the generated latent via a cross-stream convolution layer, compensating for DiT's limited ability to capture local information. These simple but effective designs endow the DiT model with superior performance in Real-ISR, which is demonstrated by extensive experiments. Project Page: https://adam-duan.github.io/projects/dit4sr/.",
    "arxiv_url": "http://arxiv.org/abs/2503.23580v1",
    "pdf_url": "http://arxiv.org/pdf/2503.23580v1",
    "published_date": "2025-03-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "image super-resolution",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization",
    "authors": [
      "Kai Liu",
      "Wei Li",
      "Lai Chen",
      "Shengqiong Wu",
      "Yanhao Zheng",
      "Jiayi Ji",
      "Fan Zhou",
      "Rongxin Jiang",
      "Jiebo Luo",
      "Hao Fei",
      "Tat-Seng Chua"
    ],
    "abstract": "This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion Transformer designed for synchronized audio-video generation (JAVG). Built upon the powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to generate high-quality audio and video content simultaneously from open-ended user prompts. To ensure optimal synchronization, we introduce a fine-grained spatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator. This module extracts both global and fine-grained spatio-temporal priors, guiding the synchronization between the visual and auditory components. Furthermore, we propose a new benchmark, JavisBench, consisting of 10,140 high-quality text-captioned sounding videos spanning diverse scenes and complex real-world scenarios. Further, we specifically devise a robust metric for evaluating the synchronization between generated audio-video pairs in real-world complex content. Experimental results demonstrate that JavisDiT significantly outperforms existing methods by ensuring both high-quality generation and precise synchronization, setting a new standard for JAVG tasks. Our code, model, and dataset will be made publicly available at https://javisdit.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2503.23377v1",
    "pdf_url": "http://arxiv.org/pdf/2503.23377v1",
    "published_date": "2025-03-30",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality Diffusion Transformers",
    "authors": [
      "Hanling Zhang",
      "Rundong Su",
      "Zhihang Yuan",
      "Pengtao Chen",
      "Mingzhu Shen Yibo Fan",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ],
    "abstract": "Text-to-image generation models, especially Multimodal Diffusion Transformers (MMDiT), have shown remarkable progress in generating high-quality images. However, these models often face significant computational bottlenecks, particularly in attention mechanisms, which hinder their scalability and efficiency. In this paper, we introduce DiTFastAttnV2, a post-training compression method designed to accelerate attention in MMDiT. Through an in-depth analysis of MMDiT's attention patterns, we identify key differences from prior DiT-based methods and propose head-wise arrow attention and caching mechanisms to dynamically adjust attention heads, effectively bridging this gap. We also design an Efficient Fused Kernel for further acceleration. By leveraging local metric methods and optimization techniques, our approach significantly reduces the search time for optimal compression schemes to just minutes while maintaining generation quality. Furthermore, with the customized kernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x end-to-end speedup on 2K image generation without compromising visual fidelity.",
    "arxiv_url": "http://arxiv.org/abs/2503.22796v1",
    "pdf_url": "http://arxiv.org/pdf/2503.22796v1",
    "published_date": "2025-03-28",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynamiCtrl: Rethinking the Basic Structure and the Role of Text for High-quality Human Image Animation",
    "authors": [
      "Haoyu Zhao",
      "Zhongang Qi",
      "Cong Wang",
      "Qingping Zheng",
      "Guansong Lu",
      "Fei Chen",
      "Hang Xu",
      "Zuxuan Wu"
    ],
    "abstract": "With diffusion transformer (DiT) excelling in video generation, its use in specific tasks has drawn increasing attention. However, adapting DiT for pose-guided human image animation faces two core challenges: (a) existing U-Net-based pose control methods may be suboptimal for the DiT backbone; and (b) removing text guidance, as in previous approaches, often leads to semantic loss and model degradation. To address these issues, we propose DynamiCtrl, a novel framework for human animation in video DiT architecture. Specifically, we use a shared VAE encoder for human images and driving poses, unifying them into a common latent space, maintaining pose fidelity, and eliminating the need for an expert pose encoder during video denoising. To integrate pose control into the DiT backbone effectively, we propose a novel Pose-adaptive Layer Norm model. It injects normalized pose features into the denoising process via conditioning on visual tokens, enabling seamless and scalable pose control across DiT blocks. Furthermore, to overcome the shortcomings of text removal, we introduce the \"Joint-text\" paradigm, which preserves the role of text embeddings to provide global semantic context. Through full-attention blocks, image and pose features are aligned with text features, enhancing semantic consistency, leveraging pretrained knowledge, and enabling multi-level control. Experiments verify the superiority of DynamiCtrl on benchmark and self-collected data (e.g., achieving the best LPIPS of 0.166), demonstrating strong character control and high-quality synthesis. The project page is available at https://gulucaptain.github.io/DynamiCtrl/.",
    "arxiv_url": "http://arxiv.org/abs/2503.21246v2",
    "pdf_url": "http://arxiv.org/pdf/2503.21246v2",
    "published_date": "2025-03-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation",
    "authors": [
      "Yuyang Peng",
      "Shishi Xiao",
      "Keming Wu",
      "Qisheng Liao",
      "Bohan Chen",
      "Kevin Lin",
      "Danqing Huang",
      "Ji Li",
      "Yuhui Yuan"
    ],
    "abstract": "Recently, state-of-the-art text-to-image generation models, such as Flux and Ideogram 2.0, have made significant progress in sentence-level visual text rendering. In this paper, we focus on the more challenging scenarios of article-level visual text rendering and address a novel task of generating high-quality business content, including infographics and slides, based on user provided article-level descriptive prompts and ultra-dense layouts. The fundamental challenges are twofold: significantly longer context lengths and the scarcity of high-quality business content data.   In contrast to most previous works that focus on a limited number of sub-regions and sentence-level prompts, ensuring precise adherence to ultra-dense layouts with tens or even hundreds of sub-regions in business content is far more challenging. We make two key technical contributions: (i) the construction of scalable, high-quality business content dataset, i.e., Infographics-650K, equipped with ultra-dense layouts and prompts by implementing a layer-wise retrieval-augmented infographic generation scheme; and (ii) a layout-guided cross attention scheme, which injects tens of region-wise prompts into a set of cropped region latent space according to the ultra-dense layouts, and refine each sub-regions flexibly during inference using a layout conditional CFG.   We demonstrate the strong results of our system compared to previous SOTA systems such as Flux and SD3 on our BizEval prompt set. Additionally, we conduct thorough ablation experiments to verify the effectiveness of each component. We hope our constructed Infographics-650K and BizEval can encourage the broader community to advance the progress of business content generation.",
    "arxiv_url": "http://arxiv.org/abs/2503.20672v1",
    "pdf_url": "http://arxiv.org/pdf/2503.20672v1",
    "published_date": "2025-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "FLUX",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MMGen: Unified Multi-modal Image Generation and Understanding in One Go",
    "authors": [
      "Jiepeng Wang",
      "Zhaoqing Wang",
      "Hao Pan",
      "Yuan Liu",
      "Dongdong Yu",
      "Changhu Wang",
      "Wenping Wang"
    ],
    "abstract": "A unified diffusion framework for multi-modal generation and understanding has the transformative potential to achieve seamless and controllable image diffusion and other cross-modal tasks. In this paper, we introduce MMGen, a unified framework that integrates multiple generative tasks into a single diffusion model. This includes: (1) multi-modal category-conditioned generation, where multi-modal outputs are generated simultaneously through a single inference process, given category information; (2) multi-modal visual understanding, which accurately predicts depth, surface normals, and segmentation maps from RGB images; and (3) multi-modal conditioned generation, which produces corresponding RGB images based on specific modality conditions and other aligned modalities. Our approach develops a novel diffusion transformer that flexibly supports multi-modal output, along with a simple modality-decoupling strategy to unify various tasks. Extensive experiments and applications demonstrate the effectiveness and superiority of MMGen across diverse tasks and conditions, highlighting its potential for applications that require simultaneous generation and understanding.",
    "arxiv_url": "http://arxiv.org/abs/2503.20644v1",
    "pdf_url": "http://arxiv.org/pdf/2503.20644v1",
    "published_date": "2025-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "Controllable",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Wan: Open and Advanced Large-Scale Video Generative Models",
    "authors": [
      "Team Wan",
      "Ang Wang",
      "Baole Ai",
      "Bin Wen",
      "Chaojie Mao",
      "Chen-Wei Xie",
      "Di Chen",
      "Feiwu Yu",
      "Haiming Zhao",
      "Jianxiao Yang",
      "Jianyuan Zeng",
      "Jiayu Wang",
      "Jingfeng Zhang",
      "Jingren Zhou",
      "Jinkai Wang",
      "Jixuan Chen",
      "Kai Zhu",
      "Kang Zhao",
      "Keyu Yan",
      "Lianghua Huang",
      "Mengyang Feng",
      "Ningyi Zhang",
      "Pandeng Li",
      "Pingyu Wu",
      "Ruihang Chu",
      "Ruili Feng",
      "Shiwei Zhang",
      "Siyang Sun",
      "Tao Fang",
      "Tianxing Wang",
      "Tianyi Gui",
      "Tingyu Weng",
      "Tong Shen",
      "Wei Lin",
      "Wei Wang",
      "Wei Wang",
      "Wenmeng Zhou",
      "Wente Wang",
      "Wenting Shen",
      "Wenyuan Yu",
      "Xianzhong Shi",
      "Xiaoming Huang",
      "Xin Xu",
      "Yan Kou",
      "Yangyu Lv",
      "Yifei Li",
      "Yijing Liu",
      "Yiming Wang",
      "Yingya Zhang",
      "Yitong Huang",
      "Yong Li",
      "You Wu",
      "Yu Liu",
      "Yulin Pan",
      "Yun Zheng",
      "Yuntao Hong",
      "Yupeng Shi",
      "Yutong Feng",
      "Zeyinzi Jiang",
      "Zhen Han",
      "Zhi-Fan Wu",
      "Ziyu Liu"
    ],
    "abstract": "This report presents Wan, a comprehensive and open suite of video foundation models designed to push the boundaries of video generation. Built upon the mainstream diffusion transformer paradigm, Wan achieves significant advancements in generative capabilities through a series of innovations, including our novel VAE, scalable pre-training strategies, large-scale data curation, and automated evaluation metrics. These contributions collectively enhance the model's performance and versatility. Specifically, Wan is characterized by four key features: Leading Performance: The 14B model of Wan, trained on a vast dataset comprising billions of images and videos, demonstrates the scaling laws of video generation with respect to both data and model size. It consistently outperforms the existing open-source models as well as state-of-the-art commercial solutions across multiple internal and external benchmarks, demonstrating a clear and significant performance superiority. Comprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B parameters, for efficiency and effectiveness respectively. It also covers multiple downstream applications, including image-to-video, instruction-guided video editing, and personal video generation, encompassing up to eight tasks. Consumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource efficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range of consumer-grade GPUs. Openness: We open-source the entire series of Wan, including source code and all models, with the goal of fostering the growth of the video generation community. This openness seeks to significantly expand the creative possibilities of video production in the industry and provide academia with high-quality video foundation models. All the code and models are available at https://github.com/Wan-Video/Wan2.1.",
    "arxiv_url": "http://arxiv.org/abs/2503.20314v2",
    "pdf_url": "http://arxiv.org/pdf/2503.20314v2",
    "published_date": "2025-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Wan-Video/Wan2.1",
    "keywords": [
      "diffusion transformer",
      "video generation",
      "video editing"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scaling Down Text Encoders of Text-to-Image Diffusion Models",
    "authors": [
      "Lifu Wang",
      "Daqing Liu",
      "Xinchen Liu",
      "Xiaodong He"
    ],
    "abstract": "Text encoders in diffusion models have rapidly evolved, transitioning from CLIP to T5-XXL. Although this evolution has significantly enhanced the models' ability to understand complex prompts and generate text, it also leads to a substantial increase in the number of parameters. Despite T5 series encoders being trained on the C4 natural language corpus, which includes a significant amount of non-visual data, diffusion models with T5 encoder do not respond to those non-visual prompts, indicating redundancy in representational power. Therefore, it raises an important question: \"Do we really need such a large text encoder?\" In pursuit of an answer, we employ vision-based knowledge distillation to train a series of T5 encoder models. To fully inherit its capabilities, we constructed our dataset based on three criteria: image quality, semantic understanding, and text-rendering. Our results demonstrate the scaling down pattern that the distilled T5-base model can generate images of comparable quality to those produced by T5-XXL, while being 50 times smaller in size. This reduction in model size significantly lowers the GPU requirements for running state-of-the-art models such as FLUX and SD3, making high-quality text-to-image generation more accessible.",
    "arxiv_url": "http://arxiv.org/abs/2503.19897v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19897v1",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "FLUX",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mask$^2$DiT: Dual Mask-based Diffusion Transformer for Multi-Scene Long Video Generation",
    "authors": [
      "Tianhao Qi",
      "Jianlong Yuan",
      "Wanquan Feng",
      "Shancheng Fang",
      "Jiawei Liu",
      "SiYu Zhou",
      "Qian He",
      "Hongtao Xie",
      "Yongdong Zhang"
    ],
    "abstract": "Sora has unveiled the immense potential of the Diffusion Transformer (DiT) architecture in single-scene video generation. However, the more challenging task of multi-scene video generation, which offers broader applications, remains relatively underexplored. To bridge this gap, we propose Mask$^2$DiT, a novel approach that establishes fine-grained, one-to-one alignment between video segments and their corresponding text annotations. Specifically, we introduce a symmetric binary mask at each attention layer within the DiT architecture, ensuring that each text annotation applies exclusively to its respective video segment while preserving temporal coherence across visual tokens. This attention mechanism enables precise segment-level textual-to-visual alignment, allowing the DiT architecture to effectively handle video generation tasks with a fixed number of scenes. To further equip the DiT architecture with the ability to generate additional scenes based on existing ones, we incorporate a segment-level conditional mask, which conditions each newly generated segment on the preceding video segments, thereby enabling auto-regressive scene extension. Both qualitative and quantitative experiments confirm that Mask$^2$DiT excels in maintaining visual consistency across segments while ensuring semantic alignment between each segment and its corresponding text description. Our project page is https://tianhao-qi.github.io/Mask2DiTProject.",
    "arxiv_url": "http://arxiv.org/abs/2503.19881v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19881v1",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion Transformers",
    "authors": [
      "Jiazhi Guan",
      "Kaisiyuan Wang",
      "Zhiliang Xu",
      "Quanwei Yang",
      "Yasheng Sun",
      "Shengyi He",
      "Borong Liang",
      "Yukang Cao",
      "Yingying Li",
      "Haocheng Feng",
      "Errui Ding",
      "Jingdong Wang",
      "Youjian Zhao",
      "Hang Zhou",
      "Ziwei Liu"
    ],
    "abstract": "Despite the recent progress of audio-driven video generation, existing methods mostly focus on driving facial movements, leading to non-coherent head and body dynamics. Moving forward, it is desirable yet challenging to generate holistic human videos with both accurate lip-sync and delicate co-speech gestures w.r.t. given audio. In this work, we propose AudCast, a generalized audio-driven human video generation framework adopting a cascade Diffusion-Transformers (DiTs) paradigm, which synthesizes holistic human videos based on a reference image and a given audio. 1) Firstly, an audio-conditioned Holistic Human DiT architecture is proposed to directly drive the movements of any human body with vivid gesture dynamics. 2) Then to enhance hand and face details that are well-knownly difficult to handle, a Regional Refinement DiT leverages regional 3D fitting as the bridge to reform the signals, producing the final results. Extensive experiments demonstrate that our framework generates high-fidelity audio-driven holistic human videos with temporal coherence and fine facial and hand details. Resources can be found at https://guanjz20.github.io/projects/AudCast.",
    "arxiv_url": "http://arxiv.org/abs/2503.19824v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19824v1",
    "published_date": "2025-03-25",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FuXi-RTM: A Physics-Guided Prediction Framework with Radiative Transfer Modeling",
    "authors": [
      "Qiusheng Huang",
      "Xiaohui Zhong",
      "Xu Fan",
      "Lei Chen",
      "Hao Li"
    ],
    "abstract": "Similar to conventional video generation, current deep learning-based weather prediction frameworks often lack explicit physical constraints, leading to unphysical outputs that limit their reliability for operational forecasting. Among various physical processes requiring proper representation, radiation plays a fundamental role as it drives Earth's weather and climate systems. However, accurate simulation of radiative transfer processes remains challenging for traditional numerical weather prediction (NWP) models due to their inherent complexity and high computational costs. Here, we propose FuXi-RTM, a hybrid physics-guided deep learning framework designed to enhance weather forecast accuracy while enforcing physical consistency. FuXi-RTM integrates a primary forecasting model (FuXi) with a fixed deep learning-based radiative transfer model (DLRTM) surrogate that efficiently replaces conventional radiation parameterization schemes. This represents the first deep learning-based weather forecasting framework to explicitly incorporate physical process modeling. Evaluated over a comprehensive 5-year dataset, FuXi-RTM outperforms its unconstrained counterpart in 88.51% of 3320 variable and lead time combinations, with improvements in radiative flux predictions. By incorporating additional physical processes, FuXi-RTM paves the way for next-generation weather forecasting systems that are both accurate and physically consistent.",
    "arxiv_url": "http://arxiv.org/abs/2503.19940v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19940v1",
    "published_date": "2025-03-25",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
    "authors": [
      "Yuchao Gu",
      "Weijia Mao",
      "Mike Zheng Shou"
    ],
    "abstract": "Long-context video modeling is essential for enabling generative models to function as world simulators, as they must maintain temporal coherence over extended time spans. However, most existing models are trained on short clips, limiting their ability to capture long-range dependencies, even with test-time extrapolation. While training directly on long videos is a natural solution, the rapid growth of vision tokens makes it computationally prohibitive. To support exploring efficient long-context video modeling, we first establish a strong autoregressive baseline called Frame AutoRegressive (FAR). FAR models temporal dependencies between continuous frames, converges faster than video diffusion transformers, and outperforms token-level autoregressive models. Based on this baseline, we observe context redundancy in video autoregression. Nearby frames are critical for maintaining temporal consistency, whereas distant frames primarily serve as context memory. To eliminate this redundancy, we propose the long short-term context modeling using asymmetric patchify kernels, which apply large kernels to distant frames to reduce redundant tokens, and standard kernels to local frames to preserve fine-grained detail. This significantly reduces the training cost of long videos. Our method achieves state-of-the-art results on both short and long video generation, providing an effective baseline for long-context autoregressive video modeling.",
    "arxiv_url": "http://arxiv.org/abs/2503.19325v3",
    "pdf_url": "http://arxiv.org/pdf/2503.19325v3",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Boosting Resolution Generalization of Diffusion Transformers with Randomized Positional Encodings",
    "authors": [
      "Cong Liu",
      "Liang Hou",
      "Mingwu Zheng",
      "Xin Tao",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai"
    ],
    "abstract": "Resolution generalization in image generation tasks enables the production of higher-resolution images with lower training resolution overhead. However, a significant challenge in resolution generalization, particularly in the widely used Diffusion Transformers, lies in the mismatch between the positional encodings encountered during testing and those used during training. While existing methods have employed techniques such as interpolation, extrapolation, or their combinations, none have fully resolved this issue. In this paper, we propose a novel two-dimensional randomized positional encodings (RPE-2D) framework that focuses on learning positional order of image patches instead of the specific distances between them, enabling seamless high- and low-resolution image generation without requiring high- and low-resolution image training. Specifically, RPE-2D independently selects positions over a broader range along both the horizontal and vertical axes, ensuring that all position encodings are trained during the inference phase, thus improving resolution generalization. Additionally, we propose a random data augmentation technique to enhance the modeling of position order. To address the issue of image cropping caused by the augmentation, we introduce corresponding micro-conditioning to enable the model to perceive the specific cropping patterns. On the ImageNet dataset, our proposed RPE-2D achieves state-of-the-art resolution generalization performance, outperforming existing competitive methods when trained at a resolution of $256 \\times 256$ and inferred at $384 \\times 384$ and $512 \\times 512$, as well as when scaling from $512 \\times 512$ to $768 \\times 768$ and $1024 \\times 1024$. And it also exhibits outstanding capabilities in low-resolution image generation, multi-stage training acceleration and multi-resolution inheritance.",
    "arxiv_url": "http://arxiv.org/abs/2503.18719v1",
    "pdf_url": "http://arxiv.org/pdf/2503.18719v1",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models",
    "authors": [
      "Jinjin Zhang",
      "Qiuyu Huang",
      "Junjie Liu",
      "Xiefan Guo",
      "Di Huang"
    ],
    "abstract": "In this paper, we present Diffusion-4K, a novel framework for direct ultra-high-resolution image synthesis using text-to-image diffusion models. The core advancements include: (1) Aesthetic-4K Benchmark: addressing the absence of a publicly available 4K image synthesis dataset, we construct Aesthetic-4K, a comprehensive benchmark for ultra-high-resolution image generation. We curated a high-quality 4K dataset with carefully selected images and captions generated by GPT-4o. Additionally, we introduce GLCM Score and Compression Ratio metrics to evaluate fine details, combined with holistic measures such as FID, Aesthetics and CLIPScore for a comprehensive assessment of ultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a wavelet-based fine-tuning approach for direct training with photorealistic 4K images, applicable to various latent diffusion models, demonstrating its effectiveness in synthesizing highly detailed 4K images. Consequently, Diffusion-4K achieves impressive performance in high-quality image synthesis and text prompt adherence, especially when powered by modern large-scale diffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results from our benchmark demonstrate the superiority of Diffusion-4K in ultra-high-resolution image synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2503.18352v2",
    "pdf_url": "http://arxiv.org/pdf/2503.18352v2",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "FLUX",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating, Fast and Slow: Scalable Parallel Video Generation with Video Interface Networks",
    "authors": [
      "Bhishma Dedhia",
      "David Bourgin",
      "Krishna Kumar Singh",
      "Yuheng Li",
      "Yan Kang",
      "Zhan Xu",
      "Niraj K. Jha",
      "Yuchen Liu"
    ],
    "abstract": "Diffusion Transformers (DiTs) can generate short photorealistic videos, yet directly training and sampling longer videos with full attention across the video remains computationally challenging. Alternative methods break long videos down into sequential generation of short video segments, requiring multiple sampling chain iterations and specialized consistency modules. To overcome these challenges, we introduce a new paradigm called Video Interface Networks (VINs), which augment DiTs with an abstraction module to enable parallel inference of video chunks. At each diffusion step, VINs encode global semantics from the noisy input of local chunks and the encoded representations, in turn, guide DiTs in denoising chunks in parallel. The coupling of VIN and DiT is learned end-to-end on the denoising objective. Further, the VIN architecture maintains fixed-size encoding tokens that encode the input via a single cross-attention step. Disentangling the encoding tokens from the input thus enables VIN to scale to long videos and learn essential semantics. Experiments on VBench demonstrate that VINs surpass existing chunk-based methods in preserving background consistency and subject coherence. We then show via an optical flow analysis that our approach attains state-of-the-art motion smoothness while using 25-40% fewer FLOPs than full generation. Finally, human raters favorably assessed the overall video quality and temporal consistency of our method in a user study.",
    "arxiv_url": "http://arxiv.org/abs/2503.17539v1",
    "pdf_url": "http://arxiv.org/pdf/2503.17539v1",
    "published_date": "2025-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EDiT: Efficient Diffusion Transformers with Linear Compressed Attention",
    "authors": [
      "Philipp Becker",
      "Abhinav Mehrotra",
      "Ruchika Chavhan",
      "Malcolm Chadwick",
      "Luca Morreale",
      "Mehdi Noroozi",
      "Alberto Gil Ramos",
      "Sourav Bhattacharya"
    ],
    "abstract": "Diffusion Transformers (DiTs) have emerged as a leading architecture for text-to-image synthesis, producing high-quality and photorealistic images. However, the quadratic scaling properties of the attention in DiTs hinder image generation with higher resolution or on devices with limited resources. This work introduces an efficient diffusion transformer (EDiT) to alleviate these efficiency bottlenecks in conventional DiTs and Multimodal DiTs (MM-DiTs). First, we present a novel linear compressed attention method that uses a multi-layer convolutional network to modulate queries with local information while keys and values are spatially aggregated. Second, we formulate a hybrid attention scheme for multi-modal inputs that combines linear attention for image-to-image interactions and standard scaled dot-product attention for interactions involving prompts. Merging these two approaches leads to an expressive, linear-time Multimodal Efficient Diffusion Transformer (MM-EDiT). We demonstrate the effectiveness of the EDiT and MM-EDiT architectures by integrating them into PixArt-Sigma(conventional DiT) and Stable Diffusion 3.5-Medium (MM-DiT), achieving up to 2.2x speedup with comparable image quality after distillation.",
    "arxiv_url": "http://arxiv.org/abs/2503.16726v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16726v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
    "authors": [
      "Liming Jiang",
      "Qing Yan",
      "Yumin Jia",
      "Zichuan Liu",
      "Hao Kang",
      "Xin Lu"
    ],
    "abstract": "Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality and aesthetics. Central to InfU is InfuseNet, a component that injects identity features into the DiT base model via residual connections, enhancing identity similarity while maintaining generation capabilities. A multi-stage training strategy, including pretraining and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample (SPMS) data, further improves text-image alignment, ameliorates image quality, and alleviates face copy-pasting. Extensive experiments demonstrate that InfU achieves state-of-the-art performance, surpassing existing baselines. In addition, the plug-and-play design of InfU ensures compatibility with various existing methods, offering a valuable contribution to the broader community.",
    "arxiv_url": "http://arxiv.org/abs/2503.16418v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16418v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "FLUX",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Ultra-Resolution Adaptation with Ease",
    "authors": [
      "Ruonan Yu",
      "Songhua Liu",
      "Zhenxiong Tan",
      "Xinchao Wang"
    ],
    "abstract": "Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key perspectives: data and parameter efficiency, and propose a set of key guidelines for ultra-resolution adaptation termed \\emph{URAE}. For data efficiency, we theoretically and empirically demonstrate that synthetic data generated by some teacher models can significantly promote training convergence. For parameter efficiency, we find that tuning minor components of the weight matrices outperforms widely-used low-rank adapters when synthetic data are unavailable, offering substantial performance gains while maintaining efficiency. Additionally, for models leveraging guidance distillation, such as FLUX, we show that disabling classifier-free guidance, \\textit{i.e.}, setting the guidance scale to 1 during adaptation, is crucial for satisfactory performance. Extensive experiments validate that URAE achieves comparable 2K-generation performance to state-of-the-art closed-source models like FLUX1.1 [Pro] Ultra with only 3K samples and 2K iterations, while setting new benchmarks for 4K-resolution generation. Codes are available \\href{https://github.com/Huage001/URAE}{here}.",
    "arxiv_url": "http://arxiv.org/abs/2503.16322v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16322v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Huage001/URAE",
    "keywords": [
      "text-to-image",
      "FLUX",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FreeFlux: Understanding and Exploiting Layer-Specific Roles in RoPE-Based MMDiT for Versatile Image Editing",
    "authors": [
      "Tianyi Wei",
      "Yifan Zhou",
      "Dongdong Chen",
      "Xingang Pan"
    ],
    "abstract": "The integration of Rotary Position Embedding (RoPE) in Multimodal Diffusion Transformer (MMDiT) has significantly enhanced text-to-image generation quality. However, the fundamental reliance of self-attention layers on positional embedding versus query-key similarity during generation remains an intriguing question. We present the first mechanistic analysis of RoPE-based MMDiT models (e.g., FLUX), introducing an automated probing strategy that disentangles positional information versus content dependencies by strategically manipulating RoPE during generation. Our analysis reveals distinct dependency patterns that do not straightforwardly correlate with depth, offering new insights into the layer-specific roles in RoPE-based MMDiT. Based on these findings, we propose a training-free, task-specific image editing framework that categorizes editing tasks into three types: position-dependent editing (e.g., object addition), content similarity-dependent editing (e.g., non-rigid editing), and region-preserved editing (e.g., background replacement). For each type, we design tailored key-value injection strategies based on the characteristics of the editing task. Extensive qualitative and quantitative evaluations demonstrate that our method outperforms state-of-the-art approaches, particularly in preserving original semantic content and achieving seamless modifications.",
    "arxiv_url": "http://arxiv.org/abs/2503.16153v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16153v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "image editing",
      "image generation",
      "text-to-image",
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation",
    "authors": [
      "Soham Roy",
      "Abhishek Mishra",
      "Shirish Karande",
      "Murari Mandal"
    ],
    "abstract": "Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog",
    "arxiv_url": "http://arxiv.org/abs/2503.16171v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16171v1",
    "published_date": "2025-03-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "FLUX",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers",
    "authors": [
      "Minglei Shi",
      "Ziyang Yuan",
      "Haotian Yang",
      "Xintao Wang",
      "Mingwu Zheng",
      "Xin Tao",
      "Wenliang Zhao",
      "Wenzhao Zheng",
      "Jie Zhou",
      "Jiwen Lu",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai"
    ],
    "abstract": "Diffusion models have demonstrated remarkable success in various image generation tasks, but their performance is often limited by the uniform processing of inputs across varying conditions and noise levels. To address this limitation, we propose a novel approach that leverages the inherent heterogeneity of the diffusion process. Our method, DiffMoE, introduces a batch-level global token pool that enables experts to access global token distributions during training, promoting specialized expert behavior. To unleash the full potential of the diffusion process, DiffMoE incorporates a capacity predictor that dynamically allocates computational resources based on noise levels and sample complexity. Through comprehensive evaluation, DiffMoE achieves state-of-the-art performance among diffusion models on ImageNet benchmark, substantially outperforming both dense architectures with 3x activated parameters and existing MoE approaches while maintaining 1x activated parameters. The effectiveness of our approach extends beyond class-conditional generation to more challenging tasks such as text-to-image generation, demonstrating its broad applicability across different diffusion model applications. Project Page: https://shiml20.github.io/DiffMoE/",
    "arxiv_url": "http://arxiv.org/abs/2503.14487v1",
    "pdf_url": "http://arxiv.org/pdf/2503.14487v1",
    "published_date": "2025-03-18",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Personalize Anything for Free with Diffusion Transformer",
    "authors": [
      "Haoran Feng",
      "Zehuan Huang",
      "Lin Li",
      "Hairong Lv",
      "Lu Sheng"
    ],
    "abstract": "Personalized image generation aims to produce images of user-specified concepts while enabling flexible editing. Recent training-free approaches, while exhibit higher computational efficiency than training-based methods, struggle with identity preservation, applicability, and compatibility with diffusion transformers (DiTs). In this paper, we uncover the untapped potential of DiT, where simply replacing denoising tokens with those of a reference subject achieves zero-shot subject reconstruction. This simple yet effective feature injection technique unlocks diverse scenarios, from personalization to image editing. Building upon this observation, we propose \\textbf{Personalize Anything}, a training-free framework that achieves personalized image generation in DiT through: 1) timestep-adaptive token replacement that enforces subject consistency via early-stage injection and enhances flexibility through late-stage regularization, and 2) patch perturbation strategies to boost structural diversity. Our method seamlessly supports layout-guided generation, multi-subject personalization, and mask-controlled editing. Evaluations demonstrate state-of-the-art performance in identity preservation and versatility. Our work establishes new insights into DiTs while delivering a practical paradigm for efficient personalization.",
    "arxiv_url": "http://arxiv.org/abs/2503.12590v1",
    "pdf_url": "http://arxiv.org/pdf/2503.12590v1",
    "published_date": "2025-03-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "Control",
      "image editing",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  }
]