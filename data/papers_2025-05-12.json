[
  {
    "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition",
    "authors": [
      "Zhiyuan Chen",
      "Keyi Li",
      "Yifan Jia",
      "Le Ye",
      "Yufei Ma"
    ],
    "abstract": "Diffusion transformer (DiT) models have achieved remarkable success in image generation, thanks for their exceptional generative capabilities and scalability. Nonetheless, the iterative nature of diffusion models (DMs) results in high computation complexity, posing challenges for deployment. Although existing cache-based acceleration methods try to utilize the inherent temporal similarity to skip redundant computations of DiT, the lack of correction may induce potential quality degradation. In this paper, we propose increment-calibrated caching, a training-free method for DiT acceleration, where the calibration parameters are generated from the pre-trained model itself with low-rank approximation. To deal with the possible correction failure arising from outlier activations, we introduce channel-aware Singular Value Decomposition (SVD), which further strengthens the calibration effect. Experimental results show that our method always achieve better performance than existing naive caching methods with a similar computation resource budget. When compared with 35-step DDIM, our method eliminates more than 45% computation and improves IS by 12 at the cost of less than 0.06 FID increase. Code is available at https://github.com/ccccczzy/icc.",
    "arxiv_url": "http://arxiv.org/abs/2505.05829v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05829v1",
    "published_date": "2025-05-09",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "https://github.com/ccccczzy/icc",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers",
    "authors": [
      "Divyansh Srivastava",
      "Xiang Zhang",
      "He Wen",
      "Chenru Wen",
      "Zhuowen Tu"
    ],
    "abstract": "We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout generation pipeline for natural scenes. Prior scene layout generation methods are either closed-vocabulary or use proprietary large language models for open-vocabulary generation, limiting their modeling capabilities and broader applicability in controllable image generation. In this work, we propose to use lightweight open-source language models to obtain scene elements from text prompts and a novel aspect-aware diffusion Transformer architecture trained in an open-vocabulary manner for conditional layout generation. Extensive experiments demonstrate that LayouSyn outperforms existing methods and achieves state-of-the-art performance on challenging spatial and numerical reasoning benchmarks. Additionally, we present two applications of LayouSyn. First, we show that coarse initialization from large language models can be seamlessly combined with our method to achieve better results. Second, we present a pipeline for adding objects to images, demonstrating the potential of LayouSyn in image editing applications.",
    "arxiv_url": "http://arxiv.org/abs/2505.04718v1",
    "pdf_url": "http://arxiv.org/pdf/2505.04718v1",
    "published_date": "2025-05-07",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image editing",
      "Control",
      "image generation",
      "diffusion transformer",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deepfakes on Demand: the rise of accessible non-consensual deepfake image generators",
    "authors": [
      "Will Hawkins",
      "Chris Russell",
      "Brent Mittelstadt"
    ],
    "abstract": "Advances in multimodal machine learning have made text-to-image (T2I) models increasingly accessible and popular. However, T2I models introduce risks such as the generation of non-consensual depictions of identifiable individuals, otherwise known as deepfakes. This paper presents an empirical study exploring the accessibility of deepfake model variants online. Through a metadata analysis of thousands of publicly downloadable model variants on two popular repositories, Hugging Face and Civitai, we demonstrate a huge rise in easily accessible deepfake models. Almost 35,000 examples of publicly downloadable deepfake model variants are identified, primarily hosted on Civitai. These deepfake models have been downloaded almost 15 million times since November 2022, with the models targeting a range of individuals from global celebrities to Instagram users with under 10,000 followers. Both Stable Diffusion and Flux models are used for the creation of deepfake models, with 96% of these targeting women and many signalling intent to generate non-consensual intimate imagery (NCII). Deepfake model variants are often created via the parameter-efficient fine-tuning technique known as low rank adaptation (LoRA), requiring as few as 20 images, 24GB VRAM, and 15 minutes of time, making this process widely accessible via consumer-grade computers. Despite these models violating the Terms of Service of hosting platforms, and regulation seeking to prevent dissemination, these results emphasise the pressing need for greater action to be taken against the creation of deepfakes and NCII.",
    "arxiv_url": "http://arxiv.org/abs/2505.03859v1",
    "pdf_url": "http://arxiv.org/pdf/2505.03859v1",
    "published_date": "2025-05-06",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "68T01"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization",
    "authors": [
      "Wenchuan Wang",
      "Mengqi Huang",
      "Yijing Tu",
      "Zhendong Mao"
    ],
    "abstract": "Customized text-to-video generation with pre-trained large-scale models has recently garnered significant attention through focusing on identity and motion consistency. Existing works typically follow the isolated customized paradigm, where the subject identity or motion dynamics are customized exclusively. However, this paradigm completely ignores the intrinsic mutual constraints and synergistic interdependencies between identity and motion, resulting in identity-motion conflicts throughout the generation process that systematically degrades. To address this, we introduce DualReal, a novel framework that, employs adaptive joint training to collaboratively construct interdependencies between dimensions. Specifically, DualReal is composed of two units: (1) Dual-aware Adaptation dynamically selects a training phase (i.e., identity or motion), learns the current information guided by the frozen dimension prior, and employs a regularization strategy to avoid knowledge leakage; (2) StageBlender Controller leverages the denoising stages and Diffusion Transformer depths to guide different dimensions with adaptive granularity, avoiding conflicts at various stages and ultimately achieving lossless fusion of identity and motion patterns. We constructed a more comprehensive benchmark than existing methods. The experimental results show that DualReal improves CLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top performance on nearly all motion quality metrics.",
    "arxiv_url": "http://arxiv.org/abs/2505.02192v1",
    "pdf_url": "http://arxiv.org/pdf/2505.02192v1",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers",
    "authors": [
      "Kwon Byung-Ki",
      "Qi Dai",
      "Lee Hyoseok",
      "Chong Luo",
      "Tae-Hyun Oh"
    ],
    "abstract": "We present JointDiT, a diffusion transformer that models the joint distribution of RGB and depth. By leveraging the architectural benefit and outstanding image prior of the state-of-the-art diffusion transformer, JointDiT not only generates high-fidelity images but also produces geometrically plausible and accurate depth maps. This solid joint distribution modeling is achieved through two simple yet effective techniques that we propose, i.e., adaptive scheduling weights, which depend on the noise levels of each modality, and the unbalanced timestep sampling strategy. With these techniques, we train our model across all noise levels for each modality, enabling JointDiT to naturally handle various combinatorial generation tasks, including joint generation, depth estimation, and depth-conditioned image generation by simply controlling the timestep of each branch. JointDiT demonstrates outstanding joint generation performance. Furthermore, it achieves comparable results in depth estimation and depth-conditioned image generation, suggesting that joint distribution modeling can serve as a replaceable alternative to conditional generation. The project page is available at https://byungki-k.github.io/JointDiT/.",
    "arxiv_url": "http://arxiv.org/abs/2505.00482v1",
    "pdf_url": "http://arxiv.org/pdf/2505.00482v1",
    "published_date": "2025-05-01",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Can We Achieve Efficient Diffusion without Self-Attention? Distilling Self-Attention into Convolutions",
    "authors": [
      "ZiYi Dong",
      "Chengxing Zhou",
      "Weijian Deng",
      "Pengxu Wei",
      "Xiangyang Ji",
      "Liang Lin"
    ],
    "abstract": "Contemporary diffusion models built upon U-Net or Diffusion Transformer (DiT) architectures have revolutionized image generation through transformer-based attention mechanisms. The prevailing paradigm has commonly employed self-attention with quadratic computational complexity to handle global spatial relationships in complex images, thereby synthesizing high-fidelity images with coherent visual semantics.Contrary to conventional wisdom, our systematic layer-wise analysis reveals an interesting discrepancy: self-attention in pre-trained diffusion models predominantly exhibits localized attention patterns, closely resembling convolutional inductive biases. This suggests that global interactions in self-attention may be less critical than commonly assumed.Driven by this, we propose \\(\\Delta\\)ConvFusion to replace conventional self-attention modules with Pyramid Convolution Blocks (\\(\\Delta\\)ConvBlocks).By distilling attention patterns into localized convolutional operations while keeping other components frozen, \\(\\Delta\\)ConvFusion achieves performance comparable to transformer-based counterparts while reducing computational cost by 6929$\\times$ and surpassing LinFusion by 5.42$\\times$ in efficiency--all without compromising generative fidelity.",
    "arxiv_url": "http://arxiv.org/abs/2504.21292v1",
    "pdf_url": "http://arxiv.org/pdf/2504.21292v1",
    "published_date": "2025-04-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer",
    "authors": [
      "Zechuan Zhang",
      "Ji Xie",
      "Yu Lu",
      "Zongxin Yang",
      "Yi Yang"
    ],
    "abstract": "Instruction-based image editing enables robust image modification via natural language prompts, yet current methods face a precision-efficiency tradeoff. Fine-tuning methods demand significant computational resources and large datasets, while training-free techniques struggle with instruction comprehension and edit quality. We resolve this dilemma by leveraging large-scale Diffusion Transformer (DiT)' enhanced generation capacity and native contextual awareness. Our solution introduces three contributions: (1) an in-context editing framework for zero-shot instruction compliance using in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning strategy that enhances flexibility with efficient adaptation and dynamic expert routing, without extensive retraining; and (3) an early filter inference-time scaling method using vision-language models (VLMs) to select better initial noise early, improving edit quality. Extensive evaluations demonstrate our method's superiority: it outperforms state-of-the-art approaches while requiring only 0.5% training data and 1% trainable parameters compared to conventional baselines. This work establishes a new paradigm that enables high-precision yet efficient instruction-guided editing. Codes and demos can be found in https://river-zhang.github.io/ICEdit-gh-pages/.",
    "arxiv_url": "http://arxiv.org/abs/2504.20690v1",
    "pdf_url": "http://arxiv.org/pdf/2504.20690v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image editing",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer",
    "authors": [
      "Junpeng Jiang",
      "Gangyi Hong",
      "Miao Zhang",
      "Hengtong Hu",
      "Kun Zhan",
      "Rui Shao",
      "Liqiang Nie"
    ],
    "abstract": "Collecting multi-view driving scenario videos to enhance the performance of 3D visual perception tasks presents significant challenges and incurs substantial costs, making generative models for realistic data an appealing alternative. Yet, the videos generated by recent works suffer from poor quality and spatiotemporal consistency, undermining their utility in advancing perception tasks under driving scenarios. To address this gap, we propose DiVE, a diffusion transformer-based generative framework meticulously engineered to produce high-fidelity, temporally coherent, and cross-view consistent multi-view videos, aligning seamlessly with bird's-eye view layouts and textual descriptions. DiVE leverages a unified cross-attention and a SketchFormer to exert precise control over multimodal data, while incorporating a view-inflated attention mechanism that adds no extra parameters, thereby guaranteeing consistency across views. Despite these advancements, synthesizing high-resolution videos under multimodal constraints introduces dual challenges: investigating the optimal classifier-free guidance coniguration under intricate multi-condition inputs and mitigating excessive computational latency in high-resolution rendering--both of which remain underexplored in prior researches. To resolve these limitations, we introduce two innovations: Multi-Control Auxiliary Branch Distillation, which streamlines multi-condition CFG selection while circumventing high computational overhead, and Resolution Progressive Sampling, a training-free acceleration strategy that staggers resolution scaling to reduce high latency due to high resolution. These innovations collectively achieve a 2.62x speedup with minimal quality degradation. Evaluated on the nuScenes dataset, DiVE achieves SOTA performance in multi-view video generation, yielding photorealistic outputs with exceptional temporal and cross-view coherence.",
    "arxiv_url": "http://arxiv.org/abs/2504.19614v1",
    "pdf_url": "http://arxiv.org/pdf/2504.19614v1",
    "published_date": "2025-04-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Physics-based super-resolved simulation of 3D elastic wave propagation adopting scalable Diffusion Transformer",
    "authors": [
      "Hugo Gabrielidis",
      "Filippo Gatti",
      "Stéphane Vialle"
    ],
    "abstract": "In this study, we develop a Diffusion Transformer (referred as to DiT1D) for synthesizing realistic earthquake time histories. The DiT1D generates realistic broadband accelerograms (0-30 Hz resolution), constrained at low frequency by 3-dimensional (3D) elastodynamics numerical simulations, ensuring the fulfillment of the minimum observable physics. The DiT1D architecture, successfully adopted in super-resolution image generation, is trained on recorded single-station 3-components (3C) accelerograms. Thanks to Multi-Head Cross-Attention (MHCA) layers, we guide the DiT1D inference by enforcing the low-frequency part of the accelerogram spectrum into it. The DiT1D learns the low-to-high frequency map from the recorded accelerograms, duly normalized, and successfully transfer it to synthetic time histories. The latter are low-frequency by nature, because of the lack of knowledge on the underground structure of the Earth, demanded to fully calibrate the numerical model. We developed a CNN-LSTM lightweight network in conjunction with the DiT1D, so to predict the peak amplitude of the broadband signal from its low-pass-filtered counterpart, and rescale the normalized accelerograms rendered by the DiT1D. Despite the DiT1D being agnostic to any earthquake event peculiarities (magnitude, site conditions, etc.), it showcases remarkable zero-shot prediction realism when applied to the output of validated earthquake simulations. The generated time histories are viable input accelerograms for earthquake-resistant structural design and the pre-trained DiT1D holds a huge potential to integrate full-scale fault-to-structure digital twins of earthquake-prone regions.",
    "arxiv_url": "http://arxiv.org/abs/2504.17308v1",
    "pdf_url": "http://arxiv.org/pdf/2504.17308v1",
    "published_date": "2025-04-24",
    "categories": [
      "physics.geo-ph"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
    "authors": [
      "Theodoros Kouzelis",
      "Efstathios Karypidis",
      "Ioannis Kakogeorgiou",
      "Spyros Gidaris",
      "Nikos Komodakis"
    ],
    "abstract": "Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-level image latents (from a variational autoencoder) and high-level semantic features (from a pretrained self-supervised encoder like DINO). Our latent-semantic diffusion approach learns to generate coherent image-feature pairs from pure noise, significantly enhancing both generative quality and training efficiency, all while requiring only minimal modifications to standard Diffusion Transformer architectures. By eliminating the need for complex distillation objectives, our unified design simplifies training and unlocks a powerful new inference strategy: Representation Guidance, which leverages learned semantics to steer and refine image generation. Evaluated in both conditional and unconditional settings, our method delivers substantial improvements in image quality and training convergence speed, establishing a new direction for representation-aware generative modeling.",
    "arxiv_url": "http://arxiv.org/abs/2504.16064v1",
    "pdf_url": "http://arxiv.org/pdf/2504.16064v1",
    "published_date": "2025-04-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiTPainter: Efficient Video Inpainting with Diffusion Transformers",
    "authors": [
      "Xian Wu",
      "Chang Liu"
    ],
    "abstract": "Many existing video inpainting algorithms utilize optical flows to construct the corresponding maps and then propagate pixels from adjacent frames to missing areas by mapping. Despite the effectiveness of the propagation mechanism, they might encounter blurry and inconsistencies when dealing with inaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT) has emerged as a revolutionary technique for video generation tasks. However, pretrained DiT models for video generation all contain a large amount of parameters, which makes it very time consuming to apply to video inpainting tasks. In this paper, we present DiTPainter, an end-to-end video inpainting model based on Diffusion Transformer (DiT). DiTPainter uses an efficient transformer network designed for video inpainting, which is trained from scratch instead of initializing from any large pretrained models. DiTPainter can address videos with arbitrary lengths and can be applied to video decaptioning and video completion tasks with an acceptable time cost. Experiments show that DiTPainter outperforms existing video inpainting algorithms with higher quality and better spatial-temporal consistency.",
    "arxiv_url": "http://arxiv.org/abs/2504.15661v1",
    "pdf_url": "http://arxiv.org/pdf/2504.15661v1",
    "published_date": "2025-04-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video inpainting",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Acquire and then Adapt: Squeezing out Text-to-Image Model for Image Restoration",
    "authors": [
      "Junyuan Deng",
      "Xinyi Wu",
      "Yongxing Yang",
      "Congchao Zhu",
      "Song Wang",
      "Zhenyao Wu"
    ],
    "abstract": "Recently, pre-trained text-to-image (T2I) models have been extensively adopted for real-world image restoration because of their powerful generative prior. However, controlling these large models for image restoration usually requires a large number of high-quality images and immense computational resources for training, which is costly and not privacy-friendly. In this paper, we find that the well-trained large T2I model (i.e., Flux) is able to produce a variety of high-quality images aligned with real-world distributions, offering an unlimited supply of training samples to mitigate the above issue. Specifically, we proposed a training data construction pipeline for image restoration, namely FluxGen, which includes unconditional image generation, image selection, and degraded image simulation. A novel light-weighted adapter (FluxIR) with squeeze-and-excitation layers is also carefully designed to control the large Diffusion Transformer (DiT)-based T2I model so that reasonable details can be restored. Experiments demonstrate that our proposed method enables the Flux model to adapt effectively to real-world image restoration tasks, achieving superior scores and visual quality on both synthetic and real-world degradation datasets - at only about 8.5\\% of the training cost compared to current approaches.",
    "arxiv_url": "http://arxiv.org/abs/2504.15159v1",
    "pdf_url": "http://arxiv.org/pdf/2504.15159v1",
    "published_date": "2025-04-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Turbo2K: Towards Ultra-Efficient and High-Quality 2K Video Synthesis",
    "authors": [
      "Jingjing Ren",
      "Wenbo Li",
      "Zhongdao Wang",
      "Haoze Sun",
      "Bangzhen Liu",
      "Haoyu Chen",
      "Jiaqi Xu",
      "Aoxue Li",
      "Shifeng Zhang",
      "Bin Shao",
      "Yong Guo",
      "Lei Zhu"
    ],
    "abstract": "Demand for 2K video synthesis is rising with increasing consumer expectations for ultra-clear visuals. While diffusion transformers (DiTs) have demonstrated remarkable capabilities in high-quality video generation, scaling them to 2K resolution remains computationally prohibitive due to quadratic growth in memory and processing costs. In this work, we propose Turbo2K, an efficient and practical framework for generating detail-rich 2K videos while significantly improving training and inference efficiency. First, Turbo2K operates in a highly compressed latent space, reducing computational complexity and memory footprint, making high-resolution video synthesis feasible. However, the high compression ratio of the VAE and limited model size impose constraints on generative quality. To mitigate this, we introduce a knowledge distillation strategy that enables a smaller student model to inherit the generative capacity of a larger, more powerful teacher model. Our analysis reveals that, despite differences in latent spaces and architectures, DiTs exhibit structural similarities in their internal representations, facilitating effective knowledge transfer. Second, we design a hierarchical two-stage synthesis framework that first generates multi-level feature at lower resolutions before guiding high-resolution video generation. This approach ensures structural coherence and fine-grained detail refinement while eliminating redundant encoding-decoding overhead, further enhancing computational efficiency.Turbo2K achieves state-of-the-art efficiency, generating 5-second, 24fps, 2K videos with significantly reduced computational cost. Compared to existing methods, Turbo2K is up to 20$\\times$ faster for inference, making high-resolution video generation more scalable and practical for real-world applications.",
    "arxiv_url": "http://arxiv.org/abs/2504.14470v1",
    "pdf_url": "http://arxiv.org/pdf/2504.14470v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Text-Audio-Visual-conditioned Diffusion Model for Video Saliency Prediction",
    "authors": [
      "Li Yu",
      "Xuanzhe Sun",
      "Wei Zhou",
      "Moncef Gabbouj"
    ],
    "abstract": "Video saliency prediction is crucial for downstream applications, such as video compression and human-computer interaction. With the flourishing of multimodal learning, researchers started to explore multimodal video saliency prediction, including audio-visual and text-visual approaches. Auditory cues guide the gaze of viewers to sound sources, while textual cues provide semantic guidance for understanding video content. Integrating these complementary cues can improve the accuracy of saliency prediction. Therefore, we attempt to simultaneously analyze visual, auditory, and textual modalities in this paper, and propose TAVDiff, a Text-Audio-Visual-conditioned Diffusion Model for video saliency prediction. TAVDiff treats video saliency prediction as an image generation task conditioned on textual, audio, and visual inputs, and predicts saliency maps through stepwise denoising. To effectively utilize text, a large multimodal model is used to generate textual descriptions for video frames and introduce a saliency-oriented image-text response (SITR) mechanism to generate image-text response maps. It is used as conditional information to guide the model to localize the visual regions that are semantically related to the textual description. Regarding the auditory modality, it is used as another conditional information for directing the model to focus on salient regions indicated by sounds. At the same time, since the diffusion transformer (DiT) directly concatenates the conditional information with the timestep, which may affect the estimation of the noise level. To achieve effective conditional guidance, we propose Saliency-DiT, which decouples the conditional information from the timestep. Experimental results show that TAVDiff outperforms existing methods, improving 1.03\\%, 2.35\\%, 2.71\\% and 0.33\\% on SIM, CC, NSS and AUC-J metrics, respectively.",
    "arxiv_url": "http://arxiv.org/abs/2504.14267v1",
    "pdf_url": "http://arxiv.org/pdf/2504.14267v1",
    "published_date": "2025-04-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Entropy Rectifying Guidance for Diffusion and Flow Models",
    "authors": [
      "Tariq Berrada Ifriqi",
      "Adriana Romero-Soriano",
      "Michal Drozdzal",
      "Jakob Verbeek",
      "Karteek Alahari"
    ],
    "abstract": "Guidance techniques are commonly used in diffusion and flow models to improve image quality and consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) -- the most widely adopted guidance technique -- contrasts conditional and unconditional predictions to improve the generated images. This results, however, in trade-offs across quality, diversity and consistency, improving some at the expense of others. While recent work has shown that it is possible to disentangle these factors to some extent, such methods come with an overhead of requiring an additional (weaker) model, or require more forward passes per sampling step. In this paper, we propose Entropy Rectifying Guidance (ERG), a simple and effective guidance mechanism based on inference-time changes in the attention mechanism of state-of-the-art diffusion transformer architectures, which allows for simultaneous improvements over image quality, diversity and prompt consistency. ERG is more general than CFG and similar guidance techniques, as it extends to unconditional sampling. ERG results in significant improvements in various generation tasks such as text-to-image, class-conditional and unconditional image generation. We also show that ERG can be seamlessly combined with other recent guidance methods such as CADS and APG, further boosting generation performance.",
    "arxiv_url": "http://arxiv.org/abs/2504.13987v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13987v1",
    "published_date": "2025-04-18",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InstantCharacter: Personalize Any Characters with a Scalable Diffusion Transformer Framework",
    "authors": [
      "Jiale Tao",
      "Yanbing Zhang",
      "Qixun Wang",
      "Yiji Cheng",
      "Haofan Wang",
      "Xu Bai",
      "Zhengguang Zhou",
      "Ruihuang Li",
      "Linqing Wang",
      "Chunyu Wang",
      "Qin Lin",
      "Qinglin Lu"
    ],
    "abstract": "Current learning-based subject customization approaches, predominantly relying on U-Net architectures, suffer from limited generalization ability and compromised image quality. Meanwhile, optimization-based methods require subject-specific fine-tuning, which inevitably degrades textual controllability. To address these challenges, we propose InstantCharacter, a scalable framework for character customization built upon a foundation diffusion transformer. InstantCharacter demonstrates three fundamental advantages: first, it achieves open-domain personalization across diverse character appearances, poses, and styles while maintaining high-fidelity results. Second, the framework introduces a scalable adapter with stacked transformer encoders, which effectively processes open-domain character features and seamlessly interacts with the latent space of modern diffusion transformers. Third, to effectively train the framework, we construct a large-scale character dataset containing 10-million-level samples. The dataset is systematically organized into paired (multi-view character) and unpaired (text-image combinations) subsets. This dual-data structure enables simultaneous optimization of identity consistency and textual editability through distinct learning pathways. Qualitative experiments demonstrate the advanced capabilities of InstantCharacter in generating high-fidelity, text-controllable, and character-consistent images, setting a new benchmark for character-driven image generation. Our source code is available at https://github.com/Tencent/InstantCharacter.",
    "arxiv_url": "http://arxiv.org/abs/2504.12395v1",
    "pdf_url": "http://arxiv.org/pdf/2504.12395v1",
    "published_date": "2025-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Tencent/InstantCharacter",
    "keywords": [
      "Control",
      "image generation",
      "diffusion transformer",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VGDFR: Diffusion-based Video Generation with Dynamic Latent Frame Rate",
    "authors": [
      "Zhihang Yuan",
      "Rui Xie",
      "Yuzhang Shang",
      "Hanling Zhang",
      "Siyuan Wang",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ],
    "abstract": "Diffusion Transformer(DiT)-based generation models have achieved remarkable success in video generation. However, their inherent computational demands pose significant efficiency challenges. In this paper, we exploit the inherent temporal non-uniformity of real-world videos and observe that videos exhibit dynamic information density, with high-motion segments demanding greater detail preservation than static scenes. Inspired by this temporal non-uniformity, we propose VGDFR, a training-free approach for Diffusion-based Video Generation with Dynamic Latent Frame Rate. VGDFR adaptively adjusts the number of elements in latent space based on the motion frequency of the latent space content, using fewer tokens for low-frequency segments while preserving detail in high-frequency segments. Specifically, our key contributions are: (1) A dynamic frame rate scheduler for DiT video generation that adaptively assigns frame rates for video segments. (2) A novel latent-space frame merging method to align latent representations with their denoised counterparts before merging those redundant in low-resolution space. (3) A preference analysis of Rotary Positional Embeddings (RoPE) across DiT layers, informing a tailored RoPE strategy optimized for semantic and local information capture. Experiments show that VGDFR can achieve a speedup up to 3x for video generation with minimal quality degradation.",
    "arxiv_url": "http://arxiv.org/abs/2504.12259v1",
    "pdf_url": "http://arxiv.org/pdf/2504.12259v1",
    "published_date": "2025-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Using LLMs as prompt modifier to avoid biases in AI image generators",
    "authors": [
      "René Peinl"
    ],
    "abstract": "This study examines how Large Language Models (LLMs) can reduce biases in text-to-image generation systems by modifying user prompts. We define bias as a model's unfair deviation from population statistics given neutral prompts. Our experiments with Stable Diffusion XL, 3.5 and Flux demonstrate that LLM-modified prompts significantly increase image diversity and reduce bias without the need to change the image generators themselves. While occasionally producing results that diverge from original user intent for elaborate prompts, this approach generally provides more varied interpretations of underspecified requests rather than superficial variations. The method works particularly well for less advanced image generators, though limitations persist for certain contexts like disability representation. All prompts and generated images are available at https://iisys-hof.github.io/llm-prompt-img-gen/",
    "arxiv_url": "http://arxiv.org/abs/2504.11104v1",
    "pdf_url": "http://arxiv.org/pdf/2504.11104v1",
    "published_date": "2025-04-15",
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.CY"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Analysis of Attention in Video Diffusion Transformers",
    "authors": [
      "Yuxin Wen",
      "Jim Wu",
      "Ajay Jain",
      "Tom Goldstein",
      "Ashwinee Panda"
    ],
    "abstract": "We conduct an in-depth analysis of attention in video diffusion transformers (VDiTs) and report a number of novel findings. We identify three key properties of attention in VDiTs: Structure, Sparsity, and Sinks. Structure: We observe that attention patterns across different VDiTs exhibit similar structure across different prompts, and that we can make use of the similarity of attention patterns to unlock video editing via self-attention map transfer. Sparse: We study attention sparsity in VDiTs, finding that proposed sparsity methods do not work for all VDiTs, because some layers that are seemingly sparse cannot be sparsified. Sinks: We make the first study of attention sinks in VDiTs, comparing and contrasting them to attention sinks in language models. We propose a number of future directions that can make use of our insights to improve the efficiency-quality Pareto frontier for VDiTs.",
    "arxiv_url": "http://arxiv.org/abs/2504.10317v1",
    "pdf_url": "http://arxiv.org/pdf/2504.10317v1",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video editing",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D$^2$iT: Dynamic Diffusion Transformer for Accurate Image Generation",
    "authors": [
      "Weinan Jia",
      "Mengqi Huang",
      "Nan Chen",
      "Lei Zhang",
      "Zhendong Mao"
    ],
    "abstract": "Diffusion models are widely recognized for their ability to generate high-fidelity images. Despite the excellent performance and scalability of the Diffusion Transformer (DiT) architecture, it applies fixed compression across different image regions during the diffusion process, disregarding the naturally varying information densities present in these regions. However, large compression leads to limited local realism, while small compression increases computational complexity and compromises global consistency, ultimately impacting the quality of generated images. To address these limitations, we propose dynamically compressing different image regions by recognizing the importance of different regions, and introduce a novel two-stage framework designed to enhance the effectiveness and efficiency of image generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical encoder to encode different image regions at different downsampling rates, tailored to their specific information densities, thereby providing more accurate and natural latent codes for the diffusion process. (2) Dynamic Diffusion Transformer (D$^2$iT) at second stage generates images by predicting multi-grained noise, consisting of coarse-grained (less latent code in smooth regions) and fine-grained (more latent codes in detailed regions), through an novel combination of the Dynamic Grain Transformer and the Dynamic Content Transformer. The strategy of combining rough prediction of noise with detailed regions correction achieves a unification of global consistency and local realism. Comprehensive experiments on various generation tasks validate the effectiveness of our approach. Code will be released at https://github.com/jiawn-creator/Dynamic-DiT.",
    "arxiv_url": "http://arxiv.org/abs/2504.09454v1",
    "pdf_url": "http://arxiv.org/pdf/2504.09454v1",
    "published_date": "2025-04-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/jiawn-creator/Dynamic-DiT",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Flux Already Knows -- Activating Subject-Driven Image Generation without Training",
    "authors": [
      "Hao Kang",
      "Stathi Fotiadis",
      "Liming Jiang",
      "Qing Yan",
      "Yumin Jia",
      "Zichuan Liu",
      "Min Jin Chong",
      "Xin Lu"
    ],
    "abstract": "We propose a simple yet effective zero-shot framework for subject-driven image generation using a vanilla Flux model. By framing the task as grid-based image completion and simply replicating the subject image(s) in a mosaic layout, we activate strong identity-preserving capabilities without any additional data, training, or inference-time fine-tuning. This \"free lunch\" approach is further strengthened by a novel cascade attention design and meta prompting technique, boosting fidelity and versatility. Experimental results show that our method outperforms baselines across multiple key metrics in benchmarks and human preference studies, with trade-offs in certain aspects. Additionally, it supports diverse edits, including logo insertion, virtual try-on, and subject replacement or insertion. These results demonstrate that a pre-trained foundational text-to-image model can enable high-quality, resource-efficient subject-driven generation, opening new possibilities for lightweight customization in downstream applications.",
    "arxiv_url": "http://arxiv.org/abs/2504.11478v2",
    "pdf_url": "http://arxiv.org/pdf/2504.11478v2",
    "published_date": "2025-04-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MixDiT: Accelerating Image Diffusion Transformer Inference with Mixed-Precision MX Quantization",
    "authors": [
      "Daeun Kim",
      "Jinwoo Hwang",
      "Changhun Oh",
      "Jongse Park"
    ],
    "abstract": "Diffusion Transformer (DiT) has driven significant progress in image generation tasks. However, DiT inferencing is notoriously compute-intensive and incurs long latency even on datacenter-scale GPUs, primarily due to its iterative nature and heavy reliance on GEMM operations inherent to its encoder-based structure. To address the challenge, prior work has explored quantization, but achieving low-precision quantization for DiT inferencing with both high accuracy and substantial speedup remains an open problem. To this end, this paper proposes MixDiT, an algorithm-hardware co-designed acceleration solution that exploits mixed Microscaling (MX) formats to quantize DiT activation values. MixDiT quantizes the DiT activation tensors by selectively applying higher precision to magnitude-based outliers, which produce mixed-precision GEMM operations. To achieve tangible speedup from the mixed-precision arithmetic, we design a MixDiT accelerator that enables precision-flexible multiplications and efficient MX precision conversions. Our experimental results show that MixDiT delivers a speedup of 2.10-5.32 times over RTX 3090, with no loss in FID.",
    "arxiv_url": "http://arxiv.org/abs/2504.08398v1",
    "pdf_url": "http://arxiv.org/pdf/2504.08398v1",
    "published_date": "2025-04-11",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion Transformers for Tabular Data Time Series Generation",
    "authors": [
      "Fabrizio Garuti",
      "Enver Sangineto",
      "Simone Luetto",
      "Lorenzo Forni",
      "Rita Cucchiara"
    ],
    "abstract": "Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, generating time series of tabular data, where each element of the series depends on the others, remains a largely unexplored domain. This gap is probably due to the difficulty of jointly solving different problems, the main of which are the heterogeneity of tabular data (a problem common to non-time-dependent approaches) and the variable length of a time series. In this paper, we propose a Diffusion Transformers (DiTs) based approach for tabular data series generation. Inspired by the recent success of DiTs in image and video generation, we extend this framework to deal with heterogeneous data and variable-length sequences. Using extensive experiments on six datasets, we show that the proposed approach outperforms previous work by a large margin.",
    "arxiv_url": "http://arxiv.org/abs/2504.07566v2",
    "pdf_url": "http://arxiv.org/pdf/2504.07566v2",
    "published_date": "2025-04-10",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation",
    "authors": [
      "Wangbo Zhao",
      "Yizeng Han",
      "Jiasheng Tang",
      "Kai Wang",
      "Hao Luo",
      "Yibing Song",
      "Gao Huang",
      "Fan Wang",
      "Yang You"
    ],
    "abstract": "Diffusion Transformer (DiT), an emerging diffusion model for visual generation, has demonstrated superior performance but suffers from substantial computational costs. Our investigations reveal that these costs primarily stem from the \\emph{static} inference paradigm, which inevitably introduces redundant computation in certain \\emph{diffusion timesteps} and \\emph{spatial regions}. To overcome this inefficiency, we propose \\textbf{Dy}namic \\textbf{Di}ffusion \\textbf{T}ransformer (DyDiT), an architecture that \\emph{dynamically} adjusts its computation along both \\emph{timestep} and \\emph{spatial} dimensions. Specifically, we introduce a \\emph{Timestep-wise Dynamic Width} (TDW) approach that adapts model width conditioned on the generation timesteps. In addition, we design a \\emph{Spatial-wise Dynamic Token} (SDT) strategy to avoid redundant computation at unnecessary spatial locations. TDW and SDT can be seamlessly integrated into DiT and significantly accelerates the generation process. Building on these designs, we further enhance DyDiT in three key aspects. First, DyDiT is integrated seamlessly with flow matching-based generation, enhancing its versatility. Furthermore, we enhance DyDiT to tackle more complex visual generation tasks, including video generation and text-to-image generation, thereby broadening its real-world applications. Finally, to address the high cost of full fine-tuning and democratize technology access, we investigate the feasibility of training DyDiT in a parameter-efficient manner and introduce timestep-based dynamic LoRA (TD-LoRA). Extensive experiments on diverse visual generation models, including DiT, SiT, Latte, and FLUX, demonstrate the effectiveness of DyDiT.",
    "arxiv_url": "http://arxiv.org/abs/2504.06803v2",
    "pdf_url": "http://arxiv.org/pdf/2504.06803v2",
    "published_date": "2025-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "video generation",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Disentangling Instruction Influence in Diffusion Transformers for Parallel Multi-Instruction-Guided Image Editing",
    "authors": [
      "Hui Liu",
      "Bin Zou",
      "Suiyun Zhang",
      "Kecheng Chen",
      "Rui Liu",
      "Haoliang Li"
    ],
    "abstract": "Instruction-guided image editing enables users to specify modifications using natural language, offering more flexibility and control. Among existing frameworks, Diffusion Transformers (DiTs) outperform U-Net-based diffusion models in scalability and performance. However, while real-world scenarios often require concurrent execution of multiple instructions, step-by-step editing suffers from accumulated errors and degraded quality, and integrating multiple instructions with a single prompt usually results in incomplete edits due to instruction conflicts. We propose Instruction Influence Disentanglement (IID), a novel framework enabling parallel execution of multiple instructions in a single denoising process, designed for DiT-based models. By analyzing self-attention mechanisms in DiTs, we identify distinctive attention patterns in multi-instruction settings and derive instruction-specific attention masks to disentangle each instruction's influence. These masks guide the editing process to ensure localized modifications while preserving consistency in non-edited regions. Extensive experiments on open-source and custom datasets demonstrate that IID reduces diffusion steps while improving fidelity and instruction completion compared to existing baselines. The codes will be publicly released upon the acceptance of the paper.",
    "arxiv_url": "http://arxiv.org/abs/2504.04784v1",
    "pdf_url": "http://arxiv.org/pdf/2504.04784v1",
    "published_date": "2025-04-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image editing",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion",
    "authors": [
      "Maksim Siniukov",
      "Di Chang",
      "Minh Tran",
      "Hongkun Gong",
      "Ashutosh Chaubey",
      "Mohammad Soleymani"
    ],
    "abstract": "Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address these challenges, we introduce DiTaiListener, powered by a video diffusion model with multimodal conditions. Our approach first generates short segments of listener responses conditioned on the speaker's speech and facial motions with DiTaiListener-Gen. It then refines the transitional frames via DiTaiListener-Edit for a seamless transition. Specifically, DiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener head portrait generation by introducing a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter integrates speakers' input in a causal manner into the video generation process to ensure temporally coherent listener responses. For long-form video generation, we introduce DiTaiListener-Edit, a transition refinement video-to-video diffusion model. The model fuses video segments into smooth and continuous videos, ensuring temporal consistency in facial expressions and image quality when merging short video segments produced by DiTaiListener-Gen. Quantitatively, DiTaiListener achieves the state-of-the-art performance on benchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO) spaces. User studies confirm the superior performance of DiTaiListener, with the model being the clear preference in terms of feedback, diversity, and smoothness, outperforming competitors by a significant margin.",
    "arxiv_url": "http://arxiv.org/abs/2504.04010v1",
    "pdf_url": "http://arxiv.org/pdf/2504.04010v1",
    "published_date": "2025-04-05",
    "categories": [
      "cs.CV",
      "cs.LG",
      "I.4.9"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Detection Limits and Statistical Separability of Tree Ring Watermarks in Rectified Flow-based Text-to-Image Generation Models",
    "authors": [
      "Ved Umrajkar",
      "Aakash Kumar Singh"
    ],
    "abstract": "Tree-Ring Watermarking is a significant technique for authenticating AI-generated images. However, its effectiveness in rectified flow-based models remains unexplored, particularly given the inherent challenges of these models with noise latent inversion. Through extensive experimentation, we evaluated and compared the detection and separability of watermarks between SD 2.1 and FLUX.1-dev models. By analyzing various text guidance configurations and augmentation attacks, we demonstrate how inversion limitations affect both watermark recovery and the statistical separation between watermarked and unwatermarked images. Our findings provide valuable insights into the current limitations of Tree-Ring Watermarking in the current SOTA models and highlight the critical need for improved inversion methods to achieve reliable watermark detection and separability. The official implementation, dataset release and all experimental results are available at this \\href{https://github.com/dsgiitr/flux-watermarking}{\\textbf{link}}.",
    "arxiv_url": "http://arxiv.org/abs/2504.03850v1",
    "pdf_url": "http://arxiv.org/pdf/2504.03850v1",
    "published_date": "2025-04-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ],
    "github_url": "https://github.com/dsgiitr/flux-watermarking",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation",
      "inversion",
      "rectified flow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
    "authors": [
      "Zhengcong Fei",
      "Debang Li",
      "Di Qiu",
      "Jiahua Wang",
      "Yikun Dou",
      "Rui Wang",
      "Jingtao Xu",
      "Mingyuan Fan",
      "Guibin Chen",
      "Yang Li",
      "Yahui Zhou"
    ],
    "abstract": "This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.",
    "arxiv_url": "http://arxiv.org/abs/2504.02436v1",
    "pdf_url": "http://arxiv.org/pdf/2504.02436v1",
    "published_date": "2025-04-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Less-to-More Generalization: Unlocking More Controllability by In-Context Generation",
    "authors": [
      "Shaojin Wu",
      "Mengqi Huang",
      "Wenxu Wu",
      "Yufeng Cheng",
      "Fei Ding",
      "Qian He"
    ],
    "abstract": "Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation.",
    "arxiv_url": "http://arxiv.org/abs/2504.02160v1",
    "pdf_url": "http://arxiv.org/pdf/2504.02160v1",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiT4SR: Taming Diffusion Transformer for Real-World Image Super-Resolution",
    "authors": [
      "Zheng-Peng Duan",
      "Jiawei Zhang",
      "Xin Jin",
      "Ziheng Zhang",
      "Zheng Xiong",
      "Dongqing Zou",
      "Jimmy Ren",
      "Chun-Le Guo",
      "Chongyi Li"
    ],
    "abstract": "Large-scale pre-trained diffusion models are becoming increasingly popular in solving the Real-World Image Super-Resolution (Real-ISR) problem because of their rich generative priors. The recent development of diffusion transformer (DiT) has witnessed overwhelming performance over the traditional UNet-based architecture in image generation, which also raises the question: Can we adopt the advanced DiT-based diffusion model for Real-ISR? To this end, we propose our DiT4SR, one of the pioneering works to tame the large-scale DiT model for Real-ISR. Instead of directly injecting embeddings extracted from low-resolution (LR) images like ControlNet, we integrate the LR embeddings into the original attention mechanism of DiT, allowing for the bidirectional flow of information between the LR latent and the generated latent. The sufficient interaction of these two streams allows the LR stream to evolve with the diffusion process, producing progressively refined guidance that better aligns with the generated latent at each diffusion step. Additionally, the LR guidance is injected into the generated latent via a cross-stream convolution layer, compensating for DiT's limited ability to capture local information. These simple but effective designs endow the DiT model with superior performance in Real-ISR, which is demonstrated by extensive experiments. Project Page: https://adam-duan.github.io/projects/dit4sr/.",
    "arxiv_url": "http://arxiv.org/abs/2503.23580v1",
    "pdf_url": "http://arxiv.org/pdf/2503.23580v1",
    "published_date": "2025-03-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image generation",
      "diffusion transformer",
      "image super-resolution"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization",
    "authors": [
      "Kai Liu",
      "Wei Li",
      "Lai Chen",
      "Shengqiong Wu",
      "Yanhao Zheng",
      "Jiayi Ji",
      "Fan Zhou",
      "Rongxin Jiang",
      "Jiebo Luo",
      "Hao Fei",
      "Tat-Seng Chua"
    ],
    "abstract": "This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion Transformer designed for synchronized audio-video generation (JAVG). Built upon the powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to generate high-quality audio and video content simultaneously from open-ended user prompts. To ensure optimal synchronization, we introduce a fine-grained spatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator. This module extracts both global and fine-grained spatio-temporal priors, guiding the synchronization between the visual and auditory components. Furthermore, we propose a new benchmark, JavisBench, consisting of 10,140 high-quality text-captioned sounding videos spanning diverse scenes and complex real-world scenarios. Further, we specifically devise a robust metric for evaluating the synchronization between generated audio-video pairs in real-world complex content. Experimental results demonstrate that JavisDiT significantly outperforms existing methods by ensuring both high-quality generation and precise synchronization, setting a new standard for JAVG tasks. Our code, model, and dataset will be made publicly available at https://javisdit.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2503.23377v1",
    "pdf_url": "http://arxiv.org/pdf/2503.23377v1",
    "published_date": "2025-03-30",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality Diffusion Transformers",
    "authors": [
      "Hanling Zhang",
      "Rundong Su",
      "Zhihang Yuan",
      "Pengtao Chen",
      "Mingzhu Shen Yibo Fan",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ],
    "abstract": "Text-to-image generation models, especially Multimodal Diffusion Transformers (MMDiT), have shown remarkable progress in generating high-quality images. However, these models often face significant computational bottlenecks, particularly in attention mechanisms, which hinder their scalability and efficiency. In this paper, we introduce DiTFastAttnV2, a post-training compression method designed to accelerate attention in MMDiT. Through an in-depth analysis of MMDiT's attention patterns, we identify key differences from prior DiT-based methods and propose head-wise arrow attention and caching mechanisms to dynamically adjust attention heads, effectively bridging this gap. We also design an Efficient Fused Kernel for further acceleration. By leveraging local metric methods and optimization techniques, our approach significantly reduces the search time for optimal compression schemes to just minutes while maintaining generation quality. Furthermore, with the customized kernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x end-to-end speedup on 2K image generation without compromising visual fidelity.",
    "arxiv_url": "http://arxiv.org/abs/2503.22796v1",
    "pdf_url": "http://arxiv.org/pdf/2503.22796v1",
    "published_date": "2025-03-28",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation",
    "authors": [
      "Yuyang Peng",
      "Shishi Xiao",
      "Keming Wu",
      "Qisheng Liao",
      "Bohan Chen",
      "Kevin Lin",
      "Danqing Huang",
      "Ji Li",
      "Yuhui Yuan"
    ],
    "abstract": "Recently, state-of-the-art text-to-image generation models, such as Flux and Ideogram 2.0, have made significant progress in sentence-level visual text rendering. In this paper, we focus on the more challenging scenarios of article-level visual text rendering and address a novel task of generating high-quality business content, including infographics and slides, based on user provided article-level descriptive prompts and ultra-dense layouts. The fundamental challenges are twofold: significantly longer context lengths and the scarcity of high-quality business content data.   In contrast to most previous works that focus on a limited number of sub-regions and sentence-level prompts, ensuring precise adherence to ultra-dense layouts with tens or even hundreds of sub-regions in business content is far more challenging. We make two key technical contributions: (i) the construction of scalable, high-quality business content dataset, i.e., Infographics-650K, equipped with ultra-dense layouts and prompts by implementing a layer-wise retrieval-augmented infographic generation scheme; and (ii) a layout-guided cross attention scheme, which injects tens of region-wise prompts into a set of cropped region latent space according to the ultra-dense layouts, and refine each sub-regions flexibly during inference using a layout conditional CFG.   We demonstrate the strong results of our system compared to previous SOTA systems such as Flux and SD3 on our BizEval prompt set. Additionally, we conduct thorough ablation experiments to verify the effectiveness of each component. We hope our constructed Infographics-650K and BizEval can encourage the broader community to advance the progress of business content generation.",
    "arxiv_url": "http://arxiv.org/abs/2503.20672v1",
    "pdf_url": "http://arxiv.org/pdf/2503.20672v1",
    "published_date": "2025-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MMGen: Unified Multi-modal Image Generation and Understanding in One Go",
    "authors": [
      "Jiepeng Wang",
      "Zhaoqing Wang",
      "Hao Pan",
      "Yuan Liu",
      "Dongdong Yu",
      "Changhu Wang",
      "Wenping Wang"
    ],
    "abstract": "A unified diffusion framework for multi-modal generation and understanding has the transformative potential to achieve seamless and controllable image diffusion and other cross-modal tasks. In this paper, we introduce MMGen, a unified framework that integrates multiple generative tasks into a single diffusion model. This includes: (1) multi-modal category-conditioned generation, where multi-modal outputs are generated simultaneously through a single inference process, given category information; (2) multi-modal visual understanding, which accurately predicts depth, surface normals, and segmentation maps from RGB images; and (3) multi-modal conditioned generation, which produces corresponding RGB images based on specific modality conditions and other aligned modalities. Our approach develops a novel diffusion transformer that flexibly supports multi-modal output, along with a simple modality-decoupling strategy to unify various tasks. Extensive experiments and applications demonstrate the effectiveness and superiority of MMGen across diverse tasks and conditions, highlighting its potential for applications that require simultaneous generation and understanding.",
    "arxiv_url": "http://arxiv.org/abs/2503.20644v1",
    "pdf_url": "http://arxiv.org/pdf/2503.20644v1",
    "published_date": "2025-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image generation",
      "diffusion transformer",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Wan: Open and Advanced Large-Scale Video Generative Models",
    "authors": [
      "Team Wan",
      "Ang Wang",
      "Baole Ai",
      "Bin Wen",
      "Chaojie Mao",
      "Chen-Wei Xie",
      "Di Chen",
      "Feiwu Yu",
      "Haiming Zhao",
      "Jianxiao Yang",
      "Jianyuan Zeng",
      "Jiayu Wang",
      "Jingfeng Zhang",
      "Jingren Zhou",
      "Jinkai Wang",
      "Jixuan Chen",
      "Kai Zhu",
      "Kang Zhao",
      "Keyu Yan",
      "Lianghua Huang",
      "Mengyang Feng",
      "Ningyi Zhang",
      "Pandeng Li",
      "Pingyu Wu",
      "Ruihang Chu",
      "Ruili Feng",
      "Shiwei Zhang",
      "Siyang Sun",
      "Tao Fang",
      "Tianxing Wang",
      "Tianyi Gui",
      "Tingyu Weng",
      "Tong Shen",
      "Wei Lin",
      "Wei Wang",
      "Wei Wang",
      "Wenmeng Zhou",
      "Wente Wang",
      "Wenting Shen",
      "Wenyuan Yu",
      "Xianzhong Shi",
      "Xiaoming Huang",
      "Xin Xu",
      "Yan Kou",
      "Yangyu Lv",
      "Yifei Li",
      "Yijing Liu",
      "Yiming Wang",
      "Yingya Zhang",
      "Yitong Huang",
      "Yong Li",
      "You Wu",
      "Yu Liu",
      "Yulin Pan",
      "Yun Zheng",
      "Yuntao Hong",
      "Yupeng Shi",
      "Yutong Feng",
      "Zeyinzi Jiang",
      "Zhen Han",
      "Zhi-Fan Wu",
      "Ziyu Liu"
    ],
    "abstract": "This report presents Wan, a comprehensive and open suite of video foundation models designed to push the boundaries of video generation. Built upon the mainstream diffusion transformer paradigm, Wan achieves significant advancements in generative capabilities through a series of innovations, including our novel VAE, scalable pre-training strategies, large-scale data curation, and automated evaluation metrics. These contributions collectively enhance the model's performance and versatility. Specifically, Wan is characterized by four key features: Leading Performance: The 14B model of Wan, trained on a vast dataset comprising billions of images and videos, demonstrates the scaling laws of video generation with respect to both data and model size. It consistently outperforms the existing open-source models as well as state-of-the-art commercial solutions across multiple internal and external benchmarks, demonstrating a clear and significant performance superiority. Comprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B parameters, for efficiency and effectiveness respectively. It also covers multiple downstream applications, including image-to-video, instruction-guided video editing, and personal video generation, encompassing up to eight tasks. Consumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource efficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range of consumer-grade GPUs. Openness: We open-source the entire series of Wan, including source code and all models, with the goal of fostering the growth of the video generation community. This openness seeks to significantly expand the creative possibilities of video production in the industry and provide academia with high-quality video foundation models. All the code and models are available at https://github.com/Wan-Video/Wan2.1.",
    "arxiv_url": "http://arxiv.org/abs/2503.20314v2",
    "pdf_url": "http://arxiv.org/pdf/2503.20314v2",
    "published_date": "2025-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Wan-Video/Wan2.1",
    "keywords": [
      "video editing",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scaling Down Text Encoders of Text-to-Image Diffusion Models",
    "authors": [
      "Lifu Wang",
      "Daqing Liu",
      "Xinchen Liu",
      "Xiaodong He"
    ],
    "abstract": "Text encoders in diffusion models have rapidly evolved, transitioning from CLIP to T5-XXL. Although this evolution has significantly enhanced the models' ability to understand complex prompts and generate text, it also leads to a substantial increase in the number of parameters. Despite T5 series encoders being trained on the C4 natural language corpus, which includes a significant amount of non-visual data, diffusion models with T5 encoder do not respond to those non-visual prompts, indicating redundancy in representational power. Therefore, it raises an important question: \"Do we really need such a large text encoder?\" In pursuit of an answer, we employ vision-based knowledge distillation to train a series of T5 encoder models. To fully inherit its capabilities, we constructed our dataset based on three criteria: image quality, semantic understanding, and text-rendering. Our results demonstrate the scaling down pattern that the distilled T5-base model can generate images of comparable quality to those produced by T5-XXL, while being 50 times smaller in size. This reduction in model size significantly lowers the GPU requirements for running state-of-the-art models such as FLUX and SD3, making high-quality text-to-image generation more accessible.",
    "arxiv_url": "http://arxiv.org/abs/2503.19897v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19897v1",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mask$^2$DiT: Dual Mask-based Diffusion Transformer for Multi-Scene Long Video Generation",
    "authors": [
      "Tianhao Qi",
      "Jianlong Yuan",
      "Wanquan Feng",
      "Shancheng Fang",
      "Jiawei Liu",
      "SiYu Zhou",
      "Qian He",
      "Hongtao Xie",
      "Yongdong Zhang"
    ],
    "abstract": "Sora has unveiled the immense potential of the Diffusion Transformer (DiT) architecture in single-scene video generation. However, the more challenging task of multi-scene video generation, which offers broader applications, remains relatively underexplored. To bridge this gap, we propose Mask$^2$DiT, a novel approach that establishes fine-grained, one-to-one alignment between video segments and their corresponding text annotations. Specifically, we introduce a symmetric binary mask at each attention layer within the DiT architecture, ensuring that each text annotation applies exclusively to its respective video segment while preserving temporal coherence across visual tokens. This attention mechanism enables precise segment-level textual-to-visual alignment, allowing the DiT architecture to effectively handle video generation tasks with a fixed number of scenes. To further equip the DiT architecture with the ability to generate additional scenes based on existing ones, we incorporate a segment-level conditional mask, which conditions each newly generated segment on the preceding video segments, thereby enabling auto-regressive scene extension. Both qualitative and quantitative experiments confirm that Mask$^2$DiT excels in maintaining visual consistency across segments while ensuring semantic alignment between each segment and its corresponding text description. Our project page is https://tianhao-qi.github.io/Mask2DiTProject.",
    "arxiv_url": "http://arxiv.org/abs/2503.19881v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19881v1",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion Transformers",
    "authors": [
      "Jiazhi Guan",
      "Kaisiyuan Wang",
      "Zhiliang Xu",
      "Quanwei Yang",
      "Yasheng Sun",
      "Shengyi He",
      "Borong Liang",
      "Yukang Cao",
      "Yingying Li",
      "Haocheng Feng",
      "Errui Ding",
      "Jingdong Wang",
      "Youjian Zhao",
      "Hang Zhou",
      "Ziwei Liu"
    ],
    "abstract": "Despite the recent progress of audio-driven video generation, existing methods mostly focus on driving facial movements, leading to non-coherent head and body dynamics. Moving forward, it is desirable yet challenging to generate holistic human videos with both accurate lip-sync and delicate co-speech gestures w.r.t. given audio. In this work, we propose AudCast, a generalized audio-driven human video generation framework adopting a cascade Diffusion-Transformers (DiTs) paradigm, which synthesizes holistic human videos based on a reference image and a given audio. 1) Firstly, an audio-conditioned Holistic Human DiT architecture is proposed to directly drive the movements of any human body with vivid gesture dynamics. 2) Then to enhance hand and face details that are well-knownly difficult to handle, a Regional Refinement DiT leverages regional 3D fitting as the bridge to reform the signals, producing the final results. Extensive experiments demonstrate that our framework generates high-fidelity audio-driven holistic human videos with temporal coherence and fine facial and hand details. Resources can be found at https://guanjz20.github.io/projects/AudCast.",
    "arxiv_url": "http://arxiv.org/abs/2503.19824v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19824v1",
    "published_date": "2025-03-25",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FuXi-RTM: A Physics-Guided Prediction Framework with Radiative Transfer Modeling",
    "authors": [
      "Qiusheng Huang",
      "Xiaohui Zhong",
      "Xu Fan",
      "Lei Chen",
      "Hao Li"
    ],
    "abstract": "Similar to conventional video generation, current deep learning-based weather prediction frameworks often lack explicit physical constraints, leading to unphysical outputs that limit their reliability for operational forecasting. Among various physical processes requiring proper representation, radiation plays a fundamental role as it drives Earth's weather and climate systems. However, accurate simulation of radiative transfer processes remains challenging for traditional numerical weather prediction (NWP) models due to their inherent complexity and high computational costs. Here, we propose FuXi-RTM, a hybrid physics-guided deep learning framework designed to enhance weather forecast accuracy while enforcing physical consistency. FuXi-RTM integrates a primary forecasting model (FuXi) with a fixed deep learning-based radiative transfer model (DLRTM) surrogate that efficiently replaces conventional radiation parameterization schemes. This represents the first deep learning-based weather forecasting framework to explicitly incorporate physical process modeling. Evaluated over a comprehensive 5-year dataset, FuXi-RTM outperforms its unconstrained counterpart in 88.51% of 3320 variable and lead time combinations, with improvements in radiative flux predictions. By incorporating additional physical processes, FuXi-RTM paves the way for next-generation weather forecasting systems that are both accurate and physically consistent.",
    "arxiv_url": "http://arxiv.org/abs/2503.19940v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19940v1",
    "published_date": "2025-03-25",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
    "authors": [
      "Yuchao Gu",
      "Weijia Mao",
      "Mike Zheng Shou"
    ],
    "abstract": "Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), a strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context video modeling faces challenges due to visual redundancy. Training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle this issue, we propose balancing locality and long-range dependency through long short-term context modeling. A high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with a manageable token context length, thereby significantly reducing training time and memory usage. Furthermore, we propose a multi-level KV cache designed to support the long short-term context modeling, which accelerating inference on long video sequences. We demonstrate that FAR achieves state-of-the-art performance in both short- and long-video generation, providing a simple yet effective baseline for video autoregressive modeling. The code is released at https://github.com/showlab/FAR.",
    "arxiv_url": "http://arxiv.org/abs/2503.19325v2",
    "pdf_url": "http://arxiv.org/pdf/2503.19325v2",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/showlab/FAR",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Boosting Resolution Generalization of Diffusion Transformers with Randomized Positional Encodings",
    "authors": [
      "Cong Liu",
      "Liang Hou",
      "Mingwu Zheng",
      "Xin Tao",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai"
    ],
    "abstract": "Resolution generalization in image generation tasks enables the production of higher-resolution images with lower training resolution overhead. However, a significant challenge in resolution generalization, particularly in the widely used Diffusion Transformers, lies in the mismatch between the positional encodings encountered during testing and those used during training. While existing methods have employed techniques such as interpolation, extrapolation, or their combinations, none have fully resolved this issue. In this paper, we propose a novel two-dimensional randomized positional encodings (RPE-2D) framework that focuses on learning positional order of image patches instead of the specific distances between them, enabling seamless high- and low-resolution image generation without requiring high- and low-resolution image training. Specifically, RPE-2D independently selects positions over a broader range along both the horizontal and vertical axes, ensuring that all position encodings are trained during the inference phase, thus improving resolution generalization. Additionally, we propose a random data augmentation technique to enhance the modeling of position order. To address the issue of image cropping caused by the augmentation, we introduce corresponding micro-conditioning to enable the model to perceive the specific cropping patterns. On the ImageNet dataset, our proposed RPE-2D achieves state-of-the-art resolution generalization performance, outperforming existing competitive methods when trained at a resolution of $256 \\times 256$ and inferred at $384 \\times 384$ and $512 \\times 512$, as well as when scaling from $512 \\times 512$ to $768 \\times 768$ and $1024 \\times 1024$. And it also exhibits outstanding capabilities in low-resolution image generation, multi-stage training acceleration and multi-resolution inheritance.",
    "arxiv_url": "http://arxiv.org/abs/2503.18719v1",
    "pdf_url": "http://arxiv.org/pdf/2503.18719v1",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models",
    "authors": [
      "Jinjin Zhang",
      "Qiuyu Huang",
      "Junjie Liu",
      "Xiefan Guo",
      "Di Huang"
    ],
    "abstract": "In this paper, we present Diffusion-4K, a novel framework for direct ultra-high-resolution image synthesis using text-to-image diffusion models. The core advancements include: (1) Aesthetic-4K Benchmark: addressing the absence of a publicly available 4K image synthesis dataset, we construct Aesthetic-4K, a comprehensive benchmark for ultra-high-resolution image generation. We curated a high-quality 4K dataset with carefully selected images and captions generated by GPT-4o. Additionally, we introduce GLCM Score and Compression Ratio metrics to evaluate fine details, combined with holistic measures such as FID, Aesthetics and CLIPScore for a comprehensive assessment of ultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a wavelet-based fine-tuning approach for direct training with photorealistic 4K images, applicable to various latent diffusion models, demonstrating its effectiveness in synthesizing highly detailed 4K images. Consequently, Diffusion-4K achieves impressive performance in high-quality image synthesis and text prompt adherence, especially when powered by modern large-scale diffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results from our benchmark demonstrate the superiority of Diffusion-4K in ultra-high-resolution image synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2503.18352v2",
    "pdf_url": "http://arxiv.org/pdf/2503.18352v2",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating, Fast and Slow: Scalable Parallel Video Generation with Video Interface Networks",
    "authors": [
      "Bhishma Dedhia",
      "David Bourgin",
      "Krishna Kumar Singh",
      "Yuheng Li",
      "Yan Kang",
      "Zhan Xu",
      "Niraj K. Jha",
      "Yuchen Liu"
    ],
    "abstract": "Diffusion Transformers (DiTs) can generate short photorealistic videos, yet directly training and sampling longer videos with full attention across the video remains computationally challenging. Alternative methods break long videos down into sequential generation of short video segments, requiring multiple sampling chain iterations and specialized consistency modules. To overcome these challenges, we introduce a new paradigm called Video Interface Networks (VINs), which augment DiTs with an abstraction module to enable parallel inference of video chunks. At each diffusion step, VINs encode global semantics from the noisy input of local chunks and the encoded representations, in turn, guide DiTs in denoising chunks in parallel. The coupling of VIN and DiT is learned end-to-end on the denoising objective. Further, the VIN architecture maintains fixed-size encoding tokens that encode the input via a single cross-attention step. Disentangling the encoding tokens from the input thus enables VIN to scale to long videos and learn essential semantics. Experiments on VBench demonstrate that VINs surpass existing chunk-based methods in preserving background consistency and subject coherence. We then show via an optical flow analysis that our approach attains state-of-the-art motion smoothness while using 25-40% fewer FLOPs than full generation. Finally, human raters favorably assessed the overall video quality and temporal consistency of our method in a user study.",
    "arxiv_url": "http://arxiv.org/abs/2503.17539v1",
    "pdf_url": "http://arxiv.org/pdf/2503.17539v1",
    "published_date": "2025-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EDiT: Efficient Diffusion Transformers with Linear Compressed Attention",
    "authors": [
      "Philipp Becker",
      "Abhinav Mehrotra",
      "Ruchika Chavhan",
      "Malcolm Chadwick",
      "Luca Morreale",
      "Mehdi Noroozi",
      "Alberto Gil Ramos",
      "Sourav Bhattacharya"
    ],
    "abstract": "Diffusion Transformers (DiTs) have emerged as a leading architecture for text-to-image synthesis, producing high-quality and photorealistic images. However, the quadratic scaling properties of the attention in DiTs hinder image generation with higher resolution or on devices with limited resources. This work introduces an efficient diffusion transformer (EDiT) to alleviate these efficiency bottlenecks in conventional DiTs and Multimodal DiTs (MM-DiTs). First, we present a novel linear compressed attention method that uses a multi-layer convolutional network to modulate queries with local information while keys and values are spatially aggregated. Second, we formulate a hybrid attention scheme for multi-modal inputs that combines linear attention for image-to-image interactions and standard scaled dot-product attention for interactions involving prompts. Merging these two approaches leads to an expressive, linear-time Multimodal Efficient Diffusion Transformer (MM-EDiT). We demonstrate the effectiveness of the EDiT and MM-EDiT architectures by integrating them into PixArt-Sigma(conventional DiT) and Stable Diffusion 3.5-Medium (MM-DiT), achieving up to 2.2x speedup with comparable image quality after distillation.",
    "arxiv_url": "http://arxiv.org/abs/2503.16726v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16726v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
    "authors": [
      "Liming Jiang",
      "Qing Yan",
      "Yumin Jia",
      "Zichuan Liu",
      "Hao Kang",
      "Xin Lu"
    ],
    "abstract": "Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality and aesthetics. Central to InfU is InfuseNet, a component that injects identity features into the DiT base model via residual connections, enhancing identity similarity while maintaining generation capabilities. A multi-stage training strategy, including pretraining and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample (SPMS) data, further improves text-image alignment, ameliorates image quality, and alleviates face copy-pasting. Extensive experiments demonstrate that InfU achieves state-of-the-art performance, surpassing existing baselines. In addition, the plug-and-play design of InfU ensures compatibility with various existing methods, offering a valuable contribution to the broader community.",
    "arxiv_url": "http://arxiv.org/abs/2503.16418v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16418v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Ultra-Resolution Adaptation with Ease",
    "authors": [
      "Ruonan Yu",
      "Songhua Liu",
      "Zhenxiong Tan",
      "Xinchao Wang"
    ],
    "abstract": "Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key perspectives: data and parameter efficiency, and propose a set of key guidelines for ultra-resolution adaptation termed \\emph{URAE}. For data efficiency, we theoretically and empirically demonstrate that synthetic data generated by some teacher models can significantly promote training convergence. For parameter efficiency, we find that tuning minor components of the weight matrices outperforms widely-used low-rank adapters when synthetic data are unavailable, offering substantial performance gains while maintaining efficiency. Additionally, for models leveraging guidance distillation, such as FLUX, we show that disabling classifier-free guidance, \\textit{i.e.}, setting the guidance scale to 1 during adaptation, is crucial for satisfactory performance. Extensive experiments validate that URAE achieves comparable 2K-generation performance to state-of-the-art closed-source models like FLUX1.1 [Pro] Ultra with only 3K samples and 2K iterations, while setting new benchmarks for 4K-resolution generation. Codes are available \\href{https://github.com/Huage001/URAE}{here}.",
    "arxiv_url": "http://arxiv.org/abs/2503.16322v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16322v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Huage001/URAE",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FreeFlux: Understanding and Exploiting Layer-Specific Roles in RoPE-Based MMDiT for Versatile Image Editing",
    "authors": [
      "Tianyi Wei",
      "Yifan Zhou",
      "Dongdong Chen",
      "Xingang Pan"
    ],
    "abstract": "The integration of Rotary Position Embedding (RoPE) in Multimodal Diffusion Transformer (MMDiT) has significantly enhanced text-to-image generation quality. However, the fundamental reliance of self-attention layers on positional embedding versus query-key similarity during generation remains an intriguing question. We present the first mechanistic analysis of RoPE-based MMDiT models (e.g., FLUX), introducing an automated probing strategy that disentangles positional information versus content dependencies by strategically manipulating RoPE during generation. Our analysis reveals distinct dependency patterns that do not straightforwardly correlate with depth, offering new insights into the layer-specific roles in RoPE-based MMDiT. Based on these findings, we propose a training-free, task-specific image editing framework that categorizes editing tasks into three types: position-dependent editing (e.g., object addition), content similarity-dependent editing (e.g., non-rigid editing), and region-preserved editing (e.g., background replacement). For each type, we design tailored key-value injection strategies based on the characteristics of the editing task. Extensive qualitative and quantitative evaluations demonstrate that our method outperforms state-of-the-art approaches, particularly in preserving original semantic content and achieving seamless modifications.",
    "arxiv_url": "http://arxiv.org/abs/2503.16153v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16153v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image editing",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation",
    "authors": [
      "Soham Roy",
      "Abhishek Mishra",
      "Shirish Karande",
      "Murari Mandal"
    ],
    "abstract": "Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog",
    "arxiv_url": "http://arxiv.org/abs/2503.16171v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16171v1",
    "published_date": "2025-03-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers",
    "authors": [
      "Minglei Shi",
      "Ziyang Yuan",
      "Haotian Yang",
      "Xintao Wang",
      "Mingwu Zheng",
      "Xin Tao",
      "Wenliang Zhao",
      "Wenzhao Zheng",
      "Jie Zhou",
      "Jiwen Lu",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai"
    ],
    "abstract": "Diffusion models have demonstrated remarkable success in various image generation tasks, but their performance is often limited by the uniform processing of inputs across varying conditions and noise levels. To address this limitation, we propose a novel approach that leverages the inherent heterogeneity of the diffusion process. Our method, DiffMoE, introduces a batch-level global token pool that enables experts to access global token distributions during training, promoting specialized expert behavior. To unleash the full potential of the diffusion process, DiffMoE incorporates a capacity predictor that dynamically allocates computational resources based on noise levels and sample complexity. Through comprehensive evaluation, DiffMoE achieves state-of-the-art performance among diffusion models on ImageNet benchmark, substantially outperforming both dense architectures with 3x activated parameters and existing MoE approaches while maintaining 1x activated parameters. The effectiveness of our approach extends beyond class-conditional generation to more challenging tasks such as text-to-image generation, demonstrating its broad applicability across different diffusion model applications. Project Page: https://shiml20.github.io/DiffMoE/",
    "arxiv_url": "http://arxiv.org/abs/2503.14487v1",
    "pdf_url": "http://arxiv.org/pdf/2503.14487v1",
    "published_date": "2025-03-18",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Personalize Anything for Free with Diffusion Transformer",
    "authors": [
      "Haoran Feng",
      "Zehuan Huang",
      "Lin Li",
      "Hairong Lv",
      "Lu Sheng"
    ],
    "abstract": "Personalized image generation aims to produce images of user-specified concepts while enabling flexible editing. Recent training-free approaches, while exhibit higher computational efficiency than training-based methods, struggle with identity preservation, applicability, and compatibility with diffusion transformers (DiTs). In this paper, we uncover the untapped potential of DiT, where simply replacing denoising tokens with those of a reference subject achieves zero-shot subject reconstruction. This simple yet effective feature injection technique unlocks diverse scenarios, from personalization to image editing. Building upon this observation, we propose \\textbf{Personalize Anything}, a training-free framework that achieves personalized image generation in DiT through: 1) timestep-adaptive token replacement that enforces subject consistency via early-stage injection and enhances flexibility through late-stage regularization, and 2) patch perturbation strategies to boost structural diversity. Our method seamlessly supports layout-guided generation, multi-subject personalization, and mask-controlled editing. Evaluations demonstrate state-of-the-art performance in identity preservation and versatility. Our work establishes new insights into DiTs while delivering a practical paradigm for efficient personalization.",
    "arxiv_url": "http://arxiv.org/abs/2503.12590v1",
    "pdf_url": "http://arxiv.org/pdf/2503.12590v1",
    "published_date": "2025-03-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image editing",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection",
    "authors": [
      "Shufan Li",
      "Konstantinos Kallidromitis",
      "Akash Gokul",
      "Arsh Koneru",
      "Yusuke Kato",
      "Kazuki Kozuka",
      "Aditya Grover"
    ],
    "abstract": "The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and a selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping text-to-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, a method that enables Diffusion Transformers to refine their generations using in-context examples of previously generated images alongside textual feedback describing necessary improvements. Instead of passively relying on random sampling and hoping for a better result in a future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it achieves a new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach.",
    "arxiv_url": "http://arxiv.org/abs/2503.12271v1",
    "pdf_url": "http://arxiv.org/pdf/2503.12271v1",
    "published_date": "2025-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture Design in Text to Image Generation",
    "authors": [
      "Chen Chen",
      "Rui Qian",
      "Wenze Hu",
      "Tsu-Jui Fu",
      "Jialing Tong",
      "Xinze Wang",
      "Lezhi Li",
      "Bowen Zhang",
      "Alex Schwing",
      "Wei Liu",
      "Yinfei Yang"
    ],
    "abstract": "In this work, we empirically study Diffusion Transformers (DiTs) for text-to-image generation, focusing on architectural choices, text-conditioning strategies, and training protocols. We evaluate a range of DiT-based architectures--including PixArt-style and MMDiT variants--and compare them with a standard DiT variant which directly processes concatenated text and noise inputs. Surprisingly, our findings reveal that the performance of standard DiT is comparable with those specialized models, while demonstrating superior parameter-efficiency, especially when scaled up. Leveraging the layer-wise parameter sharing strategy, we achieve a further reduction of 66% in model size compared to an MMDiT architecture, with minimal performance impact. Building on an in-depth analysis of critical components such as text encoders and Variational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With supervised and reward fine-tuning, DiT-Air achieves state-of-the-art performance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly competitive, surpassing most existing models despite its compact size.",
    "arxiv_url": "http://arxiv.org/abs/2503.10618v2",
    "pdf_url": "http://arxiv.org/pdf/2503.10618v2",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Long Context Tuning for Video Generation",
    "authors": [
      "Yuwei Guo",
      "Ceyuan Yang",
      "Ziyan Yang",
      "Zhibei Ma",
      "Zhijie Lin",
      "Zhenheng Yang",
      "Dahua Lin",
      "Lu Jiang"
    ],
    "abstract": "Recent advances in video generation can produce realistic, minute-long single-shot videos with scalable diffusion transformers. However, real-world narrative videos require multi-shot scenes with visual and dynamic consistency across shots. In this work, we introduce Long Context Tuning (LCT), a training paradigm that expands the context window of pre-trained single-shot video diffusion models to learn scene-level consistency directly from data. Our method expands full attention mechanisms from individual shots to encompass all shots within a scene, incorporating interleaved 3D position embedding and an asynchronous noise strategy, enabling both joint and auto-regressive shot generation without additional parameters. Models with bidirectional attention after LCT can further be fine-tuned with context-causal attention, facilitating auto-regressive generation with efficient KV-cache. Experiments demonstrate single-shot models after LCT can produce coherent multi-shot scenes and exhibit emerging capabilities, including compositional generation and interactive shot extension, paving the way for more practical visual content creation. See https://guoyww.github.io/projects/long-context-video/ for more details.",
    "arxiv_url": "http://arxiv.org/abs/2503.10589v1",
    "pdf_url": "http://arxiv.org/pdf/2503.10589v1",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation Benchmark",
    "authors": [
      "Viktor Moskvoretskii",
      "Alina Lobanova",
      "Ekaterina Neminova",
      "Chris Biemann",
      "Alexander Panchenko",
      "Irina Nikishina"
    ],
    "abstract": "This paper explores the feasibility of using text-to-image models in a zero-shot setup to generate images for taxonomy concepts. While text-based methods for taxonomy enrichment are well-established, the potential of the visual dimension remains unexplored. To address this, we propose a comprehensive benchmark for Taxonomy Image Generation that assesses models' abilities to understand taxonomy concepts and generate relevant, high-quality images. The benchmark includes common-sense and randomly sampled WordNet concepts, alongside the LLM generated predictions. The 12 models are evaluated using 9 novel taxonomy-related text-to-image metrics and human feedback. Moreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for image generation. Experimental results show that the ranking of models differs significantly from standard T2I tasks. Playground-v2 and FLUX consistently outperform across metrics and subsets and the retrieval-based approach performs poorly. These findings highlight the potential for automating the curation of structured data resources.",
    "arxiv_url": "http://arxiv.org/abs/2503.10357v1",
    "pdf_url": "http://arxiv.org/pdf/2503.10357v1",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer",
    "authors": [
      "Haoxuan Wang",
      "Jinlong Peng",
      "Qingdong He",
      "Hao Yang",
      "Ying Jin",
      "Jiafu Wu",
      "Xiaobin Hu",
      "Yanjie Pan",
      "Zhenye Gan",
      "Mingmin Chi",
      "Bo Peng",
      "Yabiao Wang"
    ],
    "abstract": "With the rapid development of diffusion models in image generation, the demand for more powerful and flexible controllable frameworks is increasing. Although existing methods can guide generation beyond text prompts, the challenge of effectively combining multiple conditional inputs while maintaining consistency with all of them remains unsolved. To address this, we introduce UniCombine, a DiT-based multi-conditional controllable generative framework capable of handling any combination of conditions, including but not limited to text prompts, spatial maps, and subject images. Specifically, we introduce a novel Conditional MMDiT Attention mechanism and incorporate a trainable LoRA module to build both the training-free and training-based versions. Additionally, we propose a new pipeline to construct SubjectSpatial200K, the first dataset designed for multi-conditional generative tasks covering both the subject-driven and spatially-aligned conditions. Extensive experimental results on multi-conditional generation demonstrate the outstanding universality and powerful capability of our approach with state-of-the-art performance.",
    "arxiv_url": "http://arxiv.org/abs/2503.09277v1",
    "pdf_url": "http://arxiv.org/pdf/2503.09277v1",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image generation",
      "diffusion transformer",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NAMI: Efficient Image Generation via Progressive Rectified Flow Transformers",
    "authors": [
      "Yuhang Ma",
      "Bo Cheng",
      "Shanyuan Liu",
      "Ao Ma",
      "Xiaoyu Wu",
      "Liebucha Wu",
      "Dawei Leng",
      "Yuhui Yin"
    ],
    "abstract": "Flow-based transformer models for image generation have achieved state-of-the-art performance with larger model parameters, but their inference deployment cost remains high. To enhance inference performance while maintaining generation quality, we propose progressive rectified flow transformers. We divide the rectified flow into different stages according to resolution, using fewer transformer layers at the low-resolution stages to generate image layouts and concept contours, and progressively adding more layers as the resolution increases. Experiments demonstrate that our approach achieves fast convergence and reduces inference time while ensuring generation quality. The main contributions of this paper are summarized as follows: (1) We introduce progressive rectified flow transformers that enable multi-resolution training, accelerating model convergence; (2) NAMI leverages piecewise flow and spatial cascading of Diffusion Transformer (DiT) to rapidly generate images, reducing inference time by 40% to generate a 1024 resolution image; (3) We propose NAMI-1K benchmark to evaluate human preference performance, aiming to mitigate distributional bias and prevent data leakage from open-source benchmarks. The results show that our model is competitive with state-of-the-art models.",
    "arxiv_url": "http://arxiv.org/abs/2503.09242v1",
    "pdf_url": "http://arxiv.org/pdf/2503.09242v1",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer",
      "rectified flow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space",
    "authors": [
      "Jian Zhu",
      "Zhengyu Jia",
      "Tian Gao",
      "Jiaxin Deng",
      "Shidi Li",
      "Fu Liu",
      "Peng Jia",
      "Xianpeng Lang",
      "Xiaolong Sun"
    ],
    "abstract": "Advanced end-to-end autonomous driving systems predict other vehicles' motions and plan ego vehicle's trajectory. The world model that can foresee the outcome of the trajectory has been used to evaluate the end-to-end autonomous driving system. However, existing world models predominantly emphasize the trajectory of the ego vehicle and leave other vehicles uncontrollable. This limitation hinders their ability to realistically simulate the interaction between the ego vehicle and the driving scenario. In addition, it remains a challenge to match multiple trajectories with each vehicle in the video to control the video generation. To address above issues, a driving World Model named EOT-WM is proposed in this paper, unifying Ego-Other vehicle Trajectories in videos. Specifically, we first project ego and other vehicle trajectories in the BEV space into the image coordinate to match each trajectory with its corresponding vehicle in the video. Then, trajectory videos are encoded by the Spatial-Temporal Variational Auto Encoder to align with driving video latents spatially and temporally in the unified visual space. A trajectory-injected diffusion Transformer is further designed to denoise the noisy video latents for video generation with the guidance of ego-other vehicle trajectories. In addition, we propose a metric based on control latent similarity to evaluate the controllability of trajectories. Extensive experiments are conducted on the nuScenes dataset, and the proposed model outperforms the state-of-the-art method by 30% in FID and 55% in FVD. The model can also predict unseen driving scenes with self-produced trajectories.",
    "arxiv_url": "http://arxiv.org/abs/2503.09215v2",
    "pdf_url": "http://arxiv.org/pdf/2503.09215v2",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
    "authors": [
      "Hyeonho Jeong",
      "Suhyeon Lee",
      "Jong Chul Ye"
    ],
    "abstract": "We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: https://hyeonho99.github.io/reangle-a-video/",
    "arxiv_url": "http://arxiv.org/abs/2503.09151v2",
    "pdf_url": "http://arxiv.org/pdf/2503.09151v2",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Zero-Shot Subject-Centric Generation for Creative Application Using Entropy Fusion",
    "authors": [
      "Kaifeng Zou",
      "Xiaoyi Feng",
      "Peng Wang",
      "Tao Huang",
      "Zizhou Huang",
      "Zhang Haihang",
      "Yuntao Zou",
      "Dagang Li"
    ],
    "abstract": "Generative models are widely used in visual content creation. However, current text-to-image models often face challenges in practical applications-such as textile pattern design and meme generation-due to the presence of unwanted elements that are difficult to separate with existing methods. Meanwhile, subject-reference generation has emerged as a key research trend, highlighting the need for techniques that can produce clean, high-quality subject images while effectively removing extraneous components. To address this challenge, we introduce a framework for reliable subject-centric image generation. In this work, we propose an entropy-based feature-weighted fusion method to merge the informative cross-attention features obtained from each sampling step of the pretrained text-to-image model FLUX, enabling a precise mask prediction and subject-centric generation. Additionally, we have developed an agent framework based on Large Language Models (LLMs) that translates users' casual inputs into more descriptive prompts, leading to highly detailed image generation. Simultaneously, the agents extract primary elements of prompts to guide the entropy-based feature fusion, ensuring focused primary element generation without extraneous components. Experimental results and user studies demonstrate our methods generates high-quality subject-centric images, outperform existing methods or other possible pipelines, highlighting the effectiveness of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2503.10697v1",
    "pdf_url": "http://arxiv.org/pdf/2503.10697v1",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation",
    "authors": [
      "Junsong Chen",
      "Shuchen Xue",
      "Yuyang Zhao",
      "Jincheng Yu",
      "Sayak Paul",
      "Junyu Chen",
      "Han Cai",
      "Enze Xie",
      "Song Han"
    ],
    "abstract": "This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast text-to-image (T2I) generation. SANA-Sprint is built on a pre-trained foundation model and augmented with hybrid distillation, dramatically reducing inference steps from 20 to 1-4. We introduce three key innovations: (1) We propose a training-free approach that transforms a pre-trained flow-matching model for continuous-time consistency distillation (sCM), eliminating costly training from scratch and achieving high training efficiency. Our hybrid distillation strategy combines sCM with latent adversarial distillation (LADD): sCM ensures alignment with the teacher model, while LADD enhances single-step generation fidelity. (2) SANA-Sprint is a unified step-adaptive model that achieves high-quality generation in 1-4 steps, eliminating step-specific training and improving efficiency. (3) We integrate ControlNet with SANA-Sprint for real-time interactive image generation, enabling instant visual feedback for user interaction. SANA-Sprint establishes a new Pareto frontier in speed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID and 0.74 GenEval in only 1 step - outperforming FLUX-schnell (7.94 FID / 0.71 GenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s (T2I) and 0.25s (ControlNet) latency for 1024 x 1024 images on H100, and 0.31s (T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for AI-powered consumer applications (AIPC). Code and pre-trained models will be open-sourced.",
    "arxiv_url": "http://arxiv.org/abs/2503.09641v2",
    "pdf_url": "http://arxiv.org/pdf/2503.09641v2",
    "published_date": "2025-03-12",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation",
      "Control"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder",
    "authors": [
      "Yitian Zhang",
      "Long Mai",
      "Aniruddha Mahapatra",
      "David Bourgin",
      "Yicong Hong",
      "Jonah Casebeer",
      "Feng Liu",
      "Yun Fu"
    ],
    "abstract": "We present a novel perspective on learning video embedders for generative modeling: rather than requiring an exact reproduction of an input video, an effective embedder should focus on synthesizing visually plausible reconstructions. This relaxed criterion enables substantial improvements in compression ratios without compromising the quality of downstream generative models. Specifically, we propose replacing the conventional encoder-decoder video embedder with an encoder-generator framework that employs a diffusion transformer (DiT) to synthesize missing details from a compact latent space. Therein, we develop a dedicated latent conditioning module to condition the DiT decoder on the encoded video latent embedding. Our experiments demonstrate that our approach enables superior encoding-decoding performance compared to state-of-the-art methods, particularly as the compression ratio increases. To demonstrate the efficacy of our approach, we report results from our video embedders achieving a temporal compression ratio of up to 32x (8x higher than leading video embedders) and validate the robustness of this ultra-compact latent space for text-to-video generation, providing a significant efficiency boost in latent diffusion model training and inference.",
    "arxiv_url": "http://arxiv.org/abs/2503.08665v1",
    "pdf_url": "http://arxiv.org/pdf/2503.08665v1",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OminiControl2: Efficient Conditioning for Diffusion Transformers",
    "authors": [
      "Zhenxiong Tan",
      "Qiaochu Xue",
      "Xingyi Yang",
      "Songhua Liu",
      "Xinchao Wang"
    ],
    "abstract": "Fine-grained control of text-to-image diffusion transformer models (DiT) remains a critical challenge for practical deployment. While recent advances such as OminiControl and others have enabled a controllable generation of diverse control signals, these methods face significant computational inefficiency when handling long conditional inputs. We present OminiControl2, an efficient framework that achieves efficient image-conditional image generation. OminiControl2 introduces two key innovations: (1) a dynamic compression strategy that streamlines conditional inputs by preserving only the most semantically relevant tokens during generation, and (2) a conditional feature reuse mechanism that computes condition token features only once and reuses them across denoising steps. These architectural improvements preserve the original framework's parameter efficiency and multi-modal versatility while dramatically reducing computational costs. Our experiments demonstrate that OminiControl2 reduces conditional processing overhead by over 90% compared to its predecessor, achieving an overall 5.9$\\times$ speedup in multi-conditional generation scenarios. This efficiency enables the practical implementation of complex, multi-modal control for high-quality image synthesis with DiT models.",
    "arxiv_url": "http://arxiv.org/abs/2503.08280v1",
    "pdf_url": "http://arxiv.org/pdf/2503.08280v1",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "Control",
      "image generation",
      "diffusion transformer",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seedream 2.0: A Native Chinese-English Bilingual Image Generation Foundation Model",
    "authors": [
      "Lixue Gong",
      "Xiaoxia Hou",
      "Fanshi Li",
      "Liang Li",
      "Xiaochen Lian",
      "Fei Liu",
      "Liyang Liu",
      "Wei Liu",
      "Wei Lu",
      "Yichun Shi",
      "Shiqi Sun",
      "Yu Tian",
      "Zhi Tian",
      "Peng Wang",
      "Xun Wang",
      "Ye Wang",
      "Guofeng Wu",
      "Jie Wu",
      "Xin Xia",
      "Xuefeng Xiao",
      "Linjie Yang",
      "Zhonghua Zhai",
      "Xinyu Zhang",
      "Qi Zhang",
      "Yuwei Zhang",
      "Shijia Zhao",
      "Jianchao Yang",
      "Weilin Huang"
    ],
    "abstract": "Rapid advancement of diffusion models has catalyzed remarkable progress in the field of image generation. However, prevalent models such as Flux, SD3.5 and Midjourney, still grapple with issues like model bias, limited text rendering capabilities, and insufficient understanding of Chinese cultural nuances. To address these limitations, we present Seedream 2.0, a native Chinese-English bilingual image generation foundation model that excels across diverse dimensions, which adeptly manages text prompt in both Chinese and English, supporting bilingual image generation and text rendering. We develop a powerful data system that facilitates knowledge integration, and a caption system that balances the accuracy and richness for image description. Particularly, Seedream is integrated with a self-developed bilingual large language model as a text encoder, allowing it to learn native knowledge directly from massive data. This enable it to generate high-fidelity images with accurate cultural nuances and aesthetic expressions described in either Chinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible character-level text rendering, while a Scaled ROPE generalizes well to untrained resolutions. Multi-phase post-training optimizations, including SFT and RLHF iterations, further improve the overall capability. Through extensive experimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art performance across multiple aspects, including prompt-following, aesthetics, text rendering, and structural correctness. Furthermore, Seedream 2.0 has been optimized through multiple RLHF iterations to closely align its output with human preferences, as revealed by its outstanding ELO score. In addition, it can be readily adapted to an instruction-based image editing model, such as SeedEdit, with strong editing capability that balances instruction-following and image consistency.",
    "arxiv_url": "http://arxiv.org/abs/2503.07703v1",
    "pdf_url": "http://arxiv.org/pdf/2503.07703v1",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "image editing",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VACE: All-in-One Video Creation and Editing",
    "authors": [
      "Zeyinzi Jiang",
      "Zhen Han",
      "Chaojie Mao",
      "Jingfeng Zhang",
      "Yulin Pan",
      "Yu Liu"
    ],
    "abstract": "Diffusion Transformer has demonstrated powerful capability and scalability in generating high-quality images and videos. Further pursuing the unification of generation and editing tasks has yielded significant progress in the domain of image content creation. However, due to the intrinsic demands for consistency across both temporal and spatial dynamics, achieving a unified approach for video synthesis remains challenging. We introduce VACE, which enables users to perform Video tasks within an All-in-one framework for Creation and Editing. These tasks include reference-to-video generation, video-to-video editing, and masked video-to-video editing. Specifically, we effectively integrate the requirements of various tasks by organizing video task inputs, such as editing, reference, and masking, into a unified interface referred to as the Video Condition Unit (VCU). Furthermore, by utilizing a Context Adapter structure, we inject different task concepts into the model using formalized representations of temporal and spatial dimensions, allowing it to handle arbitrary video synthesis tasks flexibly. Extensive experiments demonstrate that the unified model of VACE achieves performance on par with task-specific models across various subtasks. Simultaneously, it enables diverse applications through versatile task combinations. Project page: https://ali-vilab.github.io/VACE-Page/.",
    "arxiv_url": "http://arxiv.org/abs/2503.07598v2",
    "pdf_url": "http://arxiv.org/pdf/2503.07598v2",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video editing",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TIDE : Temporal-Aware Sparse Autoencoders for Interpretable Diffusion Transformers in Image Generation",
    "authors": [
      "Victor Shea-Jay Huang",
      "Le Zhuo",
      "Yi Xin",
      "Zhaokai Wang",
      "Peng Gao",
      "Hongsheng Li"
    ],
    "abstract": "Diffusion Transformers (DiTs) are a powerful yet underexplored class of generative models compared to U-Net-based diffusion models. To bridge this gap, we introduce TIDE (Temporal-aware Sparse Autoencoders for Interpretable Diffusion transformErs), a novel framework that enhances temporal reconstruction within DiT activation layers across denoising steps. TIDE employs Sparse Autoencoders (SAEs) with a sparse bottleneck layer to extract interpretable and hierarchical features, revealing that diffusion models inherently learn hierarchical features at multiple levels (e.g., 3D, semantic, class) during generative pre-training. Our approach achieves state-of-the-art reconstruction performance, with a mean squared error (MSE) of 1e-3 and a cosine similarity of 0.97, demonstrating superior accuracy in capturing activation dynamics along the denoising trajectory. Beyond interpretability, we showcase TIDE's potential in downstream applications such as sparse activation-guided image editing and style transfer, enabling improved controllability for generative systems. By providing a comprehensive training and evaluation protocol tailored for DiTs, TIDE contributes to developing more interpretable, transparent, and trustworthy generative models.",
    "arxiv_url": "http://arxiv.org/abs/2503.07050v1",
    "pdf_url": "http://arxiv.org/pdf/2503.07050v1",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image editing",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Post-Training Quantization for Diffusion Transformer via Hierarchical Timestep Grouping",
    "authors": [
      "Ning Ding",
      "Jing Han",
      "Yuchuan Tian",
      "Chao Xu",
      "Kai Han",
      "Yehui Tang"
    ],
    "abstract": "Diffusion Transformer (DiT) has now become the preferred choice for building image generation models due to its great generation capability. Unlike previous convolution-based UNet models, DiT is purely composed of a stack of transformer blocks, which renders DiT excellent in scalability like large language models. However, the growing model size and multi-step sampling paradigm bring about considerable pressure on deployment and inference. In this work, we propose a post-training quantization framework tailored for Diffusion Transforms to tackle these challenges. We firstly locate that the quantization difficulty of DiT mainly originates from the time-dependent channel-specific outliers. We propose a timestep-aware shift-and-scale strategy to smooth the activation distribution to reduce the quantization error. Secondly, based on the observation that activations of adjacent timesteps have similar distributions, we utilize a hierarchical clustering scheme to divide the denoising timesteps into multiple groups. We further design a re-parameterization scheme which absorbs the quantization parameters into nearby module to avoid redundant computations. Comprehensive experiments demonstrate that out PTQ method successfully quantize the Diffusion Transformer into 8-bit weight and 8-bit activation (W8A8) with state-of-the-art FiD score. And our method can further quantize DiT model into 4-bit weight and 8-bit activation (W4A8) without sacrificing generation quality.",
    "arxiv_url": "http://arxiv.org/abs/2503.06930v2",
    "pdf_url": "http://arxiv.org/pdf/2503.06930v2",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical Latent and Layer Caching for Video Generation",
    "authors": [
      "Junyi Wu",
      "Zhiteng Li",
      "Zheng Hui",
      "Yulun Zhang",
      "Linghe Kong",
      "Xiaokang Yang"
    ],
    "abstract": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant architecture in video generation, surpassing U-Net-based models in terms of performance. However, the enhanced capabilities of DiTs come with significant drawbacks, including increased computational and memory costs, which hinder their deployment on resource-constrained devices. Current acceleration techniques, such as quantization and cache mechanism, offer limited speedup and are often applied in isolation, failing to fully address the complexities of DiT architectures. In this paper, we propose QuantCache, a novel training-free inference acceleration framework that jointly optimizes hierarchical latent caching, adaptive importance-guided quantization, and structural redundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of 6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive experiments across multiple video generation benchmarks demonstrate the effectiveness of our method, setting a new standard for efficient DiT inference. The code and models will be available at https://github.com/JunyiWuCode/QuantCache.",
    "arxiv_url": "http://arxiv.org/abs/2503.06545v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06545v1",
    "published_date": "2025-03-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/JunyiWuCode/QuantCache",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Get In Video: Add Anything You Want to the Video",
    "authors": [
      "Shaobin Zhuang",
      "Zhipeng Huang",
      "Binxin Yang",
      "Ying Zhang",
      "Fangyikang Wang",
      "Canmiao Fu",
      "Chong Sun",
      "Zheng-Jun Zha",
      "Chen Li",
      "Yali Wang"
    ],
    "abstract": "Video editing increasingly demands the ability to incorporate specific real-world instances into existing footage, yet current approaches fundamentally fail to capture the unique visual characteristics of particular subjects and ensure natural instance/scene interactions. We formalize this overlooked yet critical editing paradigm as \"Get-In-Video Editing\", where users provide reference images to precisely specify visual elements they wish to incorporate into videos. Addressing this task's dual challenges, severe training data scarcity and technical challenges in maintaining spatiotemporal coherence, we introduce three key contributions. First, we develop GetIn-1M dataset created through our automated Recognize-Track-Erase pipeline, which sequentially performs video captioning, salient instance identification, object detection, temporal tracking, and instance removal to generate high-quality video editing pairs with comprehensive annotations (reference image, tracking mask, instance prompt). Second, we present GetInVideo, a novel end-to-end framework that leverages a diffusion transformer architecture with 3D full attention to process reference images, condition videos, and masks simultaneously, maintaining temporal coherence, preserving visual identity, and ensuring natural scene interactions when integrating reference objects into videos. Finally, we establish GetInBench, the first comprehensive benchmark for Get-In-Video Editing scenario, demonstrating our approach's superior performance through extensive evaluations. Our work enables accessible, high-quality incorporation of specific real-world subjects into videos, significantly advancing personalized video editing capabilities.",
    "arxiv_url": "http://arxiv.org/abs/2503.06268v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06268v1",
    "published_date": "2025-03-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video editing",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "X2I: Seamless Integration of Multimodal Understanding into Diffusion Transformer via Attention Distillation",
    "authors": [
      "Jian Ma",
      "Qirong Peng",
      "Xu Guo",
      "Chen Chen",
      "Haonan Lu",
      "Zhenyu Yang"
    ],
    "abstract": "Text-to-image (T2I) models are well known for their ability to produce highly realistic images, while multimodal large language models (MLLMs) are renowned for their proficiency in understanding and integrating multiple modalities. However, currently there is no straightforward and efficient framework to transfer the multimodal comprehension abilities of MLLMs to T2I models to enable them to understand multimodal inputs. In this paper, we propose the X2I framework, which endows Diffusion Transformer (DiT) models with the capability to comprehend various modalities, including multilingual text, screenshot documents, images, videos, and audio. X2I is trained using merely 100K English corpus with 160 GPU hours. Building on the DiT teacher model, we adopt an innovative distillation method to extract the inference capabilities of the teacher model and design a lightweight AlignNet structure to serve as an intermediate bridge. Compared to the teacher model, X2I shows a decrease in performance degradation of less than 1\\% while gaining various multimodal understanding abilities, including multilingual to image, image to image, image-text to image, video to image, audio to image, and utilizing creative fusion to enhance imagery. Furthermore, it is applicable for LoRA training in the context of image-text to image generation, filling a void in the industry in this area. We further design a simple LightControl to enhance the fidelity of instructional image editing. Finally, extensive experiments demonstrate the effectiveness, efficiency, multifunctionality, and transferability of our X2I. The open-source code and checkpoints for X2I can be found at the following link: https://github.com/OPPO-Mente-Lab/X2I.",
    "arxiv_url": "http://arxiv.org/abs/2503.06134v2",
    "pdf_url": "http://arxiv.org/pdf/2503.06134v2",
    "published_date": "2025-03-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/OPPO-Mente-Lab/X2I",
    "keywords": [
      "text-to-image",
      "image editing",
      "Control",
      "image generation",
      "diffusion transformer",
      "image to image"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MagicInfinite: Generating Infinite Talking Videos with Your Words and Voice",
    "authors": [
      "Hongwei Yi",
      "Tian Ye",
      "Shitong Shao",
      "Xuancheng Yang",
      "Jiantong Zhao",
      "Hanzhong Guo",
      "Terrance Wang",
      "Qingyu Yin",
      "Zeke Xie",
      "Lei Zhu",
      "Wei Li",
      "Michael Lingelbach",
      "Daquan Zhou"
    ],
    "abstract": "We present MagicInfinite, a novel diffusion Transformer (DiT) framework that overcomes traditional portrait animation limitations, delivering high-fidelity results across diverse character types-realistic humans, full-body figures, and stylized anime characters. It supports varied facial poses, including back-facing views, and animates single or multiple characters with input masks for precise speaker designation in multi-character scenes. Our approach tackles key challenges with three innovations: (1) 3D full-attention mechanisms with a sliding window denoising strategy, enabling infinite video generation with temporal coherence and visual quality across diverse character styles; (2) a two-stage curriculum learning scheme, integrating audio for lip sync, text for expressive dynamics, and reference images for identity preservation, enabling flexible multi-modal control over long sequences; and (3) region-specific masks with adaptive loss functions to balance global textual control and local audio guidance, supporting speaker-specific animations. Efficiency is enhanced via our innovative unified step and cfg distillation techniques, achieving a 20x inference speed boost over the basemodel: generating a 10 second 540x540p video in 10 seconds or 720x720p in 30 seconds on 8 H100 GPUs, without quality loss. Evaluations on our new benchmark demonstrate MagicInfinite's superiority in audio-lip synchronization, identity preservation, and motion naturalness across diverse scenarios. It is publicly available at https://www.hedra.com/, with examples at https://magicinfinite.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2503.05978v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05978v1",
    "published_date": "2025-03-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Rethinking Video Super-Resolution: Towards Diffusion-Based Methods without Motion Alignment",
    "authors": [
      "Zhihao Zhan",
      "Wang Pang",
      "Xiang Zhu",
      "Yechao Bai"
    ],
    "abstract": "In this work, we rethink the approach to video super-resolution by introducing a method based on the Diffusion Posterior Sampling framework, combined with an unconditional video diffusion transformer operating in latent space. The video generation model, a diffusion transformer, functions as a space-time model. We argue that a powerful model, which learns the physics of the real world, can easily handle various kinds of motion patterns as prior knowledge, thus eliminating the need for explicit estimation of optical flows or motion parameters for pixel alignment. Furthermore, a single instance of the proposed video diffusion transformer model can adapt to different sampling conditions without re-training. Empirical results on synthetic and real-world datasets illustrate the feasibility of diffusion-based, alignment-free video super-resolution.",
    "arxiv_url": "http://arxiv.org/abs/2503.03355v4",
    "pdf_url": "http://arxiv.org/pdf/2503.03355v4",
    "published_date": "2025-03-05",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Q&C: When Quantization Meets Cache in Efficient Image Generation",
    "authors": [
      "Xin Ding",
      "Xin Li",
      "Haotong Qin",
      "Zhibo Chen"
    ],
    "abstract": "Quantization and cache mechanisms are typically applied individually for efficient Diffusion Transformers (DiTs), each demonstrating notable potential for acceleration. However, the promoting effect of combining the two mechanisms on efficient generation remains under-explored. Through empirical investigation, we find that the combination of quantization and cache mechanisms for DiT is not straightforward, and two key challenges lead to severe catastrophic performance degradation: (i) the sample efficacy of calibration datasets in post-training quantization (PTQ) is significantly eliminated by cache operation; (ii) the combination of the above mechanisms introduces more severe exposure bias within sampling distribution, resulting in amplified error accumulation in the image generation process. In this work, we take advantage of these two acceleration mechanisms and propose a hybrid acceleration method by tackling the above challenges, aiming to further improve the efficiency of DiTs while maintaining excellent generation capability. Concretely, a temporal-aware parallel clustering (TAP) is designed to dynamically improve the sample selection efficacy for the calibration within PTQ for different diffusion steps. A variance compensation (VC) strategy is derived to correct the sampling distribution. It mitigates exposure bias through an adaptive correction factor generation. Extensive experiments have shown that our method has accelerated DiTs by 12.7x while preserving competitive generation capability. The code will be available at https://github.com/xinding-sys/Quant-Cache.",
    "arxiv_url": "http://arxiv.org/abs/2503.02508v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02508v1",
    "published_date": "2025-03-04",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "https://github.com/xinding-sys/Quant-Cache",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video Generation",
    "authors": [
      "Yifei Xia",
      "Suhan Ling",
      "Fangcheng Fu",
      "Yujie Wang",
      "Huixia Li",
      "Xuefeng Xiao",
      "Bin Cui"
    ],
    "abstract": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is often hindered by significant latency, primarily due to the computational demands of attention mechanisms. For instance, generating an 8-second 720p video (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500 PFLOPs consumed by attention computations. To address this issue, we propose AdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention method. Firstly, to realize the Dynamic Pattern, we introduce a blockified pattern to efficiently capture the hierarchical sparsity inherent in DiTs. This is based on our observation that sparse characteristics of DiTs exhibit hierarchical and blockified structures between and within different modalities. This blockified approach significantly reduces the complexity of attention computation while maintaining high fidelity in the generated videos. Secondly, to enable Online Precise Search, we propose the Fused LSE-Cached Search with Head-adaptive Hierarchical Block Sparse Attention. This method is motivated by our finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and heads, but remain invariant across denoising steps. By leveraging this invariance across denoising steps, it adapts to the dynamic nature of DiTs and allows for precise, real-time identification of sparse indices with minimal overhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can be integrated seamlessly with existing DiTs, requiring neither additional fine-tuning nor a dataset-dependent profiling. Extensive experiments validate that AdaSpa delivers substantial acceleration across various models while preserving video quality, establishing itself as a robust and scalable approach to efficient video generation.",
    "arxiv_url": "http://arxiv.org/abs/2502.21079v1",
    "pdf_url": "http://arxiv.org/pdf/2502.21079v1",
    "published_date": "2025-02-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think",
    "authors": [
      "Liang Chen",
      "Shuai Bai",
      "Wenhao Chai",
      "Weichu Xie",
      "Haozhe Zhao",
      "Leon Vinci",
      "Junyang Lin",
      "Baobao Chang"
    ],
    "abstract": "The field of advanced text-to-image generation is witnessing the emergence of unified frameworks that integrate powerful text encoders, such as CLIP and T5, with Diffusion Transformer backbones. Although there have been efforts to control output images with additional conditions, like canny and depth map, a comprehensive framework for arbitrary text-image interleaved control is still lacking. This gap is especially evident when attempting to merge concepts or visual elements from multiple images in the generation process. To mitigate the gap, we conducted preliminary experiments showing that large multimodal models (LMMs) offer an effective shared representation space, where image and text can be well-aligned to serve as a condition for external diffusion models. Based on this discovery, we propose Dream Engine, an efficient and unified framework designed for arbitrary text-image interleaved control in image generation models. Building on powerful text-to-image models like SD3.5, we replace the original text-only encoders by incorporating versatile multimodal information encoders such as QwenVL. Our approach utilizes a two-stage training paradigm, consisting of joint text-image alignment and multimodal interleaved instruction tuning. Our experiments demonstrate that this training method is effective, achieving a 0.69 overall score on the GenEval benchmark, and matching the performance of state-of-the-art text-to-image models like SD3.5 and FLUX.",
    "arxiv_url": "http://arxiv.org/abs/2502.20172v1",
    "pdf_url": "http://arxiv.org/pdf/2502.20172v1",
    "published_date": "2025-02-27",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute",
    "authors": [
      "Sotiris Anagnostidis",
      "Gregor Bachmann",
      "Yeongmin Kim",
      "Jonas Kohler",
      "Markos Georgopoulos",
      "Artsiom Sanakoyeu",
      "Yuming Du",
      "Albert Pumarola",
      "Ali Thabet",
      "Edgar Schönfeld"
    ],
    "abstract": "Despite their remarkable performance, modern Diffusion Transformers are hindered by substantial resource requirements during inference, stemming from the fixed and large amount of compute needed for each denoising step. In this work, we revisit the conventional static paradigm that allocates a fixed compute budget per denoising iteration and propose a dynamic strategy instead. Our simple and sample-efficient framework enables pre-trained DiT models to be converted into \\emph{flexible} ones -- dubbed FlexiDiT -- allowing them to process inputs at varying compute budgets. We demonstrate how a single \\emph{flexible} model can generate images without any drop in quality, while reducing the required FLOPs by more than $40$\\% compared to their static counterparts, for both class-conditioned and text-conditioned image generation. Our method is general and agnostic to input and conditioning modalities. We show how our approach can be readily extended for video generation, where FlexiDiT models generate samples with up to $75$\\% less compute without compromising performance.",
    "arxiv_url": "http://arxiv.org/abs/2502.20126v1",
    "pdf_url": "http://arxiv.org/pdf/2502.20126v1",
    "published_date": "2025-02-27",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers",
    "authors": [
      "Min Zhao",
      "Guande He",
      "Yixiao Chen",
      "Hongzhou Zhu",
      "Chongxuan Li",
      "Jun Zhu"
    ],
    "abstract": "Recent advancements in video generation have enabled models to synthesize high-quality, minute-long videos. However, generating even longer videos with temporal coherence remains a major challenge, and existing length extrapolation methods lead to temporal repetition or motion deceleration. In this work, we systematically analyze the role of frequency components in positional embeddings and identify an intrinsic frequency that primarily governs extrapolation behavior. Based on this insight, we propose RIFLEx, a minimal yet effective approach that reduces the intrinsic frequency to suppress repetition while preserving motion consistency, without requiring any additional modifications. RIFLEx offers a true free lunch--achieving high-quality $2\\times$ extrapolation on state-of-the-art video diffusion transformers in a completely training-free manner. Moreover, it enhances quality and enables $3\\times$ extrapolation by minimal fine-tuning without long videos. Project page and codes: \\href{https://riflex-video.github.io/}{https://riflex-video.github.io/.}",
    "arxiv_url": "http://arxiv.org/abs/2502.15894v1",
    "pdf_url": "http://arxiv.org/pdf/2502.15894v1",
    "published_date": "2025-02-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hardware-Friendly Static Quantization Method for Video Diffusion Transformers",
    "authors": [
      "Sanghyun Yi",
      "Qingfeng Liu",
      "Mostafa El-Khamy"
    ],
    "abstract": "Diffusion Transformers for video generation have gained significant research interest since the impressive performance of SORA. Efficient deployment of such generative-AI models on GPUs has been demonstrated with dynamic quantization. However, resource-constrained devices cannot support dynamic quantization, and need static quantization of the models for their efficient deployment on AI processors. In this paper, we propose a novel method for the post-training quantization of OpenSora\\cite{opensora}, a Video Diffusion Transformer, without relying on dynamic quantization techniques. Our approach employs static quantization, achieving video quality comparable to FP16 and dynamically quantized ViDiT-Q methods, as measured by CLIP, and VQA metrics. In particular, we utilize per-step calibration data to adequately provide a post-training statically quantized model for each time step, incorporating channel-wise quantization for weights and tensor-wise quantization for activations. By further applying the smooth-quantization technique, we can obtain high-quality video outputs with the statically quantized models. Extensive experimental results demonstrate that static quantization can be a viable alternative to dynamic quantization for video diffusion transformers, offering a more efficient approach without sacrificing performance.",
    "arxiv_url": "http://arxiv.org/abs/2502.15077v2",
    "pdf_url": "http://arxiv.org/pdf/2502.15077v2",
    "published_date": "2025-02-20",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers",
    "authors": [
      "Ke Cao",
      "Jing Wang",
      "Ao Ma",
      "Jiasong Feng",
      "Zhanjie Zhang",
      "Xuanhua He",
      "Shanyuan Liu",
      "Bo Cheng",
      "Dawei Leng",
      "Yuhui Yin",
      "Jie Zhang"
    ],
    "abstract": "The Diffusion Transformer plays a pivotal role in advancing text-to-image and text-to-video generation, owing primarily to its inherent scalability. However, existing controlled diffusion transformer methods incur significant parameter and computational overheads and suffer from inefficient resource allocation due to their failure to account for the varying relevance of control information across different transformer layers. To address this, we propose the Relevance-Guided Efficient Controllable Generation framework, RelaCtrl, enabling efficient and resource-optimized integration of control signals into the Diffusion Transformer. First, we evaluate the relevance of each layer in the Diffusion Transformer to the control information by assessing the \"ControlNet Relevance Score\"-i.e., the impact of skipping each control layer on both the quality of generation and the control effectiveness during inference. Based on the strength of the relevance, we then tailor the positioning, parameter scale, and modeling capacity of the control layers to reduce unnecessary parameters and redundant computations. Additionally, to further improve efficiency, we replace the self-attention and FFN in the commonly used copy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM), enabling efficient implementation of both the token mixer and channel mixer. Both qualitative and quantitative experimental results demonstrate that our approach achieves superior performance with only 15% of the parameters and computational complexity compared to PixArt-delta.",
    "arxiv_url": "http://arxiv.org/abs/2502.14377v4",
    "pdf_url": "http://arxiv.org/pdf/2502.14377v4",
    "published_date": "2025-02-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "video generation",
      "Control",
      "diffusion transformer",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Designing Parameter and Compute Efficient Diffusion Transformers using Distillation",
    "authors": [
      "Vignesh Sundaresha"
    ],
    "abstract": "Diffusion Transformers (DiTs) with billions of model parameters form the backbone of popular image and video generation models like DALL.E, Stable-Diffusion and SORA. Though these models are necessary in many low-latency applications like Augmented/Virtual Reality, they cannot be deployed on resource-constrained Edge devices (like Apple Vision Pro or Meta Ray-Ban glasses) due to their huge computational complexity. To overcome this, we turn to knowledge distillation and perform a thorough design-space exploration to achieve the best DiT for a given parameter size. In particular, we provide principles for how to choose design knobs such as depth, width, attention heads and distillation setup for a DiT. During the process, a three-way trade-off emerges between model performance, size and speed that is crucial for Edge implementation of diffusion. We also propose two distillation approaches - Teaching Assistant (TA) method and Multi-In-One (MI1) method - to perform feature distillation in the DiT context. Unlike existing solutions, we demonstrate and benchmark the efficacy of our approaches on practical Edge devices such as NVIDIA Jetson Orin Nano.",
    "arxiv_url": "http://arxiv.org/abs/2502.14226v1",
    "pdf_url": "http://arxiv.org/pdf/2502.14226v1",
    "published_date": "2025-02-20",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation",
    "authors": [
      "Yunpeng Zhang",
      "Qiang Wang",
      "Fan Jiang",
      "Yaqi Fan",
      "Mu Xu",
      "Yonggang Qi"
    ],
    "abstract": "Tuning-free approaches adapting large-scale pre-trained video diffusion models for identity-preserving text-to-video generation (IPT2V) have gained popularity recently due to their efficacy and scalability. However, significant challenges remain to achieve satisfied facial dynamics while keeping the identity unchanged. In this work, we present a novel tuning-free IPT2V framework by enhancing face knowledge of the pre-trained video model built on diffusion transformers (DiT), dubbed FantasyID. Essentially, 3D facial geometry prior is incorporated to ensure plausible facial structures during video synthesis. To prevent the model from learning copy-paste shortcuts that simply replicate reference face across frames, a multi-view face augmentation strategy is devised to capture diverse 2D facial appearance features, hence increasing the dynamics over the facial expressions and head poses. Additionally, after blending the 2D and 3D features as guidance, instead of naively employing cross-attention to inject guidance cues into DiT layers, a learnable layer-aware adaptive mechanism is employed to selectively inject the fused features into each individual DiT layers, facilitating balanced modeling of identity preservation and motion dynamics. Experimental results validate our model's superiority over the current tuning-free IPT2V methods.",
    "arxiv_url": "http://arxiv.org/abs/2502.13995v1",
    "pdf_url": "http://arxiv.org/pdf/2502.13995v1",
    "published_date": "2025-02-19",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AnyRefill: A Unified, Data-Efficient Framework for Left-Prompt-Guided Vision Tasks",
    "authors": [
      "Ming Xie",
      "Chenjie Cao",
      "Yunuo Cai",
      "Xiangyang Xue",
      "Yu-Gang Jiang",
      "Yanwei Fu"
    ],
    "abstract": "In this paper, we present a novel Left-Prompt-Guided (LPG) paradigm to address a diverse range of reference-based vision tasks. Inspired by the human creative process, we reformulate these tasks using a left-right stitching formulation to construct contextual input. Building upon this foundation, we propose AnyRefill, an extension of LeftRefill, that effectively adapts Text-to-Image (T2I) models to various vision tasks. AnyRefill leverages the inpainting priors of advanced T2I model based on the Diffusion Transformer (DiT) architecture, and incorporates flexible components to enhance its capabilities. By combining task-specific LoRAs with the stitching input, AnyRefill unlocks its potential across diverse tasks, including conditional generation, visual perception, and image editing, without requiring additional visual encoders. Meanwhile, AnyRefill exhibits remarkable data efficiency, requiring minimal task-specific fine-tuning while maintaining high generative performance. Through extensive ablation studies, we demonstrate that AnyRefill outperforms other image condition injection methods and achieves competitive results compared to state-of-the-art open-source methods. Notably, AnyRefill delivers results comparable to advanced commercial tools, such as IC-Light and SeedEdit, even in challenging scenarios. Comprehensive experiments and ablation studies across versatile tasks validate the strong generation of the proposed simple yet effective LPG formulation, establishing AnyRefill as a unified, highly data-efficient solution for reference-based vision tasks.",
    "arxiv_url": "http://arxiv.org/abs/2502.11158v2",
    "pdf_url": "http://arxiv.org/pdf/2502.11158v2",
    "published_date": "2025-02-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "diffusion transformer",
      "image editing"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BCDDM: Branch-Corrected Denoising Diffusion Model for Black Hole Image Generation",
    "authors": [
      "Ao liu",
      "Zelin Zhang",
      "Songbai Chen",
      "Cuihong Wen",
      "Jieci Wang"
    ],
    "abstract": "The properties of black holes and accretion flows can be inferred by fitting Event Horizon Telescope (EHT) data to simulated images generated through general relativistic ray tracing (GRRT). However, due to the computationally intensive nature of GRRT, the efficiency of generating specific radiation flux images needs to be improved. This paper introduces the Branch Correction Denoising Diffusion Model (BCDDM), which uses a branch correction mechanism and a weighted mixed loss function to improve the accuracy of generated black hole images based on seven physical parameters of the radiatively inefficient accretion flow (RIAF) model. Our experiments show a strong correlation between the generated images and their physical parameters. By enhancing the GRRT dataset with BCDDM-generated images and using ResNet50 for parameter regression, we achieve significant improvements in parameter prediction performance. This approach reduces computational costs and provides a faster, more efficient method for dataset expansion, parameter estimation, and model fitting.",
    "arxiv_url": "http://arxiv.org/abs/2502.08528v2",
    "pdf_url": "http://arxiv.org/pdf/2502.08528v2",
    "published_date": "2025-02-12",
    "categories": [
      "astro-ph.GA",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT",
    "authors": [
      "Dongyang Liu",
      "Shicheng Li",
      "Yutong Liu",
      "Zhen Li",
      "Kai Wang",
      "Xinyue Li",
      "Qi Qin",
      "Yufei Liu",
      "Yi Xin",
      "Zhongyu Li",
      "Bin Fu",
      "Chenyang Si",
      "Yuewen Cao",
      "Conghui He",
      "Ziwei Liu",
      "Yu Qiao",
      "Qibin Hou",
      "Hongsheng Li",
      "Peng Gao"
    ],
    "abstract": "Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data. To address this, we introduce Lumina-Video, a framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis. Lumina-Video incorporates a Multi-scale Next-DiT architecture, which jointly learns multiple patchifications to enhance both efficiency and flexibility. By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos' dynamic degree. Combined with a progressive training scheme with increasingly higher resolution and FPS, and a multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency. We additionally propose Lumina-V2A, a video-to-audio model based on Next-DiT, to create synchronized sounds for generated videos. Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.",
    "arxiv_url": "http://arxiv.org/abs/2502.06782v2",
    "pdf_url": "http://arxiv.org/pdf/2502.06782v2",
    "published_date": "2025-02-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Alpha-VLLM/Lumina-Video",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers",
    "authors": [
      "D. She",
      "Mushui Liu",
      "Jingxuan Pang",
      "Jin Wang",
      "Zhen Yang",
      "Wanggui He",
      "Guanghao Zhang",
      "Yi Wang",
      "Qihan Huang",
      "Haobin Tang",
      "Yunlong Yu",
      "Siming Fu"
    ],
    "abstract": "Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffusion transformer for personalized video generation from a reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish a new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality.",
    "arxiv_url": "http://arxiv.org/abs/2502.06527v2",
    "pdf_url": "http://arxiv.org/pdf/2502.06527v2",
    "published_date": "2025-02-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Universal Approximation of Visual Autoregressive Transformers",
    "authors": [
      "Yifang Chen",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song"
    ],
    "abstract": "We investigate the fundamental limits of transformer-based foundation models, extending our analysis to include Visual Autoregressive (VAR) transformers. VAR represents a big step toward generating images using a novel, scalable, coarse-to-fine ``next-scale prediction'' framework. These models set a new quality bar, outperforming all previous methods, including Diffusion Transformers, while having state-of-the-art performance for image synthesis tasks. Our primary contributions establish that, for single-head VAR transformers with a single self-attention layer and single interpolation layer, the VAR Transformer is universal. From the statistical perspective, we prove that such simple VAR transformers are universal approximators for any image-to-image Lipschitz functions. Furthermore, we demonstrate that flow-based autoregressive transformers inherit similar approximation capabilities. Our results provide important design principles for effective and computationally efficient VAR Transformer strategies that can be used to extend their utility to more sophisticated VAR models in image generation and other related areas.",
    "arxiv_url": "http://arxiv.org/abs/2502.06167v1",
    "pdf_url": "http://arxiv.org/pdf/2502.06167v1",
    "published_date": "2025-02-10",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile",
    "authors": [
      "Hangliang Ding",
      "Dacheng Li",
      "Runlong Su",
      "Peiyuan Zhang",
      "Zhijie Deng",
      "Ion Stoica",
      "Hao Zhang"
    ],
    "abstract": "Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.",
    "arxiv_url": "http://arxiv.org/abs/2502.06155v2",
    "pdf_url": "http://arxiv.org/pdf/2502.06155v2",
    "published_date": "2025-02-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation",
    "authors": [
      "Qijun Gan",
      "Yi Ren",
      "Chen Zhang",
      "Zhenhui Ye",
      "Pan Xie",
      "Xiang Yin",
      "Zehuan Yuan",
      "Bingyue Peng",
      "Jianke Zhu"
    ],
    "abstract": "Human motion video generation has advanced significantly, while existing methods still struggle with accurately rendering detailed body parts like hands and faces, especially in long sequences and intricate motions. Current approaches also rely on fixed resolution and struggle to maintain visual consistency. To address these limitations, we propose HumanDiT, a pose-guided Diffusion Transformer (DiT)-based framework trained on a large and wild dataset containing 14,000 hours of high-quality video to produce high-fidelity videos with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT, supports numerous video resolutions and variable sequence lengths, facilitating learning for long-sequence video generation; (ii) we introduce a prefix-latent reference strategy to maintain personalized characteristics across extended sequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to generate subsequent pose sequences, facilitating video continuation from static images or existing videos. It also utilizes a Pose Adapter to enable pose transfer with given sequences. Extensive experiments demonstrate its superior performance in generating long-form, pose-accurate videos across diverse scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2502.04847v3",
    "pdf_url": "http://arxiv.org/pdf/2502.04847v3",
    "published_date": "2025-02-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fast Video Generation with Sliding Tile Attention",
    "authors": [
      "Peiyuan Zhang",
      "Yongqi Chen",
      "Runlong Su",
      "Hangliang Ding",
      "Ion Stoica",
      "Zhenghong Liu",
      "Hao Zhang"
    ],
    "abstract": "Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being hardware-efficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09% drop on VBench.",
    "arxiv_url": "http://arxiv.org/abs/2502.04507v1",
    "pdf_url": "http://arxiv.org/pdf/2502.04507v1",
    "published_date": "2025-02-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video Generation",
    "authors": [
      "Lei Zhao",
      "Linfeng Feng",
      "Dongxu Ge",
      "Rujin Chen",
      "Fangqiu Yi",
      "Chi Zhang",
      "Xiao-Lei Zhang",
      "Xuelong Li"
    ],
    "abstract": "With the rise of diffusion models, audio-video generation has been revolutionized. However, most existing methods rely on separate modules for each modality, with limited exploration of unified generative architectures. In addition, many are confined to a single task and small-scale datasets. To address these limitations, we first propose UniForm, a unified multi-task diffusion transformer that jointly generates audio and visual modalities in a shared latent space. A single diffusion process models both audio and video, capturing the inherent correlations between sound and vision. Second, we introduce task-specific noise schemes and task tokens, enabling a single model to support multiple tasks, including text-to-audio-video, audio-to-video, and video-to-audio generation. Furthermore, by leveraging large language models and a large-scale text-audio-video combined dataset, UniForm achieves greater generative diversity than prior approaches. Extensive experiments show that UniForm achieves the state-of-the-art performance across audio-video generation tasks, producing content that is both well-aligned and close to real-world data distributions. Our demos are available at https://uniform-t2av.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2502.03897v4",
    "pdf_url": "http://arxiv.org/pdf/2502.03897v4",
    "published_date": "2025-02-06",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniCP: A Unified Caching and Pruning Framework for Efficient Video Generation",
    "authors": [
      "Wenzhang Sun",
      "Qirui Hou",
      "Donglin Di",
      "Jiahui Yang",
      "Yongjia Ma",
      "Jianxun Cui"
    ],
    "abstract": "Diffusion Transformers (DiT) excel in video generation but encounter significant computational challenges due to the quadratic complexity of attention. Notably, attention differences between adjacent diffusion steps follow a U-shaped pattern. Current methods leverage this property by caching attention blocks, however, they still struggle with sudden error spikes and large discrepancies. To address these issues, we propose UniCP a unified caching and pruning framework for efficient video generation. UniCP optimizes both temporal and spatial dimensions through. Error Aware Dynamic Cache Window (EDCW): Dynamically adjusts cache window sizes for different blocks at various timesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and Dynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS integrates caching and pruning by enabling dynamic switching between pruned and cached outputs. By adjusting cache windows and pruning redundant components, UniCP enhances computational efficiency and maintains video detail fidelity. Experimental results show that UniCP outperforms existing methods in both performance and efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2502.04393v1",
    "pdf_url": "http://arxiv.org/pdf/2502.04393v1",
    "published_date": "2025-02-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity",
    "authors": [
      "Haocheng Xi",
      "Shuo Yang",
      "Yilong Zhao",
      "Chenfeng Xu",
      "Muyang Li",
      "Xiuyu Li",
      "Yujun Lin",
      "Han Cai",
      "Jintao Zhang",
      "Dacheng Li",
      "Jianfei Chen",
      "Ion Stoica",
      "Kurt Keutzer",
      "Song Han"
    ],
    "abstract": "Diffusion Transformers (DiTs) dominate video generation but their high computational cost severely limits real-world applicability, usually requiring tens of minutes to generate a few seconds of video even on high-performance GPUs. This inefficiency primarily arises from the quadratic computational complexity of 3D Full Attention with respect to the context length. In this paper, we propose a training-free framework termed Sparse VideoGen (SVG) that leverages the inherent sparsity in 3D Full Attention to boost inference efficiency. We reveal that the attention heads can be dynamically classified into two groups depending on distinct sparse patterns: (1) Spatial Head, where only spatially-related tokens within each frame dominate the attention output, and (2) Temporal Head, where only temporally-related tokens across different frames dominate. Based on this insight, SVG proposes an online profiling strategy to capture the dynamic sparse patterns and predicts the type of attention head. Combined with a novel hardware-efficient tensor layout transformation and customized kernel implementations, SVG achieves up to 2.28x and 2.33x end-to-end speedup on CogVideoX-v1.5 and HunyuanVideo, respectively, while preserving generation quality. Our code is open-sourced and is available at https://github.com/svg-project/Sparse-VideoGen",
    "arxiv_url": "http://arxiv.org/abs/2502.01776v2",
    "pdf_url": "http://arxiv.org/pdf/2502.01776v2",
    "published_date": "2025-02-03",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "https://github.com/svg-project/Sparse-VideoGen",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MakeAnything: Harnessing Diffusion Transformers for Multi-Domain Procedural Sequence Generation",
    "authors": [
      "Yiren Song",
      "Cheng Liu",
      "Mike Zheng Shou"
    ],
    "abstract": "A hallmark of human intelligence is the ability to create complex artifacts through structured multi-step processes. Generating procedural tutorials with AI is a longstanding but challenging goal, facing three key obstacles: (1) scarcity of multi-task procedural datasets, (2) maintaining logical continuity and visual consistency between steps, and (3) generalizing across multiple domains. To address these challenges, we propose a multi-domain dataset covering 21 tasks with over 24,000 procedural sequences. Building upon this foundation, we introduce MakeAnything, a framework based on the diffusion transformer (DIT), which leverages fine-tuning to activate the in-context capabilities of DIT for generating consistent procedural sequences. We introduce asymmetric low-rank adaptation (LoRA) for image generation, which balances generalization capabilities and task-specific performance by freezing encoder parameters while adaptively tuning decoder layers. Additionally, our ReCraft model enables image-to-process generation through spatiotemporal consistency constraints, allowing static images to be decomposed into plausible creation sequences. Extensive experiments demonstrate that MakeAnything surpasses existing methods, setting new performance benchmarks for procedural generation tasks.",
    "arxiv_url": "http://arxiv.org/abs/2502.01572v2",
    "pdf_url": "http://arxiv.org/pdf/2502.01572v2",
    "published_date": "2025-02-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
    "authors": [
      "Gaojie Lin",
      "Jianwen Jiang",
      "Jiaqi Yang",
      "Zerong Zheng",
      "Chao Liang"
    ],
    "abstract": "End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io)",
    "arxiv_url": "http://arxiv.org/abs/2502.01061v2",
    "pdf_url": "http://arxiv.org/pdf/2502.01061v2",
    "published_date": "2025-02-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning",
    "authors": [
      "Yuanhuiyi Lyu",
      "Xu Zheng",
      "Lutao Jiang",
      "Yibo Yan",
      "Xin Zou",
      "Huiyu Zhou",
      "Linfeng Zhang",
      "Xuming Hu"
    ],
    "abstract": "Recent text-to-image generative models, e.g., Stable Diffusion V3 and Flux, have achieved notable progress. However, these models are strongly restricted to their limited knowledge, a.k.a., their own fixed parameters, that are trained with closed datasets. This leads to significant hallucinations or distortions when facing fine-grained and unseen novel real-world objects, e.g., the appearance of the Tesla Cybertruck. To this end, we present the first real-object-based retrieval-augmented generation framework (RealRAG), which augments fine-grained and unseen novel object generation by learning and retrieving real-world images to overcome the knowledge gaps of generative models. Specifically, to integrate missing memory for unseen novel object generation, we train a reflective retriever by self-reflective contrastive learning, which injects the generator's knowledge into the sef-reflective negatives, ensuring that the retrieved augmented images compensate for the model's missing knowledge. Furthermore, the real-object-based framework integrates fine-grained visual knowledge for the generative models, tackling the distortion problem and improving the realism for fine-grained object generation. Our Real-RAG is superior in its modular application to all types of state-of-the-art text-to-image generative models and also delivers remarkable performance boosts with all of them, such as a gain of 16.18% FID score with the auto-regressive model on the Stanford Car benchmark.",
    "arxiv_url": "http://arxiv.org/abs/2502.00848v1",
    "pdf_url": "http://arxiv.org/pdf/2502.00848v1",
    "published_date": "2025-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer",
    "authors": [
      "Enze Xie",
      "Junsong Chen",
      "Yuyang Zhao",
      "Jincheng Yu",
      "Ligeng Zhu",
      "Chengyue Wu",
      "Yujun Lin",
      "Zhekai Zhang",
      "Muyang Li",
      "Junyu Chen",
      "Han Cai",
      "Bingchen Liu",
      "Daquan Zhou",
      "Song Han"
    ],
    "abstract": "This paper presents SANA-1.5, a linear Diffusion Transformer for efficient scaling in text-to-image generation. Building upon SANA-1.0, we introduce three key innovations: (1) Efficient Training Scaling: A depth-growth paradigm that enables scaling from 1.6B to 4.8B parameters with significantly reduced computational resources, combined with a memory-efficient 8-bit optimizer. (2) Model Depth Pruning: A block importance analysis technique for efficient model compression to arbitrary sizes with minimal quality loss. (3) Inference-time Scaling: A repeated sampling strategy that trades computation for model capacity, enabling smaller models to match larger model quality at inference time. Through these strategies, SANA-1.5 achieves a text-image alignment score of 0.81 on GenEval, which can be further improved to 0.96 through inference scaling with VILA-Judge, establishing a new SoTA on GenEval benchmark. These innovations enable efficient model scaling across different compute budgets while maintaining high quality, making high-quality image generation more accessible. Our code and pre-trained models are released.",
    "arxiv_url": "http://arxiv.org/abs/2501.18427v3",
    "pdf_url": "http://arxiv.org/pdf/2501.18427v3",
    "published_date": "2025-01-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Accelerate High-Quality Diffusion Models with Inner Loop Feedback",
    "authors": [
      "Matthew Gwilliam",
      "Han Cai",
      "Di Wu",
      "Abhinav Shrivastava",
      "Zhiyu Cheng"
    ],
    "abstract": "We propose Inner Loop Feedback (ILF), a novel approach to accelerate diffusion models' inference. ILF trains a lightweight module to predict future features in the denoising process by leveraging the outputs from a chosen diffusion backbone block at a given time step. This approach exploits two key intuitions; (1) the outputs of a given block at adjacent time steps are similar, and (2) performing partial computations for a step imposes a lower burden on the model than skipping the step entirely. Our method is highly flexible, since we find that the feedback module itself can simply be a block from the diffusion backbone, with all settings copied. Its influence on the diffusion forward can be tempered with a learnable scaling factor from zero initialization. We train this module using distillation losses; however, unlike some prior work where a full diffusion backbone serves as the student, our model freezes the backbone, training only the feedback module. While many efforts to optimize diffusion models focus on achieving acceptable image quality in extremely few steps (1-4 steps), our emphasis is on matching best case results (typically achieved in 20 steps) while significantly reducing runtime. ILF achieves this balance effectively, demonstrating strong performance for both class-to-image generation with diffusion transformer (DiT) and text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The quality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP Image Quality Assessment, ImageReward, and qualitative comparisons. Project information is available at https://mgwillia.github.io/ilf.",
    "arxiv_url": "http://arxiv.org/abs/2501.13107v3",
    "pdf_url": "http://arxiv.org/pdf/2501.13107v3",
    "published_date": "2025-01-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LiT: Delving into a Simplified Linear Diffusion Transformer for Image Generation",
    "authors": [
      "Jiahao Wang",
      "Ning Kang",
      "Lewei Yao",
      "Mengzhao Chen",
      "Chengyue Wu",
      "Songyang Zhang",
      "Shuchen Xue",
      "Yong Liu",
      "Taiqiang Wu",
      "Xihui Liu",
      "Kaipeng Zhang",
      "Shifeng Zhang",
      "Wenqi Shao",
      "Zhenguo Li",
      "Ping Luo"
    ],
    "abstract": "In commonly used sub-quadratic complexity modules, linear attention benefits from simplicity and high parallelism, making it promising for image synthesis tasks. However, the architectural design and learning strategy for linear attention remain underexplored in this field. In this paper, we offer a suite of ready-to-use solutions for efficient linear diffusion Transformers. Our core contributions include: (1) Simplified Linear Attention using few heads, observing the free-lunch effect of performance without latency increase. (2) Weight inheritance from a fully pre-trained diffusion Transformer: initializing linear Transformer using pre-trained diffusion Transformer and loading all parameters except for those related to linear attention. (3) Hybrid knowledge distillation objective: using a pre-trained diffusion Transformer to help the training of the student linear Transformer, supervising not only the predicted noise but also the variance of the reverse diffusion process. These guidelines lead to our proposed Linear Diffusion Transformer (LiT), an efficient text-to-image Transformer that can be deployed offline on a laptop. Experiments show that in class-conditional 256*256 and 512*512 ImageNet benchmark LiT achieves highly competitive FID while reducing training steps by 80% and 77% compared to DiT. LiT also rivals methods based on Mamba or Gated Linear Attention. Besides, for text-to-image generation, LiT allows for the rapid synthesis of up to 1K resolution photorealistic images. Project page: https://techmonsterwang.github.io/LiT/.",
    "arxiv_url": "http://arxiv.org/abs/2501.12976v1",
    "pdf_url": "http://arxiv.org/pdf/2501.12976v1",
    "published_date": "2025-01-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CatV2TON: Taming Diffusion Transformers for Vision-Based Virtual Try-On with Temporal Concatenation",
    "authors": [
      "Zheng Chong",
      "Wenqing Zhang",
      "Shiyue Zhang",
      "Jun Zheng",
      "Xiao Dong",
      "Haoxiang Li",
      "Yiling Wu",
      "Dongmei Jiang",
      "Xiaodan Liang"
    ],
    "abstract": "Virtual try-on (VTON) technology has gained attention due to its potential to transform online retail by enabling realistic clothing visualization of images and videos. However, most existing methods struggle to achieve high-quality results across image and video try-on tasks, especially in long video scenarios. In this work, we introduce CatV2TON, a simple and effective vision-based virtual try-on (V2TON) method that supports both image and video try-on tasks with a single diffusion transformer model. By temporally concatenating garment and person inputs and training on a mix of image and video datasets, CatV2TON achieves robust try-on performance across static and dynamic settings. For efficient long-video generation, we propose an overlapping clip-based inference strategy that uses sequential frame guidance and Adaptive Clip Normalization (AdaCN) to maintain temporal consistency with reduced resource demands. We also present ViViD-S, a refined video try-on dataset, achieved by filtering back-facing frames and applying 3D mask smoothing for enhanced temporal consistency. Comprehensive experiments demonstrate that CatV2TON outperforms existing methods in both image and video try-on tasks, offering a versatile and reliable solution for realistic virtual try-ons across diverse scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2501.11325v1",
    "pdf_url": "http://arxiv.org/pdf/2501.11325v1",
    "published_date": "2025-01-20",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T42 (Primary) 168T45 (Secondary)",
      "I.4.9"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learnings from Scaling Visual Tokenizers for Reconstruction and Generation",
    "authors": [
      "Philippe Hansen-Estruch",
      "David Yan",
      "Ching-Yao Chung",
      "Orr Zohar",
      "Jialiang Wang",
      "Tingbo Hou",
      "Tao Xu",
      "Sriram Vishwanath",
      "Peter Vajda",
      "Xinlei Chen"
    ],
    "abstract": "Visual tokenization via auto-encoding empowers state-of-the-art image and video generative models by compressing pixels into a latent space. Although scaling Transformer-based generators has been central to recent advances, the tokenizer component itself is rarely scaled, leaving open questions about how auto-encoder design choices influence both its objective of reconstruction and downstream generative performance. Our work aims to conduct an exploration of scaling in auto-encoders to fill in this blank. To facilitate this exploration, we replace the typical convolutional backbone with an enhanced Vision Transformer architecture for Tokenization (ViTok). We train ViTok on large-scale image and video datasets far exceeding ImageNet-1K, removing data constraints on tokenizer scaling. We first study how scaling the auto-encoder bottleneck affects both reconstruction and generation -- and find that while it is highly correlated with reconstruction, its relationship with generation is more complex. We next explored the effect of separately scaling the auto-encoders' encoder and decoder on reconstruction and generation performance. Crucially, we find that scaling the encoder yields minimal gains for either reconstruction or generation, while scaling the decoder boosts reconstruction but the benefits for generation are mixed. Building on our exploration, we design ViTok as a lightweight auto-encoder that achieves competitive performance with state-of-the-art auto-encoders on ImageNet-1K and COCO reconstruction tasks (256p and 512p) while outperforming existing auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates competitive performance on image generation for ImageNet-1K and sets new state-of-the-art benchmarks for class-conditional video generation on UCF-101.",
    "arxiv_url": "http://arxiv.org/abs/2501.09755v1",
    "pdf_url": "http://arxiv.org/pdf/2501.09755v1",
    "published_date": "2025-01-16",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.10; I.4.2; I.4.5"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing Image Generation Fidelity via Progressive Prompts",
    "authors": [
      "Zhen Xiong",
      "Yuqi Li",
      "Chuanguang Yang",
      "Tiao Tan",
      "Zhihong Zhu",
      "Siyuan Li",
      "Yue Ma"
    ],
    "abstract": "The diffusion transformer (DiT) architecture has attracted significant attention in image generation, achieving better fidelity, performance, and diversity. However, most existing DiT - based image generation methods focus on global - aware synthesis, and regional prompt control has been less explored. In this paper, we propose a coarse - to - fine generation pipeline for regional prompt - following generation. Specifically, we first utilize the powerful large language model (LLM) to generate both high - level descriptions of the image (such as content, topic, and objects) and low - level descriptions (such as details and style). Then, we explore the influence of cross - attention layers at different depths. We find that deeper layers are always responsible for high - level content control, while shallow layers handle low - level content control. Various prompts are injected into the proposed regional cross - attention control for coarse - to - fine generation. By using the proposed pipeline, we enhance the controllability of DiT - based image generation. Extensive quantitative and qualitative results show that our pipeline can improve the performance of the generated images.",
    "arxiv_url": "http://arxiv.org/abs/2501.07070v1",
    "pdf_url": "http://arxiv.org/pdf/2501.07070v1",
    "published_date": "2025-01-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-subject Open-set Personalization in Video Generation",
    "authors": [
      "Tsai-Shien Chen",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Yuwei Fang",
      "Kwot Sin Lee",
      "Ivan Skorokhodov",
      "Kfir Aberman",
      "Jun-Yan Zhu",
      "Ming-Hsuan Yang",
      "Sergey Tulyakov"
    ],
    "abstract": "Video personalization methods allow us to synthesize videos with specific concepts such as people, pets, and places. However, existing methods often focus on limited domains, require time-consuming optimization per subject, or support only a single subject. We present Video Alchemist $-$ a video model with built-in multi-subject, open-set personalization capabilities for both foreground objects and background, eliminating the need for time-consuming test-time optimization. Our model is built on a new Diffusion Transformer module that fuses each conditional reference image and its corresponding subject-level text prompt with cross-attention layers. Developing such a large model presents two main challenges: dataset and evaluation. First, as paired datasets of reference images and videos are extremely hard to collect, we sample selected video frames as reference images and synthesize a clip of the target video. However, while models can easily denoise training videos given reference frames, they fail to generalize to new contexts. To mitigate this issue, we design a new automatic data construction pipeline with extensive image augmentations. Second, evaluating open-set video personalization is a challenge in itself. To address this, we introduce a personalization benchmark that focuses on accurate subject fidelity and supports diverse personalization scenarios. Finally, our extensive experiments show that our method significantly outperforms existing personalization methods in both quantitative and qualitative evaluations.",
    "arxiv_url": "http://arxiv.org/abs/2501.06187v2",
    "pdf_url": "http://arxiv.org/pdf/2501.06187v2",
    "published_date": "2025-01-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DIS-FLUX: simple and efficient multi-instance generation with DiT rendering",
    "authors": [
      "Dewei Zhou",
      "Ji Xie",
      "Zongxin Yang",
      "Yi Yang"
    ],
    "abstract": "The growing demand for controllable outputs in text-to-image generation has driven significant advancements in multi-instance generation (MIG), enabling users to define both instance layouts and attributes. Currently, the state-of-the-art methods in MIG are primarily adapter-based. However, these methods necessitate retraining a new adapter each time a more advanced model is released, resulting in significant resource consumption. A methodology named Depth-Driven Decoupled Instance Synthesis (3DIS) has been introduced, which decouples MIG into two distinct phases: 1) depth-based scene construction and 2) detail rendering with widely pre-trained depth control models. The 3DIS method requires adapter training solely during the scene construction phase, while enabling various models to perform training-free detail rendering. Initially, 3DIS focused on rendering techniques utilizing U-Net architectures such as SD1.5, SD2, and SDXL, without exploring the potential of recent DiT-based models like FLUX. In this paper, we present 3DIS-FLUX, an extension of the 3DIS framework that integrates the FLUX model for enhanced rendering capabilities. Specifically, we employ the FLUX.1-Depth-dev model for depth map controlled image generation and introduce a detail renderer that manipulates the Attention Mask in FLUX's Joint Attention mechanism based on layout information. This approach allows for the precise rendering of fine-grained attributes of each instance. Our experimental results indicate that 3DIS-FLUX, leveraging the FLUX model, outperforms the original 3DIS method, which utilized SD2 and SDXL, and surpasses current state-of-the-art adapter-based methods in terms of both performance and image quality. Project Page: https://limuloo.github.io/3DIS/.",
    "arxiv_url": "http://arxiv.org/abs/2501.05131v1",
    "pdf_url": "http://arxiv.org/pdf/2501.05131v1",
    "published_date": "2025-01-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "Control",
      "image generation",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning",
    "authors": [
      "Yuzhou Huang",
      "Ziyang Yuan",
      "Quande Liu",
      "Qiulin Wang",
      "Xintao Wang",
      "Ruimao Zhang",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai"
    ],
    "abstract": "Text-to-video generation has made remarkable advancements through diffusion models. However, Multi-Concept Video Customization (MCVC) remains a significant challenge. We identify two key challenges in this task: 1) the identity decoupling problem, where directly adopting existing customization methods inevitably mix attributes when handling multiple concepts simultaneously, and 2) the scarcity of high-quality video-entity pairs, which is crucial for training such a model that represents and decouples various concepts well. To address these challenges, we introduce ConceptMaster, an innovative framework that effectively tackles the critical issues of identity decoupling while maintaining concept fidelity in customized videos. Specifically, we introduce a novel strategy of learning decoupled multi-concept embeddings that are injected into the diffusion models in a standalone manner, which effectively guarantees the quality of customized videos with multiple identities, even for highly similar visual concepts. To further overcome the scarcity of high-quality MCVC data, we carefully establish a data construction pipeline, which enables systematic collection of precise multi-concept video-entity data across diverse concepts. A comprehensive benchmark is designed to validate the effectiveness of our model from three critical dimensions: concept fidelity, identity decoupling ability, and video generation quality across six different concept composition scenarios. Extensive experiments demonstrate that our ConceptMaster significantly outperforms previous approaches for this task, paving the way for generating personalized and semantically accurate videos across multiple concepts.",
    "arxiv_url": "http://arxiv.org/abs/2501.04698v1",
    "pdf_url": "http://arxiv.org/pdf/2501.04698v1",
    "published_date": "2025-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Circuit Complexity Bounds for Visual Autoregressive Model",
    "authors": [
      "Yekun Ke",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song"
    ],
    "abstract": "Understanding the expressive ability of a specific model is essential for grasping its capacity limitations. Recently, several studies have established circuit complexity bounds for Transformer architecture. Besides, the Visual AutoRegressive (VAR) model has risen to be a prominent method in the field of image generation, outperforming previous techniques, such as Diffusion Transformers, in generating high-quality images. We investigate the circuit complexity of the VAR model and establish a bound in this study. Our primary result demonstrates that the VAR model is equivalent to a simulation by a uniform $\\mathsf{TC}^0$ threshold circuit with hidden dimension $d \\leq O(n)$ and $\\mathrm{poly}(n)$ precision. This is the first study to rigorously highlight the limitations in the expressive power of VAR models despite their impressive performance. We believe our findings will offer valuable insights into the inherent constraints of these models and guide the development of more efficient and expressive architectures in the future.",
    "arxiv_url": "http://arxiv.org/abs/2501.04299v1",
    "pdf_url": "http://arxiv.org/pdf/2501.04299v1",
    "published_date": "2025-01-08",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CC",
      "cs.CL",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers",
    "authors": [
      "Yuechen Zhang",
      "Yaoyang Liu",
      "Bin Xia",
      "Bohao Peng",
      "Zexin Yan",
      "Eric Lo",
      "Jiaya Jia"
    ],
    "abstract": "We present Magic Mirror, a framework for generating identity-preserved videos with cinematic-level quality and dynamic motion. While recent advances in video diffusion models have shown impressive capabilities in text-to-video generation, maintaining consistent identity while producing natural motion remains challenging. Previous methods either require person-specific fine-tuning or struggle to balance identity preservation with motion diversity. Built upon Video Diffusion Transformers, our method introduces three key components: (1) a dual-branch facial feature extractor that captures both identity and structural features, (2) a lightweight cross-modal adapter with Conditioned Adaptive Normalization for efficient identity integration, and (3) a two-stage training strategy combining synthetic identity pairs with video data. Extensive experiments demonstrate that Magic Mirror effectively balances identity consistency with natural motion, outperforming existing methods across multiple metrics while requiring minimal parameters added. The code and model will be made publicly available at: https://github.com/dvlab-research/MagicMirror/",
    "arxiv_url": "http://arxiv.org/abs/2501.03931v1",
    "pdf_url": "http://arxiv.org/pdf/2501.03931v1",
    "published_date": "2025-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/dvlab-research/MagicMirror",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TransPixeler: Advancing Text-to-Video Generation with Transparency",
    "authors": [
      "Luozhou Wang",
      "Yijun Li",
      "Zhifei Chen",
      "Jui-Hsien Wang",
      "Zhifei Zhang",
      "He Zhang",
      "Zhe Lin",
      "Yingcong Chen"
    ],
    "abstract": "Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixeler, a method to extend pretrained video models for RGBA generation while retaining the original RGB capabilities. TransPixar leverages a diffusion transformer (DiT) architecture, incorporating alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB and alpha channels with high consistency. By optimizing attention mechanisms, TransPixar preserves the strengths of the original RGB model and achieves strong alignment between RGB and alpha channels despite limited training data. Our approach effectively generates diverse and consistent RGBA videos, advancing the possibilities for VFX and interactive content creation.",
    "arxiv_url": "http://arxiv.org/abs/2501.03006v2",
    "pdf_url": "http://arxiv.org/pdf/2501.03006v2",
    "published_date": "2025-01-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking",
    "authors": [
      "Weikang Bian",
      "Zhaoyang Huang",
      "Xiaoyu Shi",
      "Yijin Li",
      "Fu-Yun Wang",
      "Hongsheng Li"
    ],
    "abstract": "4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.",
    "arxiv_url": "http://arxiv.org/abs/2501.02690v1",
    "pdf_url": "http://arxiv.org/pdf/2501.02690v1",
    "published_date": "2025-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EliGen: Entity-Level Controlled Image Generation with Regional Attention",
    "authors": [
      "Hong Zhang",
      "Zhongjie Duan",
      "Xingjun Wang",
      "Yingda Chen",
      "Yu Zhang"
    ],
    "abstract": "Recent advancements in diffusion models have significantly advanced text-to-image generation, yet global text prompts alone remain insufficient for achieving fine-grained control over individual entities within an image. To address this limitation, we present EliGen, a novel framework for Entity-level controlled image Generation. Firstly, we put forward regional attention, a mechanism for diffusion transformers that requires no additional parameters, seamlessly integrating entity prompts and arbitrary-shaped spatial masks. By contributing a high-quality dataset with fine-grained spatial and semantic entity-level annotations, we train EliGen to achieve robust and accurate entity-level manipulation, surpassing existing methods in both spatial precision and image quality. Additionally, we propose an inpainting fusion pipeline, extending its capabilities to multi-entity image inpainting tasks. We further demonstrate its flexibility by integrating it with other open-source models such as IP-Adapter, In-Context LoRA and MLLM, unlocking new creative possibilities. The source code, model, and dataset are published at https://github.com/modelscope/DiffSynth-Studio.git.",
    "arxiv_url": "http://arxiv.org/abs/2501.01097v3",
    "pdf_url": "http://arxiv.org/pdf/2501.01097v3",
    "published_date": "2025-01-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/modelscope/DiffSynth-Studio.git",
    "keywords": [
      "text-to-image",
      "image inpainting",
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dual Diffusion for Unified Image Generation and Understanding",
    "authors": [
      "Zijie Li",
      "Henry Li",
      "Yichun Shi",
      "Amir Barati Farimani",
      "Yuval Kluger",
      "Linjie Yang",
      "Peng Wang"
    ],
    "abstract": "Diffusion models have gained tremendous success in text-to-image generation, yet still lag behind with visual understanding tasks, an area dominated by autoregressive vision-language models. We propose a large-scale and fully end-to-end diffusion model for multi-modal understanding and generation that significantly improves on existing diffusion-based multimodal models, and is the first of its kind to support the full suite of vision-language modeling capabilities. Inspired by the multimodal diffusion transformer (MM-DiT) and recent advances in discrete diffusion language modeling, we leverage a cross-modal maximum likelihood estimation framework that simultaneously trains the conditional likelihoods of both images and text jointly under a single loss function, which is back-propagated through both branches of the diffusion transformer. The resulting model is highly flexible and capable of a wide range of tasks including image generation, captioning, and visual question answering. Our model attained competitive performance compared to recent unified image understanding and generation models, demonstrating the potential of multimodal diffusion modeling as a promising alternative to autoregressive next-token prediction models.",
    "arxiv_url": "http://arxiv.org/abs/2501.00289v2",
    "pdf_url": "http://arxiv.org/pdf/2501.00289v2",
    "published_date": "2024-12-31",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Open-Sora: Democratizing Efficient Video Production for All",
    "authors": [
      "Zangwei Zheng",
      "Xiangyu Peng",
      "Tianji Yang",
      "Chenhui Shen",
      "Shenggui Li",
      "Hongxin Liu",
      "Yukun Zhou",
      "Tianyi Li",
      "Yang You"
    ],
    "abstract": "Vision and language are the two foundational senses for humans, and they build up our cognitive ability and intelligence. While significant breakthroughs have been made in AI language ability, artificial visual intelligence, especially the ability to generate and simulate the world we see, is far lagging behind. To facilitate the development and accessibility of artificial visual intelligence, we created Open-Sora, an open-source video generation model designed to produce high-fidelity video content. Open-Sora supports a wide spectrum of visual generation tasks, including text-to-image generation, text-to-video generation, and image-to-video generation. The model leverages advanced deep learning architectures and training/inference techniques to enable flexible video synthesis, which could generate video content of up to 15 seconds, up to 720p resolution, and arbitrary aspect ratios. Specifically, we introduce Spatial-Temporal Diffusion Transformer (STDiT), an efficient diffusion framework for videos that decouples spatial and temporal attention. We also introduce a highly compressive 3D autoencoder to make representations compact and further accelerate training with an ad hoc training strategy. Through this initiative, we aim to foster innovation, creativity, and inclusivity within the community of AI content creation. By embracing the open-source principle, Open-Sora democratizes full access to all the training/inference/data preparation codes as well as model weights. All resources are publicly available at: https://github.com/hpcaitech/Open-Sora.",
    "arxiv_url": "http://arxiv.org/abs/2412.20404v1",
    "pdf_url": "http://arxiv.org/pdf/2412.20404v1",
    "published_date": "2024-12-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/hpcaitech/Open-Sora",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UNIC-Adapter: Unified Image-instruction Adapter with Multi-modal Transformer for Image Generation",
    "authors": [
      "Lunhao Duan",
      "Shanshan Zhao",
      "Wenjun Yan",
      "Yinglun Li",
      "Qing-Guo Chen",
      "Zhao Xu",
      "Weihua Luo",
      "Kaifu Zhang",
      "Mingming Gong",
      "Gui-Song Xia"
    ],
    "abstract": "Recently, text-to-image generation models have achieved remarkable advancements, particularly with diffusion models facilitating high-quality image synthesis from textual descriptions. However, these models often struggle with achieving precise control over pixel-level layouts, object appearances, and global styles when using text prompts alone. To mitigate this issue, previous works introduce conditional images as auxiliary inputs for image generation, enhancing control but typically necessitating specialized models tailored to different types of reference inputs. In this paper, we explore a new approach to unify controllable generation within a single framework. Specifically, we propose the unified image-instruction adapter (UNIC-Adapter) built on the Multi-Modal-Diffusion Transformer architecture, to enable flexible and controllable generation across diverse conditions without the need for multiple specialized models. Our UNIC-Adapter effectively extracts multi-modal instruction information by incorporating both conditional images and task instructions, injecting this information into the image generation process through a cross-attention mechanism enhanced by Rotary Position Embedding. Experimental results across a variety of tasks, including pixel-level spatial control, subject-driven image generation, and style-image-based image synthesis, demonstrate the effectiveness of our UNIC-Adapter in unified controllable image generation.",
    "arxiv_url": "http://arxiv.org/abs/2412.18928v1",
    "pdf_url": "http://arxiv.org/pdf/2412.18928v1",
    "published_date": "2024-12-25",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "Control",
      "image generation",
      "diffusion transformer",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
    "authors": [
      "Chang Zou",
      "Evelyn Zhang",
      "Runlin Guo",
      "Haohang Xu",
      "Conghui He",
      "Xuming Hu",
      "Linfeng Zhang"
    ],
    "abstract": "Diffusion Transformers (DiT) have become the dominant methods in image and video generation yet still suffer substantial computational costs. As an effective approach for DiT acceleration, feature caching methods are designed to cache the features of DiT in previous timesteps and reuse them in the next timesteps, allowing us to skip the computation in the next timesteps. However, on the one hand, aggressively reusing all the features cached in previous timesteps leads to a severe drop in generation quality. On the other hand, conservatively caching only the features in the redundant layers or tokens but still computing the important ones successfully preserves the generation quality but results in reductions in acceleration ratios. Observing such a tradeoff between generation quality and acceleration performance, this paper begins by quantitatively studying the accumulated error from cached features. Surprisingly, we find that aggressive caching does not introduce significantly more caching errors in the caching step, and the conservative feature caching can fix the error introduced by aggressive caching. Thereby, we propose a dual caching strategy that adopts aggressive and conservative caching iteratively, leading to significant acceleration and high generation quality at the same time. Besides, we further introduce a V-caching strategy for token-wise conservative caching, which is compatible with flash attention and requires no training and calibration data.   Our codes have been released in Github: \\textbf{Code: \\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
    "arxiv_url": "http://arxiv.org/abs/2412.18911v1",
    "pdf_url": "http://arxiv.org/pdf/2412.18911v1",
    "published_date": "2024-12-25",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "https://github.com/Shenyi-Z/DuCa",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "1.58-bit FLUX",
    "authors": [
      "Chenglin Yang",
      "Celong Liu",
      "Xueqing Deng",
      "Dongwon Kim",
      "Xing Mei",
      "Xiaohui Shen",
      "Liang-Chieh Chen"
    ],
    "abstract": "We present 1.58-bit FLUX, the first successful approach to quantizing the state-of-the-art text-to-image generation model, FLUX.1-dev, using 1.58-bit weights (i.e., values in {-1, 0, +1}) while maintaining comparable performance for generating 1024 x 1024 images. Notably, our quantization method operates without access to image data, relying solely on self-supervision from the FLUX.1-dev model. Additionally, we develop a custom kernel optimized for 1.58-bit operations, achieving a 7.7x reduction in model storage, a 5.1x reduction in inference memory, and improved inference latency. Extensive evaluations on the GenEval and T2I Compbench benchmarks demonstrate the effectiveness of 1.58-bit FLUX in maintaining generation quality while significantly enhancing computational efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2412.18653v1",
    "pdf_url": "http://arxiv.org/pdf/2412.18653v1",
    "published_date": "2024-12-24",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation",
    "authors": [
      "Minghong Cai",
      "Xiaodong Cun",
      "Xiaoyu Li",
      "Wenze Liu",
      "Zhaoyang Zhang",
      "Yong Zhang",
      "Ying Shan",
      "Xiangyu Yue"
    ],
    "abstract": "Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer MM-DiT architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiT's attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training.",
    "arxiv_url": "http://arxiv.org/abs/2412.18597v2",
    "pdf_url": "http://arxiv.org/pdf/2412.18597v2",
    "published_date": "2024-12-24",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "video editing",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FFA Sora, video generation as fundus fluorescein angiography simulator",
    "authors": [
      "Xinyuan Wu",
      "Lili Wang",
      "Ruoyu Chen",
      "Bowen Liu",
      "Weiyi Zhang",
      "Xi Yang",
      "Yifan Feng",
      "Mingguang He",
      "Danli Shi"
    ],
    "abstract": "Fundus fluorescein angiography (FFA) is critical for diagnosing retinal vascular diseases, but beginners often struggle with image interpretation. This study develops FFA Sora, a text-to-video model that converts FFA reports into dynamic videos via a Wavelet-Flow Variational Autoencoder (WF-VAE) and a diffusion transformer (DiT). Trained on an anonymized dataset, FFA Sora accurately simulates disease features from the input text, as confirmed by objective metrics: Frechet Video Distance (FVD) = 329.78, Learned Perceptual Image Patch Similarity (LPIPS) = 0.48, and Visual-question-answering Score (VQAScore) = 0.61. Specific evaluations showed acceptable alignment between the generated videos and textual prompts, with BERTScore of 0.35. Additionally, the model demonstrated strong privacy-preserving performance in retrieval evaluations, achieving an average Recall@K of 0.073. Human assessments indicated satisfactory visual quality, with an average score of 1.570(scale: 1 = best, 5 = worst). This model addresses privacy concerns associated with sharing large-scale FFA data and enhances medical education.",
    "arxiv_url": "http://arxiv.org/abs/2412.17346v1",
    "pdf_url": "http://arxiv.org/pdf/2412.17346v1",
    "published_date": "2024-12-23",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers",
    "authors": [
      "Haoran You",
      "Connelly Barnes",
      "Yuqian Zhou",
      "Yan Kang",
      "Zhenbang Du",
      "Wei Zhou",
      "Lingzhi Zhang",
      "Yotam Nitzan",
      "Xiaoyang Liu",
      "Zhe Lin",
      "Eli Shechtman",
      "Sohrab Amirghodsi",
      "Yingyan Celine Lin"
    ],
    "abstract": "Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) image generation quality but suffer from high latency and memory inefficiency, making them difficult to deploy on resource-constrained devices. One major efficiency bottleneck is that existing DiTs apply equal computation across all regions of an image. However, not all image tokens are equally important, and certain localized areas require more computation, such as objects. To address this, we propose DiffCR, a dynamic DiT inference framework with differentiable compression ratios, which automatically learns to dynamically route computation across layers and timesteps for each image token, resulting in efficient DiTs. Specifically, DiffCR integrates three features: (1) A token-level routing scheme where each DiT layer includes a router that is fine-tuned jointly with model weights to predict token importance scores. In this way, unimportant tokens bypass the entire layer's computation; (2) A layer-wise differentiable ratio mechanism where different DiT layers automatically learn varying compression ratios from a zero initialization, resulting in large compression ratios in redundant layers while others remain less compressed or even uncompressed; (3) A timestep-wise differentiable ratio mechanism where each denoising timestep learns its own compression ratio. The resulting pattern shows higher ratios for noisier timesteps and lower ratios as the image becomes clearer. Extensive experiments on text-to-image and inpainting tasks show that DiffCR effectively captures dynamism across token, layer, and timestep axes, achieving superior trade-offs between generation quality and efficiency compared to prior works. The project website is available at https://www.haoranyou.com/diffcr.",
    "arxiv_url": "http://arxiv.org/abs/2412.16822v2",
    "pdf_url": "http://arxiv.org/pdf/2412.16822v2",
    "published_date": "2024-12-22",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up",
    "authors": [
      "Songhua Liu",
      "Zhenxiong Tan",
      "Xinchao Wang"
    ],
    "abstract": "Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, we aim at a linear attention mechanism in this paper that reduces the complexity of pre-trained DiTs to linear. We begin our exploration with a comprehensive summary of existing efficient attention mechanisms and identify four key factors crucial for successful linearization of pre-trained DiTs: locality, formulation consistency, high-rank attention maps, and feature integrity. Based on these insights, we introduce a convolution-like local attention strategy termed CLEAR, which limits feature interactions to a local window around each query token, and thus achieves linear complexity. Our experiments indicate that, by fine-tuning the attention layer on merely 10K self-generated samples for 10K iterations, we can effectively transfer knowledge from a pre-trained DiT to a student model with linear complexity, yielding results comparable to the teacher model. Simultaneously, it reduces attention computations by 99.5% and accelerates generation by 6.3 times for generating 8K-resolution images. Furthermore, we investigate favorable properties in the distilled attention layers, such as zero-shot generalization cross various models and plugins, and improved support for multi-GPU parallel inference. Models and codes are available here: https://github.com/Huage001/CLEAR.",
    "arxiv_url": "http://arxiv.org/abs/2412.16112v1",
    "pdf_url": "http://arxiv.org/pdf/2412.16112v1",
    "published_date": "2024-12-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Huage001/CLEAR",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Scaling of Diffusion Transformers for Text-to-Image Generation",
    "authors": [
      "Hao Li",
      "Shamit Lal",
      "Zhiheng Li",
      "Yusheng Xie",
      "Ying Wang",
      "Yang Zou",
      "Orchid Majumder",
      "R. Manmatha",
      "Zhuowen Tu",
      "Stefano Ermon",
      "Stefano Soatto",
      "Ashwin Swaminathan"
    ],
    "abstract": "We empirically study the scaling properties of various Diffusion Transformers (DiTs) for text-to-image generation by performing extensive and rigorous ablations, including training scaled DiTs ranging from 0.3B upto 8B parameters on datasets up to 600M images. We find that U-ViT, a pure self-attention based DiT model provides a simpler design and scales more effectively in comparison with cross-attention based DiT variants, which allows straightforward expansion for extra conditions and other modalities. We identify a 2.3B U-ViT model can get better performance than SDXL UNet and other DiT variants in controlled setting. On the data scaling side, we investigate how increasing dataset size and enhanced long caption improve the text-image alignment performance and the learning efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2412.12391v1",
    "pdf_url": "http://arxiv.org/pdf/2412.12391v1",
    "published_date": "2024-12-16",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Causal Diffusion Transformers for Generative Modeling",
    "authors": [
      "Chaorui Deng",
      "Deyao Zhu",
      "Kunchang Li",
      "Shi Guang",
      "Haoqi Fan"
    ],
    "abstract": "We introduce Causal Diffusion as the autoregressive (AR) counterpart of Diffusion models. It is a next-token(s) forecasting framework that is friendly to both discrete and continuous modalities and compatible with existing next-token prediction models like LLaMA and GPT. While recent works attempt to combine diffusion with AR models, we show that introducing sequential factorization to a diffusion model can substantially improve its performance and enables a smooth transition between AR and diffusion generation modes. Hence, we propose CausalFusion - a decoder-only transformer that dual-factorizes data across sequential tokens and diffusion noise levels, leading to state-of-the-art results on the ImageNet generation benchmark while also enjoying the AR advantage of generating an arbitrary number of tokens for in-context reasoning. We further demonstrate CausalFusion's multimodal capabilities through a joint image generation and captioning model, and showcase CausalFusion's ability for zero-shot in-context image manipulations. We hope that this work could provide the community with a fresh perspective on training multimodal models over discrete and continuous data.",
    "arxiv_url": "http://arxiv.org/abs/2412.12095v2",
    "pdf_url": "http://arxiv.org/pdf/2412.12095v2",
    "published_date": "2024-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Video Diffusion Transformers are In-Context Learners",
    "authors": [
      "Zhengcong Fei",
      "Di Qiu",
      "Debang Li",
      "Changqian Yu",
      "Mingyuan Fan"
    ],
    "abstract": "This paper investigates a solution for enabling in-context capabilities of video diffusion transformers, with minimal tuning required for activation. Specifically, we propose a simple pipeline to leverage in-context generation: ($\\textbf{i}$) concatenate videos along spacial or time dimension, ($\\textbf{ii}$) jointly caption multi-scene video clips from one source, and ($\\textbf{iii}$) apply task-specific fine-tuning using carefully curated small datasets. Through a series of diverse controllable tasks, we demonstrate qualitatively that existing advanced text-to-video models can effectively perform in-context generation. Notably, it allows for the creation of consistent multi-scene videos exceeding 30 seconds in duration, without additional computational overhead. Importantly, this method requires no modifications to the original models, results in high-fidelity video outputs that better align with prompt specifications and maintain role consistency. Our framework presents a valuable tool for the research community and offers critical insights for advancing product-level controllable video generation systems. The data, code, and model weights are publicly available at: https://github.com/feizc/Video-In-Context.",
    "arxiv_url": "http://arxiv.org/abs/2412.10783v3",
    "pdf_url": "http://arxiv.org/pdf/2412.10783v3",
    "published_date": "2024-12-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/feizc/Video-In-Context",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity",
    "authors": [
      "Hongjie Wang",
      "Chih-Yao Ma",
      "Yen-Cheng Liu",
      "Ji Hou",
      "Tao Xu",
      "Jialiang Wang",
      "Felix Juefei-Xu",
      "Yaqiao Luo",
      "Peizhao Zhang",
      "Tingbo Hou",
      "Peter Vajda",
      "Niraj K. Jha",
      "Xiaoliang Dai"
    ],
    "abstract": "Text-to-video generation enhances content creation but is highly computationally intensive: The computational cost of Diffusion Transformers (DiTs) scales quadratically in the number of pixels. This makes minute-length video generation extremely expensive, limiting most existing models to generating videos of only 10-20 seconds length. We propose a Linear-complexity text-to-video Generation (LinGen) framework whose cost scales linearly in the number of pixels. For the first time, LinGen enables high-resolution minute-length video generation on a single GPU without compromising quality. It replaces the computationally-dominant and quadratic-complexity block, self-attention, with a linear-complexity block called MATE, which consists of an MA-branch and a TE-branch. The MA-branch targets short-to-long-range correlations, combining a bidirectional Mamba2 block with our token rearrangement method, Rotary Major Scan, and our review tokens developed for long video generation. The TE-branch is a novel TEmporal Swin Attention block that focuses on temporal correlations between adjacent tokens and medium-range tokens. The MATE block addresses the adjacency preservation issue of Mamba and improves the consistency of generated videos significantly. Experimental results show that LinGen outperforms DiT (with a 75.6% win rate) in video quality with up to 15$\\times$ (11.5$\\times$) FLOPs (latency) reduction. Furthermore, both automatic metrics and human evaluation demonstrate our LinGen-4B yields comparable video quality to state-of-the-art models (with a 50.5%, 52.1%, 49.1% win rate with respect to Gen-3, LumaLabs, and Kling, respectively). This paves the way to hour-length movie generation and real-time interactive video generation. We provide 68s video generation results and more examples in our project website: https://lineargen.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2412.09856v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09856v1",
    "published_date": "2024-12-13",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MSC: Multi-Scale Spatio-Temporal Causal Attention for Autoregressive Video Diffusion",
    "authors": [
      "Xunnong Xu",
      "Mengying Cao"
    ],
    "abstract": "Diffusion transformers enable flexible generative modeling for video. However, it is still technically challenging and computationally expensive to generate high-resolution videos with rich semantics and complex motion. Similar to languages, video data are also auto-regressive by nature, so it is counter-intuitive to use attention mechanism with bi-directional dependency in the model. Here we propose a Multi-Scale Causal (MSC) framework to address these problems. Specifically, we introduce multiple resolutions in the spatial dimension and high-low frequencies in the temporal dimension to realize efficient attention calculation. Furthermore, attention blocks on multiple scales are combined in a controlled way to allow causal conditioning on noisy image frames for diffusion training, based on the idea that noise destroys information at different rates on different resolutions. We theoretically show that our approach can greatly reduce the computational complexity and enhance the efficiency of training. The causal attention diffusion framework can also be used for auto-regressive long video generation, without violating the natural order of frame sequences.",
    "arxiv_url": "http://arxiv.org/abs/2412.09828v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09828v1",
    "published_date": "2024-12-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG",
    "authors": [
      "Kavana Venkatesh",
      "Yusuf Dalva",
      "Ismini Lourentzou",
      "Pinar Yanardag"
    ],
    "abstract": "We introduce a novel approach to enhance the capabilities of text-to-image models by incorporating a graph-based RAG. Our system dynamically retrieves detailed character information and relational data from the knowledge graph, enabling the generation of visually accurate and contextually rich images. This capability significantly improves upon the limitations of existing T2I models, which often struggle with the accurate depiction of complex or culturally specific subjects due to dataset constraints. Furthermore, we propose a novel self-correcting mechanism for text-to-image models to ensure consistency and fidelity in visual outputs, leveraging the rich context from the graph to guide corrections. Our qualitative and quantitative experiments demonstrate that Context Canvas significantly enhances the capabilities of popular models such as Flux, Stable Diffusion, and DALL-E, and improves the functionality of ControlNet for fine-grained image editing tasks. To our knowledge, Context Canvas represents the first application of graph-based RAG in enhancing T2I models, representing a significant advancement for producing high-fidelity, context-aware multi-faceted images.",
    "arxiv_url": "http://arxiv.org/abs/2412.09614v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09614v1",
    "published_date": "2024-12-12",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image editing",
      "Control"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers",
    "authors": [
      "Yusuf Dalva",
      "Kavana Venkatesh",
      "Pinar Yanardag"
    ],
    "abstract": "Rectified flow models have emerged as a dominant approach in image generation, showcasing impressive capabilities in high-quality image synthesis. However, despite their effectiveness in visual generation, rectified flow models often struggle with disentangled editing of images. This limitation prevents the ability to perform precise, attribute-specific modifications without affecting unrelated aspects of the image. In this paper, we introduce FluxSpace, a domain-agnostic image editing method leveraging a representation space with the ability to control the semantics of images generated by rectified flow transformers, such as Flux. By leveraging the representations learned by the transformer blocks within the rectified flow models, we propose a set of semantically interpretable representations that enable a wide range of image editing tasks, from fine-grained image editing to artistic creation. This work offers a scalable and effective image editing approach, along with its disentanglement capabilities.",
    "arxiv_url": "http://arxiv.org/abs/2412.09611v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09611v1",
    "published_date": "2024-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "image editing",
      "Control",
      "image generation",
      "rectified flow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multimodal Latent Language Modeling with Next-Token Diffusion",
    "authors": [
      "Yutao Sun",
      "Hangbo Bao",
      "Wenhui Wang",
      "Zhiliang Peng",
      "Li Dong",
      "Shaohan Huang",
      "Jianyong Wang",
      "Furu Wei"
    ],
    "abstract": "Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop $\\sigma$-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models.",
    "arxiv_url": "http://arxiv.org/abs/2412.08635v1",
    "pdf_url": "http://arxiv.org/pdf/2412.08635v1",
    "published_date": "2024-12-11",
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
    "authors": [
      "Tianwei Yin",
      "Qiang Zhang",
      "Richard Zhang",
      "William T. Freeman",
      "Fredo Durand",
      "Eli Shechtman",
      "Xun Huang"
    ],
    "abstract": "Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to an autoregressive transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher's ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all previous video generation models. It enables fast streaming generation of high-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner. We will release the code based on an open-source model in the future.",
    "arxiv_url": "http://arxiv.org/abs/2412.07772v2",
    "pdf_url": "http://arxiv.org/pdf/2412.07772v2",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STIV: Scalable Text and Image Conditioned Video Generation",
    "authors": [
      "Zongyu Lin",
      "Wei Liu",
      "Chen Chen",
      "Jiasen Lu",
      "Wenze Hu",
      "Tsu-Jui Fu",
      "Jesse Allardice",
      "Zhengfeng Lai",
      "Liangchen Song",
      "Bowen Zhang",
      "Cha Chen",
      "Yiran Fei",
      "Yifan Jiang",
      "Lezhi Li",
      "Yizhou Sun",
      "Kai-Wei Chang",
      "Yinfei Yang"
    ],
    "abstract": "The field of video generation has made remarkable advancements, yet there remains a pressing need for a clear, systematic recipe that can guide the development of robust and scalable models. In this work, we present a comprehensive study that systematically explores the interplay of model architectures, training recipes, and data curation strategies, culminating in a simple and scalable text-image-conditioned video generation method, named STIV. Our framework integrates image condition into a Diffusion Transformer (DiT) through frame replacement, while incorporating text conditioning via a joint image-text conditional classifier-free guidance. This design enables STIV to perform both text-to-video (T2V) and text-image-to-video (TI2V) tasks simultaneously. Additionally, STIV can be easily extended to various applications, such as video prediction, frame interpolation, multi-view generation, and long video generation, etc. With comprehensive ablation studies on T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simple design. An 8.7B model with 512 resolution achieves 83.1 on VBench T2V, surpassing both leading open and closed-source models like CogVideoX-5B, Pika, Kling, and Gen-3. The same-sized model also achieves a state-of-the-art result of 90.1 on VBench I2V task at 512 resolution. By providing a transparent and extensible recipe for building cutting-edge video generation models, we aim to empower future research and accelerate progress toward more versatile and reliable video generation solutions.",
    "arxiv_url": "http://arxiv.org/abs/2412.07730v1",
    "pdf_url": "http://arxiv.org/pdf/2412.07730v1",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer",
    "authors": [
      "Jinyi Hu",
      "Shengding Hu",
      "Yuxuan Song",
      "Yufei Huang",
      "Mingxuan Wang",
      "Hao Zhou",
      "Zhiyuan Liu",
      "Wei-Ying Ma",
      "Maosong Sun"
    ],
    "abstract": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion Transformer, that innovatively combines autoregressive and diffusion paradigms for modeling continuous visual information. By introducing a block-wise autoregressive unit, ACDiT offers a flexible interpolation between token-wise autoregression and full-sequence diffusion, bypassing the limitations of discrete tokenization. The generation of each block is formulated as a conditional diffusion process, conditioned on prior blocks. ACDiT is easy to implement, as simple as creating a Skip-Causal Attention Mask (SCAM) on standard diffusion transformer during training. During inference, the process iterates between diffusion denoising and autoregressive decoding that can make full use of KV-Cache. We show that ACDiT performs best among all autoregressive baselines under similar model scales on image and video generation tasks. We also demonstrate that benefiting from autoregressive modeling, pretrained ACDiT can be transferred in visual understanding tasks despite being trained with the diffusion objective. The analysis of the trade-off between autoregressive modeling and diffusion demonstrates the potential of ACDiT to be used in long-horizon visual generation tasks. We hope that ACDiT offers a novel perspective on visual autoregressive generation and unlocks new avenues for unified models.",
    "arxiv_url": "http://arxiv.org/abs/2412.07720v2",
    "pdf_url": "http://arxiv.org/pdf/2412.07720v2",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlexDiT: Dynamic Token Density Control for Diffusion Transformer",
    "authors": [
      "Shuning Chang",
      "Pichao Wang",
      "Jiasheng Tang",
      "Yi Yang"
    ],
    "abstract": "Diffusion Transformers (DiT) deliver impressive generative performance but face prohibitive computational demands due to both the quadratic complexity of token-based self-attention and the need for extensive sampling steps. While recent research has focused on accelerating sampling, the structural inefficiencies of DiT remain underexplored. We propose FlexDiT, a framework that dynamically adapts token density across both spatial and temporal dimensions to achieve computational efficiency without compromising generation quality. Spatially, FlexDiT employs a three-segment architecture that allocates token density based on feature requirements at each layer: Poolingformer in the bottom layers for efficient global feature extraction, Sparse-Dense Token Modules (SDTM) in the middle layers to balance global context with local detail, and dense tokens in the top layers to refine high-frequency details. Temporally, FlexDiT dynamically modulates token density across denoising stages, progressively increasing token count as finer details emerge in later timesteps. This synergy between FlexDiT's spatially adaptive architecture and its temporal pruning strategy enables a unified framework that balances efficiency and fidelity throughout the generation process. Our experiments demonstrate FlexDiT's effectiveness, achieving a 55% reduction in FLOPs and a 175% improvement in inference speed on DiT-XL with only a 0.09 increase in FID score on 512$\\times$512 ImageNet images, a 56% reduction in FLOPs across video generation datasets including FaceForensics, SkyTimelapse, UCF101, and Taichi-HD, and a 69% improvement in inference speed on PixArt-$\\alpha$ on text-to-image generation task with a 0.24 FID score decrease. FlexDiT provides a scalable solution for high-quality diffusion-based generation compatible with further sampling optimization techniques.",
    "arxiv_url": "http://arxiv.org/abs/2412.06028v1",
    "pdf_url": "http://arxiv.org/pdf/2412.06028v1",
    "published_date": "2024-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "video generation",
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MotionStone: Decoupled Motion Intensity Modulation with Diffusion Transformer for Image-to-Video Generation",
    "authors": [
      "Shuwei Shi",
      "Biao Gong",
      "Xi Chen",
      "Dandan Zheng",
      "Shuai Tan",
      "Zizheng Yang",
      "Yuyuan Li",
      "Jingwen He",
      "Kecheng Zheng",
      "Jingdong Chen",
      "Ming Yang",
      "Yinqiang Zheng"
    ],
    "abstract": "The image-to-video (I2V) generation is conditioned on the static image, which has been enhanced recently by the motion intensity as an additional control signal. These motion-aware models are appealing to generate diverse motion patterns, yet there lacks a reliable motion estimator for training such models on large-scale video set in the wild. Traditional metrics, e.g., SSIM or optical flow, are hard to generalize to arbitrary videos, while, it is very tough for human annotators to label the abstract motion intensity neither. Furthermore, the motion intensity shall reveal both local object motion and global camera movement, which has not been studied before. This paper addresses the challenge with a new motion estimator, capable of measuring the decoupled motion intensities of objects and cameras in video. We leverage the contrastive learning on randomly paired videos and distinguish the video with greater motion intensity. Such a paradigm is friendly for annotation and easy to scale up to achieve stable performance on motion estimation. We then present a new I2V model, named MotionStone, developed with the decoupled motion estimator. Experimental results demonstrate the stability of the proposed motion estimator and the state-of-the-art performance of MotionStone on I2V generation. These advantages warrant the decoupled motion estimator to serve as a general plug-in enhancer for both data processing and video generation training.",
    "arxiv_url": "http://arxiv.org/abs/2412.05848v1",
    "pdf_url": "http://arxiv.org/pdf/2412.05848v1",
    "published_date": "2024-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Self-Guidance: Boosting Flow and Diffusion Generation on Their Own",
    "authors": [
      "Tiancheng Li",
      "Weijian Luo",
      "Zhiyang Chen",
      "Liyuan Ma",
      "Guo-Jun Qi"
    ],
    "abstract": "Proper guidance strategies are essential to achieve high-quality generation results without retraining diffusion and flow-based text-to-image models. Existing guidance either requires specific training or strong inductive biases of diffusion model networks, potentially limiting their applications. Motivated by the observation that artifact outliers can be detected by a significant decline in the density from a noisier to a cleaner noise level, we propose Self-Guidance (SG), which improves the image quality by suppressing the generation of low-quality samples. SG only relies on the sampling probabilities of its own diffusion model at different noise levels with no need of any guidance-specific training. This makes it flexible to be used in a plug-and-play manner with other sampling algorithms, maximizing its potential to achieve competitive performances in many generative tasks. We conduct experiments on text-to-image and text-to-video generation with different architectures, including UNet and transformer models. With open-sourced diffusion models such as Stable Diffusion 3.5 and FLUX, Self-Guidance surpasses existing algorithms on multiple metrics, including both FID and Human Preference Score. Moreover, we find that SG has a surprisingly positive effect on the generation of physiologically correct human body structures such as hands, faces, and arms, showing its ability of eliminating human body artifacts with minimal efforts. We will release our code along with this paper.",
    "arxiv_url": "http://arxiv.org/abs/2412.05827v2",
    "pdf_url": "http://arxiv.org/pdf/2412.05827v2",
    "published_date": "2024-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Language-Guided Image Tokenization for Generation",
    "authors": [
      "Kaiwen Zha",
      "Lijun Yu",
      "Alireza Fathi",
      "David A. Ross",
      "Cordelia Schmid",
      "Dina Katabi",
      "Xiuye Gu"
    ],
    "abstract": "Image tokenization, the process of transforming raw image pixels into a compact low-dimensional latent representation, has proven crucial for scalable and efficient image generation. However, mainstream image tokenization methods generally have limited compression rates, making high-resolution image generation computationally expensive. To address this challenge, we propose to leverage language for efficient image tokenization, and we call our method Text-Conditioned Image Tokenization (TexTok). TexTok is a simple yet effective tokenization framework that leverages language to provide a compact, high-level semantic representation. By conditioning the tokenization process on descriptive text captions, TexTok simplifies semantic learning, allowing more learning capacity and token space to be allocated to capture fine-grained visual details, leading to enhanced reconstruction quality and higher compression rates. Compared to the conventional tokenizer without text conditioning, TexTok achieves average reconstruction FID improvements of 29.2% and 48.1% on ImageNet-256 and -512 benchmarks respectively, across varying numbers of tokens. These tokenization improvements consistently translate to 16.3% and 34.3% average improvements in generation FID. By simply replacing the tokenizer in Diffusion Transformer (DiT) with TexTok, our system can achieve a 93.5x inference speedup while still outperforming the original DiT using only 32 tokens on ImageNet-512. TexTok with a vanilla DiT generator achieves state-of-the-art FID scores of 1.46 and 1.62 on ImageNet-256 and -512 respectively. Furthermore, we demonstrate TexTok's superiority on the text-to-image generation task, effectively utilizing the off-the-shelf text captions in tokenization. Project page is at: https://kaiwenzha.github.io/textok/.",
    "arxiv_url": "http://arxiv.org/abs/2412.05796v2",
    "pdf_url": "http://arxiv.org/pdf/2412.05796v2",
    "published_date": "2024-12-08",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mind the Time: Temporally-Controlled Multi-Event Video Generation",
    "authors": [
      "Ziyi Wu",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Ivan Skorokhodov",
      "Yuwei Fang",
      "Varnith Chordia",
      "Igor Gilitschenski",
      "Sergey Tulyakov"
    ],
    "abstract": "Real-world videos consist of sequences of events. Generating such sequences with precise temporal control is infeasible with existing video generators that rely on a single paragraph of text as input. When tasked with generating multiple events described using a single prompt, such methods often ignore some of the events or fail to arrange them in the correct order. To address this limitation, we present MinT, a multi-event video generator with temporal control. Our key insight is to bind each event to a specific period in the generated video, which allows the model to focus on one event at a time. To enable time-aware interactions between event captions and video tokens, we design a time-based positional encoding method, dubbed ReRoPE. This encoding helps to guide the cross-attention operation. By fine-tuning a pre-trained video diffusion transformer on temporally grounded data, our approach produces coherent videos with smoothly connected events. For the first time in the literature, our model offers control over the timing of events in generated videos. Extensive experiments demonstrate that MinT outperforms existing commercial and open-source models by a large margin.",
    "arxiv_url": "http://arxiv.org/abs/2412.05263v2",
    "pdf_url": "http://arxiv.org/pdf/2412.05263v2",
    "published_date": "2024-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative Layout-to-Image Generation",
    "authors": [
      "Hui Zhang",
      "Dexiang Hong",
      "Yitong Wang",
      "Jie Shao",
      "Xinglong Wu",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "abstract": "Diffusion models have been recognized for their ability to generate images that are not only visually appealing but also of high artistic quality. As a result, Layout-to-Image (L2I) generation has been proposed to leverage region-specific positions and descriptions to enable more precise and controllable generation. However, previous methods primarily focus on UNet-based models (e.g., SD1.5 and SDXL), and limited effort has explored Multimodal Diffusion Transformers (MM-DiTs), which have demonstrated powerful image generation capabilities. Enabling MM-DiT for layout-to-image generation seems straightforward but is challenging due to the complexity of how layout is introduced, integrated, and balanced among multiple modalities. To this end, we explore various network variants to efficiently incorporate layout guidance into MM-DiT, and ultimately present SiamLayout. To Inherit the advantages of MM-DiT, we use a separate set of network weights to process the layout, treating it as equally important as the image and text modalities. Meanwhile, to alleviate the competition among modalities, we decouple the image-layout interaction into a siamese branch alongside the image-text one and fuse them in the later stage. Moreover, we contribute a large-scale layout dataset, named LayoutSAM, which includes 2.7 million image-text pairs and 10.7 million entities. Each entity is annotated with a bounding box and a detailed description. We further construct the LayoutSAM-Eval benchmark as a comprehensive tool for evaluating the L2I generation quality. Finally, we introduce the Layout Designer, which taps into the potential of large language models in layout planning, transforming them into experts in layout generation and optimization. Our code, model, and dataset will be available at https://creatilayout.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2412.03859v2",
    "pdf_url": "http://arxiv.org/pdf/2412.03859v2",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image generation",
      "diffusion transformer",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Navigation World Models",
    "authors": [
      "Amir Bar",
      "Gaoyue Zhou",
      "Danny Tran",
      "Trevor Darrell",
      "Yann LeCun"
    ],
    "abstract": "Navigation is a fundamental skill of agents with visual-motor capabilities. We introduce a Navigation World Model (NWM), a controllable video generation model that predicts future visual observations based on past observations and navigation actions. To capture complex environment dynamics, NWM employs a Conditional Diffusion Transformer (CDiT), trained on a diverse collection of egocentric videos of both human and robotic agents, and scaled up to 1 billion parameters. In familiar environments, NWM can plan navigation trajectories by simulating them and evaluating whether they achieve the desired goal. Unlike supervised navigation policies with fixed behavior, NWM can dynamically incorporate constraints during planning. Experiments demonstrate its effectiveness in planning trajectories from scratch or by ranking trajectories sampled from an external policy. Furthermore, NWM leverages its learned visual priors to imagine trajectories in unfamiliar environments from a single input image, making it a flexible and powerful tool for next-generation navigation systems.",
    "arxiv_url": "http://arxiv.org/abs/2412.03572v2",
    "pdf_url": "http://arxiv.org/pdf/2412.03572v2",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seeing Beyond Views: Multi-View Driving Scene Video Generation with Holistic Attention",
    "authors": [
      "Hannan Lu",
      "Xiaohe Wu",
      "Shudong Wang",
      "Xiameng Qin",
      "Xinyu Zhang",
      "Junyu Han",
      "Wangmeng Zuo",
      "Ji Tao"
    ],
    "abstract": "Generating multi-view videos for autonomous driving training has recently gained much attention, with the challenge of addressing both cross-view and cross-frame consistency. Existing methods typically apply decoupled attention mechanisms for spatial, temporal, and view dimensions. However, these approaches often struggle to maintain consistency across dimensions, particularly when handling fast-moving objects that appear at different times and viewpoints. In this paper, we present CogDriving, a novel network designed for synthesizing high-quality multi-view driving videos. CogDriving leverages a Diffusion Transformer architecture with holistic-4D attention modules, enabling simultaneous associations across the spatial, temporal, and viewpoint dimensions. We also propose a lightweight controller tailored for CogDriving, i.e., Micro-Controller, which uses only 1.1% of the parameters of the standard ControlNet, enabling precise control over Bird's-Eye-View layouts. To enhance the generation of object instances crucial for autonomous driving, we propose a re-weighted learning objective, dynamically adjusting the learning weights for object instances during training. CogDriving demonstrates strong performance on the nuScenes validation set, achieving an FVD score of 37.8, highlighting its ability to generate realistic driving videos. The project can be found at https://luhannan.github.io/CogDrivingPage/.",
    "arxiv_url": "http://arxiv.org/abs/2412.03520v2",
    "pdf_url": "http://arxiv.org/pdf/2412.03520v2",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MaterialPicker: Multi-Modal Material Generation with Diffusion Transformers",
    "authors": [
      "Xiaohe Ma",
      "Valentin Deschaintre",
      "Miloš Hašan",
      "Fujun Luan",
      "Kun Zhou",
      "Hongzhi Wu",
      "Yiwei Hu"
    ],
    "abstract": "High-quality material generation is key for virtual environment authoring and inverse rendering. We propose MaterialPicker, a multi-modal material generator leveraging a Diffusion Transformer (DiT) architecture, improving and simplifying the creation of high-quality materials from text prompts and/or photographs. Our method can generate a material based on an image crop of a material sample, even if the captured surface is distorted, viewed at an angle or partially occluded, as is often the case in photographs of natural scenes. We further allow the user to specify a text prompt to provide additional guidance for the generation. We finetune a pre-trained DiT-based video generator into a material generator, where each material map is treated as a frame in a video sequence. We evaluate our approach both quantitatively and qualitatively and show that it enables more diverse material generation and better distortion correction than previous work.",
    "arxiv_url": "http://arxiv.org/abs/2412.03225v2",
    "pdf_url": "http://arxiv.org/pdf/2412.03225v2",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Panoptic Diffusion Models: co-generation of images and segmentation maps",
    "authors": [
      "Yinghan Long",
      "Kaushik Roy"
    ],
    "abstract": "Recently, diffusion models have demonstrated impressive capabilities in text-guided and image-conditioned image generation. However, existing diffusion models cannot simultaneously generate an image and a panoptic segmentation of objects and stuff from the prompt. Incorporating an inherent understanding of shapes and scene layouts can improve the creativity and realism of diffusion models. To address this limitation, we present Panoptic Diffusion Model (PDM), the first model designed to generate both images and panoptic segmentation maps concurrently. PDM bridges the gap between image and text by constructing segmentation layouts that provide detailed, built-in guidance throughout the generation process. This ensures the inclusion of categories mentioned in text prompts and enriches the diversity of segments within the background. We demonstrate the effectiveness of PDM across two architectures: a unified diffusion transformer and a two-stream transformer with a pretrained backbone. We propose a Multi-Scale Patching mechanism to generate high-resolution segmentation maps. Additionally, when ground-truth maps are available, PDM can function as a text-guided image-to-image generation model. Finally, we propose a novel metric for evaluating the quality of generated maps and show that PDM achieves state-of-the-art results in image generation with implicit scene control.",
    "arxiv_url": "http://arxiv.org/abs/2412.02929v2",
    "pdf_url": "http://arxiv.org/pdf/2412.02929v2",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SyncFlow: Toward Temporally Aligned Joint Audio-Video Generation from Text",
    "authors": [
      "Haohe Liu",
      "Gael Le Lan",
      "Xinhao Mei",
      "Zhaoheng Ni",
      "Anurag Kumar",
      "Varun Nagaraja",
      "Wenwu Wang",
      "Mark D. Plumbley",
      "Yangyang Shi",
      "Vikas Chandra"
    ],
    "abstract": "Video and audio are closely correlated modalities that humans naturally perceive together. While recent advancements have enabled the generation of audio or video from text, producing both modalities simultaneously still typically relies on either a cascaded process or multi-modal contrastive encoders. These approaches, however, often lead to suboptimal results due to inherent information losses during inference and conditioning. In this paper, we introduce SyncFlow, a system that is capable of simultaneously generating temporally synchronized audio and video from text. The core of SyncFlow is the proposed dual-diffusion-transformer (d-DiT) architecture, which enables joint video and audio modelling with proper information fusion. To efficiently manage the computational cost of joint audio and video modelling, SyncFlow utilizes a multi-stage training strategy that separates video and audio learning before joint fine-tuning. Our empirical evaluations demonstrate that SyncFlow produces audio and video outputs that are more correlated than baseline methods with significantly enhanced audio quality and audio-visual correspondence. Moreover, we demonstrate strong zero-shot capabilities of SyncFlow, including zero-shot video-to-audio generation and adaptation to novel video resolutions without further training.",
    "arxiv_url": "http://arxiv.org/abs/2412.15220v1",
    "pdf_url": "http://arxiv.org/pdf/2412.15220v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis",
    "authors": [
      "Yu Yuan",
      "Xijun Wang",
      "Yichen Sheng",
      "Prateek Chennuri",
      "Xingguang Zhang",
      "Stanley Chan"
    ],
    "abstract": "Image generation today can produce somewhat realistic images from text prompts. However, if one asks the generator to synthesize a specific camera setting such as creating different fields of view using a 24mm lens versus a 70mm lens, the generator will not be able to interpret and generate scene-consistent images. This limitation not only hinders the adoption of generative tools in professional photography but also highlights the broader challenge of aligning data-driven models with real-world physical settings. In this paper, we introduce Generative Photography, a framework that allows controlling camera intrinsic settings during content generation. The core innovation of this work are the concepts of Dimensionality Lifting and Differential Camera Intrinsics Learning, enabling smooth and consistent transitions across different camera settings. Experimental results show that our method produces significantly more scene-consistent photorealistic images than state-of-the-art models such as Stable Diffusion 3 and FLUX. Our code and additional results are available at https://generative-photography.github.io/project.",
    "arxiv_url": "http://arxiv.org/abs/2412.02168v3",
    "pdf_url": "http://arxiv.org/pdf/2412.02168v3",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation",
      "Control"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "World-consistent Video Diffusion with Explicit 3D Modeling",
    "authors": [
      "Qihang Zhang",
      "Shuangfei Zhai",
      "Miguel Angel Bautista",
      "Kevin Miao",
      "Alexander Toshev",
      "Joshua Susskind",
      "Jiatao Gu"
    ],
    "abstract": "Recent advancements in diffusion models have set new benchmarks in image and video generation, enabling realistic visual synthesis across single- and multi-frame contexts. However, these models still struggle with efficiently and explicitly generating 3D-consistent content. To address this, we propose World-consistent Video Diffusion (WVD), a novel framework that incorporates explicit 3D supervision using XYZ images, which encode global 3D coordinates for each image pixel. More specifically, we train a diffusion transformer to learn the joint distribution of RGB and XYZ frames. This approach supports multi-task adaptability via a flexible inpainting strategy. For example, WVD can estimate XYZ frames from ground-truth RGB or generate novel RGB frames using XYZ projections along a specified camera trajectory. In doing so, WVD unifies tasks like single-image-to-3D generation, multi-view stereo, and camera-controlled video generation. Our approach demonstrates competitive performance across multiple benchmarks, providing a scalable solution for 3D-consistent video and image generation with a single pretrained model.",
    "arxiv_url": "http://arxiv.org/abs/2412.01821v1",
    "pdf_url": "http://arxiv.org/pdf/2412.01821v1",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image generation",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CPA: Camera-pose-awareness Diffusion Transformer for Video Generation",
    "authors": [
      "Yuelei Wang",
      "Jian Zhang",
      "Pengtao Jiang",
      "Hao Zhang",
      "Jinwei Chen",
      "Bo Li"
    ],
    "abstract": "Despite the significant advancements made by Diffusion Transformer (DiT)-based methods in video generation, there remains a notable gap with controllable camera pose perspectives. Existing works such as OpenSora do NOT adhere precisely to anticipated trajectories and physical interactions, thereby limiting the flexibility in downstream applications. To alleviate this issue, we introduce CPA, a unified camera-pose-awareness text-to-video generation approach that elaborates the camera movement and integrates the textual, visual, and spatial conditions. Specifically, we deploy the Sparse Motion Encoding (SME) module to transform camera pose information into a spatial-temporal embedding and activate the Temporal Attention Injection (TAI) module to inject motion patches into each ST-DiT block. Our plug-in architecture accommodates the original DiT parameters, facilitating diverse types of camera poses and flexible object movement. Extensive qualitative and quantitative experiments demonstrate that our method outperforms LDM-based methods for long video generation while achieving optimal performance in trajectory consistency and object consistency.",
    "arxiv_url": "http://arxiv.org/abs/2412.01429v1",
    "pdf_url": "http://arxiv.org/pdf/2412.01429v1",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TinyFusion: Diffusion Transformers Learned Shallow",
    "authors": [
      "Gongfan Fang",
      "Kunjun Li",
      "Xinyin Ma",
      "Xinchao Wang"
    ],
    "abstract": "Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, a depth pruning method designed to remove redundant layers from diffusion transformers via end-to-end learning. The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre-training cost, achieving a 2$\\times$ speedup with an FID score of 2.86, outperforming competitors with comparable efficiency. Code is available at https://github.com/VainF/TinyFusion.",
    "arxiv_url": "http://arxiv.org/abs/2412.01199v1",
    "pdf_url": "http://arxiv.org/pdf/2412.01199v1",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "https://github.com/VainF/TinyFusion",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Video Diffusion Transformer",
    "authors": [
      "Jiahao Cui",
      "Hui Li",
      "Yun Zhan",
      "Hanlin Shang",
      "Kaihui Cheng",
      "Yuqi Ma",
      "Shan Mu",
      "Hang Zhou",
      "Jingdong Wang",
      "Siyu Zhu"
    ],
    "abstract": "Existing methodologies for animating portrait images face significant challenges, particularly in handling non-frontal perspectives, rendering dynamic objects around the portrait, and generating immersive, realistic backgrounds. In this paper, we introduce the first application of a pretrained transformer-based video generative model that demonstrates strong generalization capabilities and generates highly dynamic, realistic videos for portrait animation, effectively addressing these challenges. The adoption of a new video backbone model makes previous U-Net-based methods for identity maintenance, audio conditioning, and video extrapolation inapplicable. To address this limitation, we design an identity reference network consisting of a causal 3D VAE combined with a stacked series of transformer layers, ensuring consistent facial identity across video sequences. Additionally, we investigate various speech audio conditioning and motion frame mechanisms to enable the generation of continuous video driven by speech audio. Our method is validated through experiments on benchmark and newly proposed wild datasets, demonstrating substantial improvements over prior methods in generating realistic portraits characterized by diverse orientations within dynamic and immersive scenes. Further visualizations and the source code are available at: https://fudan-generative-vision.github.io/hallo3/.",
    "arxiv_url": "http://arxiv.org/abs/2412.00733v4",
    "pdf_url": "http://arxiv.org/pdf/2412.00733v4",
    "published_date": "2024-12-01",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AMO Sampler: Enhancing Text Rendering with Overshooting",
    "authors": [
      "Xixi Hu",
      "Keyang Xu",
      "Bo Liu",
      "Qiang Liu",
      "Hongliang Fei"
    ],
    "abstract": "Achieving precise alignment between textual instructions and generated images in text-to-image generation is a significant challenge, particularly in rendering written text within images. Sate-of-the-art models like Stable Diffusion 3 (SD3), Flux, and AuraFlow still struggle with accurate text depiction, resulting in misspelled or inconsistent text. We introduce a training-free method with minimal computational overhead that significantly enhances text rendering quality. Specifically, we introduce an overshooting sampler for pretrained rectified flow (RF) models, by alternating between over-simulating the learned ordinary differential equation (ODE) and reintroducing noise. Compared to the Euler sampler, the overshooting sampler effectively introduces an extra Langevin dynamics term that can help correct the compounding error from successive Euler steps and therefore improve the text rendering. However, when the overshooting strength is high, we observe over-smoothing artifacts on the generated images. To address this issue, we propose an Attention Modulated Overshooting sampler (AMO), which adaptively controls the strength of overshooting for each image patch according to their attention score with the text content. AMO demonstrates a 32.3% and 35.9% improvement in text rendering accuracy on SD3 and Flux without compromising overall image quality or increasing inference cost. Code available at: https://github.com/hxixixh/amo-release.",
    "arxiv_url": "http://arxiv.org/abs/2411.19415v2",
    "pdf_url": "http://arxiv.org/pdf/2411.19415v2",
    "published_date": "2024-11-28",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "https://github.com/hxixixh/amo-release",
    "keywords": [
      "FLUX",
      "text-to-image",
      "Control",
      "image generation",
      "rectified flow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation",
    "authors": [
      "Hui Li",
      "Mingwang Xu",
      "Yun Zhan",
      "Shan Mu",
      "Jiaye Li",
      "Kaihui Cheng",
      "Yuxuan Chen",
      "Tan Chen",
      "Mao Ye",
      "Jingdong Wang",
      "Siyu Zhu"
    ],
    "abstract": "Recent advancements in visual generation technologies have markedly increased the scale and availability of video datasets, which are crucial for training effective video generation models. However, a significant lack of high-quality, human-centric video datasets presents a challenge to progress in this field. To bridge this gap, we introduce OpenHumanVid, a large-scale and high-quality human-centric video dataset characterized by precise and detailed captions that encompass both human appearance and motion states, along with supplementary human motion conditions, including skeleton sequences and speech audio. To validate the efficacy of this dataset and the associated training strategies, we propose an extension of existing classical diffusion transformer architectures and conduct further pretraining of our models on the proposed dataset. Our findings yield two critical insights: First, the incorporation of a large-scale, high-quality dataset substantially enhances evaluation metrics for generated human videos while preserving performance in general video generation tasks. Second, the effective alignment of text with human appearance, human motion, and facial motion is essential for producing high-quality video outputs. Based on these insights and corresponding methodologies, the straightforward extended network trained on the proposed dataset demonstrates an obvious improvement in the generation of human-centric videos. Project page https://fudan-generative-vision.github.io/OpenHumanVid",
    "arxiv_url": "http://arxiv.org/abs/2412.00115v3",
    "pdf_url": "http://arxiv.org/pdf/2412.00115v3",
    "published_date": "2024-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers",
    "authors": [
      "Sherwin Bahmani",
      "Ivan Skorokhodov",
      "Guocheng Qian",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Andrea Tagliasacchi",
      "David B. Lindell",
      "Sergey Tulyakov"
    ],
    "abstract": "Numerous works have recently integrated 3D camera control into foundational text-to-video models, but the resulting camera control is often imprecise, and video generation quality suffers. In this work, we analyze camera motion from a first principles perspective, uncovering insights that enable precise 3D camera manipulation without compromising synthesis quality. First, we determine that motion induced by camera movements in videos is low-frequency in nature. This motivates us to adjust train and test pose conditioning schedules, accelerating training convergence while improving visual and motion quality. Then, by probing the representations of an unconditional video diffusion transformer, we observe that they implicitly perform camera pose estimation under the hood, and only a sub-portion of their layers contain the camera information. This suggested us to limit the injection of camera conditioning to a subset of the architecture to prevent interference with other video features, leading to a 4x reduction of training parameters, improved training speed, and 10% higher visual quality. Finally, we complement the typical dataset for camera control learning with a curated dataset of 20K diverse, dynamic videos with stationary cameras. This helps the model distinguish between camera and scene motion and improves the dynamics of generated pose-conditioned videos. We compound these findings to design the Advanced 3D Camera Control (AC3D) architecture, the new state-of-the-art model for generative video modeling with camera control.",
    "arxiv_url": "http://arxiv.org/abs/2411.18673v4",
    "pdf_url": "http://arxiv.org/pdf/2411.18673v4",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Prediction with Action: Visual Policy Learning via Joint Denoising Process",
    "authors": [
      "Yanjiang Guo",
      "Yucheng Hu",
      "Jianke Zhang",
      "Yen-Jen Wang",
      "Xiaoyu Chen",
      "Chaochao Lu",
      "Jianyu Chen"
    ],
    "abstract": "Diffusion models have demonstrated remarkable capabilities in image generation tasks, including image editing and video creation, representing a good understanding of the physical world. On the other line, diffusion models have also shown promise in robotic control tasks by denoising actions, known as diffusion policy. Although the diffusion generative model and diffusion policy exhibit distinct capabilities--image prediction and robotic action, respectively--they technically follow a similar denoising process. In robotic tasks, the ability to predict future images and generate actions is highly correlated since they share the same underlying dynamics of the physical world. Building on this insight, we introduce PAD, a novel visual policy learning framework that unifies image Prediction and robot Action within a joint Denoising process. Specifically, PAD utilizes Diffusion Transformers (DiT) to seamlessly integrate images and robot states, enabling the simultaneous prediction of future images and robot actions. Additionally, PAD supports co-training on both robotic demonstrations and large-scale video datasets and can be easily extended to other robotic modalities, such as depth images. PAD outperforms previous methods, achieving a significant 26.3% relative improvement on the full Metaworld benchmark, by utilizing a single text-conditioned visual policy within a data-efficient imitation learning setting. Furthermore, PAD demonstrates superior generalization to unseen tasks in real-world robot manipulation settings with 28.0% success rate increase compared to the strongest baseline. Project page at https://sites.google.com/view/pad-paper",
    "arxiv_url": "http://arxiv.org/abs/2411.18179v1",
    "pdf_url": "http://arxiv.org/pdf/2411.18179v1",
    "published_date": "2024-11-27",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image editing",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Type-R: Automatically Retouching Typos for Text-to-Image Generation",
    "authors": [
      "Wataru Shimoda",
      "Naoto Inoue",
      "Daichi Haraguchi",
      "Hayato Mitani",
      "Seiichi Uchida",
      "Kota Yamaguchi"
    ],
    "abstract": "While recent text-to-image models can generate photorealistic images from text prompts that reflect detailed instructions, they still face significant challenges in accurately rendering words in the image. In this paper, we propose to retouch erroneous text renderings in the post-processing pipeline. Our approach, called Type-R, identifies typographical errors in the generated image, erases the erroneous text, regenerates text boxes for missing words, and finally corrects typos in the rendered words. Through extensive experiments, we show that Type-R, in combination with the latest text-to-image models such as Stable Diffusion or Flux, achieves the highest text rendering accuracy while maintaining image quality and also outperforms text-focused generation baselines in terms of balancing text accuracy and image quality.",
    "arxiv_url": "http://arxiv.org/abs/2411.18159v2",
    "pdf_url": "http://arxiv.org/pdf/2411.18159v2",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Stabilized and Efficient Diffusion Transformers through Long-Skip-Connections with Spectral Constraints",
    "authors": [
      "Guanjie Chen",
      "Xinyu Zhao",
      "Yucheng Zhou",
      "Xiaoye Qu",
      "Tianlong Chen",
      "Yu Cheng"
    ],
    "abstract": "Diffusion Transformers (DiT) have emerged as a powerful architecture for image and video generation, offering superior quality and scalability. However, their practical application suffers from inherent dynamic feature instability, leading to error amplification during cached inference. Through systematic analysis, we identify the absence of long-range feature preservation mechanisms as the root cause of unstable feature propagation and perturbation sensitivity. To this end, we propose Skip-DiT, a novel DiT variant enhanced with Long-Skip-Connections (LSCs) - the key efficiency component in U-Nets. Theoretical spectral norm and visualization analysis demonstrate how LSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic feature enable an efficient statical caching mechanism that reuses deep features across timesteps while updating shallow components. Extensive experiments across image and video generation tasks demonstrate that Skip-DiT achieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2 times inference acceleration without quality loss and high fidelity to original output, outperforming existing DiT caching methods across various quantitative metrics. Our findings establish long-skip connections as critical architectural components for training stable and efficient diffusion transformers.",
    "arxiv_url": "http://arxiv.org/abs/2411.17616v3",
    "pdf_url": "http://arxiv.org/pdf/2411.17616v3",
    "published_date": "2024-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Identity-Preserving Text-to-Video Generation by Frequency Decomposition",
    "authors": [
      "Shenghai Yuan",
      "Jinfa Huang",
      "Xianyi He",
      "Yunyuan Ge",
      "Yujun Shi",
      "Liuhan Chen",
      "Jiebo Luo",
      "Li Yuan"
    ],
    "abstract": "Identity-preserving text-to-video (IPT2V) generation aims to create high-fidelity videos with consistent human identity. It is an important task in video generation but remains an open problem for generative models. This paper pushes the technical frontier of IPT2V in two directions that have not been resolved in literature: (1) A tuning-free pipeline without tedious case-by-case finetuning, and (2) A frequency-aware heuristic identity-preserving DiT-based control scheme. We propose ConsisID, a tuning-free DiT-based controllable IPT2V model to keep human identity consistent in the generated video. Inspired by prior findings in frequency analysis of diffusion transformers, it employs identity-control signals in the frequency domain, where facial features can be decomposed into low-frequency global features and high-frequency intrinsic features. First, from a low-frequency perspective, we introduce a global facial extractor, which encodes reference images and facial key points into a latent space, generating features enriched with low-frequency information. These features are then integrated into shallow layers of the network to alleviate training challenges associated with DiT. Second, from a high-frequency perspective, we design a local facial extractor to capture high-frequency details and inject them into transformer blocks, enhancing the model's ability to preserve fine-grained features. We propose a hierarchical training strategy to leverage frequency information for identity preservation, transforming a vanilla pre-trained video generation model into an IPT2V model. Extensive experiments demonstrate that our frequency-aware heuristic scheme provides an optimal control solution for DiT-based models. Thanks to this scheme, our ConsisID generates high-quality, identity-preserving videos, making strides towards more effective IPT2V. Code: https://github.com/PKU-YuanGroup/ConsisID.",
    "arxiv_url": "http://arxiv.org/abs/2411.17440v3",
    "pdf_url": "http://arxiv.org/pdf/2411.17440v3",
    "published_date": "2024-11-26",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "https://github.com/PKU-YuanGroup/ConsisID",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LetsTalk: Latent Diffusion Transformer for Talking Video Synthesis",
    "authors": [
      "Haojie Zhang",
      "Zhihao Liang",
      "Ruibo Fu",
      "Zhengqi Wen",
      "Xuefei Liu",
      "Chenxing Li",
      "Jianhua Tao",
      "Yaling Liang"
    ],
    "abstract": "Portrait image animation using audio has rapidly advanced, enabling the creation of increasingly realistic and expressive animated faces. The challenges of this multimodality-guided video generation task involve fusing various modalities while ensuring consistency in timing and portrait. We further seek to produce vivid talking heads. To address these challenges, we present LetsTalk (LatEnt Diffusion TranSformer for Talking Video Synthesis), a diffusion transformer that incorporates modular temporal and spatial attention mechanisms to merge multimodality and enhance spatial-temporal consistency. To handle multimodal conditions, we first summarize three fusion schemes, ranging from shallow to deep fusion compactness, and thoroughly explore their impact and applicability. Then we propose a suitable solution according to the modality differences of image, audio, and video generation. For portrait, we utilize a deep fusion scheme (Symbiotic Fusion) to ensure portrait consistency. For audio, we implement a shallow fusion scheme (Direct Fusion) to achieve audio-animation alignment while preserving diversity. Our extensive experiments demonstrate that our approach generates temporally coherent and realistic videos with enhanced diversity and liveliness.",
    "arxiv_url": "http://arxiv.org/abs/2411.16748v1",
    "pdf_url": "http://arxiv.org/pdf/2411.16748v1",
    "published_date": "2024-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OminiControl: Minimal and Universal Control for Diffusion Transformer",
    "authors": [
      "Zhenxiong Tan",
      "Songhua Liu",
      "Xingyi Yang",
      "Qiaochu Xue",
      "Xinchao Wang"
    ],
    "abstract": "We present OminiControl, a novel approach that rethinks how image conditions are integrated into Diffusion Transformer (DiT) architectures. Current image conditioning methods either introduce substantial parameter overhead or handle only specific control tasks effectively, limiting their practical versatility. OminiControl addresses these limitations through three key innovations: (1) a minimal architectural design that leverages the DiT's own VAE encoder and transformer blocks, requiring just 0.1% additional parameters; (2) a unified sequence processing strategy that combines condition tokens with image tokens for flexible token interactions; and (3) a dynamic position encoding mechanism that adapts to both spatially-aligned and non-aligned control tasks. Our extensive experiments show that this streamlined approach not only matches but surpasses the performance of specialized methods across multiple conditioning tasks. To overcome data limitations in subject-driven generation, we also introduce Subjects200K, a large-scale dataset of identity-consistent image pairs synthesized using DiT models themselves. This work demonstrates that effective image control can be achieved without architectural complexity, opening new possibilities for efficient and versatile image generation systems.",
    "arxiv_url": "http://arxiv.org/abs/2411.15098v5",
    "pdf_url": "http://arxiv.org/pdf/2411.15098v5",
    "published_date": "2024-11-22",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HeadRouter: A Training-free Image Editing Framework for MM-DiTs by Adaptively Routing Attention Heads",
    "authors": [
      "Yu Xu",
      "Fan Tang",
      "Juan Cao",
      "Yuxin Zhang",
      "Xiaoyu Kong",
      "Jintao Li",
      "Oliver Deussen",
      "Tong-Yee Lee"
    ],
    "abstract": "Diffusion Transformers (DiTs) have exhibited robust capabilities in image generation tasks. However, accurate text-guided image editing for multimodal DiTs (MM-DiTs) still poses a significant challenge. Unlike UNet-based structures that could utilize self/cross-attention maps for semantic editing, MM-DiTs inherently lack support for explicit and consistent incorporated text guidance, resulting in semantic misalignment between the edited results and texts. In this study, we disclose the sensitivity of different attention heads to different image semantics within MM-DiTs and introduce HeadRouter, a training-free image editing framework that edits the source image by adaptively routing the text guidance to different attention heads in MM-DiTs. Furthermore, we present a dual-token refinement module to refine text/image token representations for precise semantic guidance and accurate region expression. Experimental results on multiple benchmarks demonstrate HeadRouter's performance in terms of editing fidelity and image quality.",
    "arxiv_url": "http://arxiv.org/abs/2411.15034v1",
    "pdf_url": "http://arxiv.org/pdf/2411.15034v1",
    "published_date": "2024-11-22",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image editing",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Stable Flow: Vital Layers for Training-Free Image Editing",
    "authors": [
      "Omri Avrahami",
      "Or Patashnik",
      "Ohad Fried",
      "Egor Nemchinov",
      "Kfir Aberman",
      "Dani Lischinski",
      "Daniel Cohen-Or"
    ],
    "abstract": "Diffusion models have revolutionized the field of content synthesis and editing. Recent models have replaced the traditional UNet architecture with the Diffusion Transformer (DiT), and employed flow-matching for improved training and sampling. However, they exhibit limited generation diversity. In this work, we leverage this limitation to perform consistent image edits via selective injection of attention features. The main challenge is that, unlike the UNet-based models, DiT lacks a coarse-to-fine synthesis structure, making it unclear in which layers to perform the injection. Therefore, we propose an automatic method to identify \"vital layers\" within DiT, crucial for image formation, and demonstrate how these layers facilitate a range of controlled stable edits, from non-rigid modifications to object addition, using the same mechanism. Next, to enable real-image editing, we introduce an improved image inversion method for flow models. Finally, we evaluate our approach through qualitative and quantitative comparisons, along with a user study, and demonstrate its effectiveness across multiple applications. The project page is available at https://omriavrahami.com/stable-flow",
    "arxiv_url": "http://arxiv.org/abs/2411.14430v2",
    "pdf_url": "http://arxiv.org/pdf/2411.14430v2",
    "published_date": "2024-11-21",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image editing",
      "inversion",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TaQ-DiT: Time-aware Quantization for Diffusion Transformers",
    "authors": [
      "Xinyan Liu",
      "Huihong Shi",
      "Yang Xu",
      "Zhongfeng Wang"
    ],
    "abstract": "Transformer-based diffusion models, dubbed Diffusion Transformers (DiTs), have achieved state-of-the-art performance in image and video generation tasks. However, their large model size and slow inference speed limit their practical applications, calling for model compression methods such as quantization. Unfortunately, existing DiT quantization methods overlook (1) the impact of reconstruction and (2) the varying quantization sensitivities across different layers, which hinder their achievable performance. To tackle these issues, we propose innovative time-aware quantization for DiTs (TaQ-DiT). Specifically, (1) we observe a non-convergence issue when reconstructing weights and activations separately during quantization and introduce a joint reconstruction method to resolve this problem. (2) We discover that Post-GELU activations are particularly sensitive to quantization due to their significant variability across different denoising steps as well as extreme asymmetries and variations within each step. To address this, we propose time-variance-aware transformations to facilitate more effective quantization. Experimental results show that when quantizing DiTs' weights to 4-bit and activations to 8-bit (W4A8), our method significantly surpasses previous quantization methods.",
    "arxiv_url": "http://arxiv.org/abs/2411.14172v1",
    "pdf_url": "http://arxiv.org/pdf/2411.14172v1",
    "published_date": "2024-11-21",
    "categories": [
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PoM: Efficient Image and Video Generation with the Polynomial Mixer",
    "authors": [
      "David Picard",
      "Nicolas Dufour"
    ],
    "abstract": "Diffusion models based on Multi-Head Attention (MHA) have become ubiquitous to generate high quality images and videos. However, encoding an image or a video as a sequence of patches results in costly attention patterns, as the requirements both in terms of memory and compute grow quadratically. To alleviate this problem, we propose a drop-in replacement for MHA called the Polynomial Mixer (PoM) that has the benefit of encoding the entire sequence into an explicit state. PoM has a linear complexity with respect to the number of tokens. This explicit state also allows us to generate frames in a sequential fashion, minimizing memory and compute requirement, while still being able to train in parallel. We show the Polynomial Mixer is a universal sequence-to-sequence approximator, just like regular MHA. We adapt several Diffusion Transformers (DiT) for generating images and videos with PoM replacing MHA, and we obtain high quality samples while using less computational resources. The code is available at https://github.com/davidpicard/HoMM.",
    "arxiv_url": "http://arxiv.org/abs/2411.12663v1",
    "pdf_url": "http://arxiv.org/pdf/2411.12663v1",
    "published_date": "2024-11-19",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "https://github.com/davidpicard/HoMM",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Oscillation Inversion: Understand the structure of Large Flow Model through the Lens of Inversion Method",
    "authors": [
      "Yan Zheng",
      "Zhenxiao Liang",
      "Xiaoyan Cong",
      "Lanqing guo",
      "Yuehao Wang",
      "Peihao Wang",
      "Zhangyang Wang"
    ],
    "abstract": "We explore the oscillatory behavior observed in inversion methods applied to large-scale text-to-image diffusion models, with a focus on the \"Flux\" model. By employing a fixed-point-inspired iterative approach to invert real-world images, we observe that the solution does not achieve convergence, instead oscillating between distinct clusters. Through both toy experiments and real-world diffusion models, we demonstrate that these oscillating clusters exhibit notable semantic coherence. We offer theoretical insights, showing that this behavior arises from oscillatory dynamics in rectified flow models. Building on this understanding, we introduce a simple and fast distribution transfer technique that facilitates image enhancement, stroke-based recoloring, as well as visual prompt-guided image editing. Furthermore, we provide quantitative results demonstrating the effectiveness of our method for tasks such as image enhancement, makeup transfer, reconstruction quality, and guided sampling quality. Higher-quality examples of videos and images are available at \\href{https://yanyanzheng96.github.io/oscillation_inversion/}{this link}.",
    "arxiv_url": "http://arxiv.org/abs/2411.11135v1",
    "pdf_url": "http://arxiv.org/pdf/2411.11135v1",
    "published_date": "2024-11-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image editing",
      "inversion",
      "rectified flow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers",
    "authors": [
      "Joseph Liu",
      "Joshua Geddes",
      "Ziyu Guo",
      "Haomiao Jiang",
      "Mahesh Kumar Nandwana"
    ],
    "abstract": "Diffusion Transformers (DiT) have emerged as powerful generative models for various tasks, including image, video, and speech synthesis. However, their inference process remains computationally expensive due to the repeated evaluation of resource-intensive attention and feed-forward modules. To address this, we introduce SmoothCache, a model-agnostic inference acceleration technique for DiT architectures. SmoothCache leverages the observed high similarity between layer outputs across adjacent diffusion timesteps. By analyzing layer-wise representation errors from a small calibration set, SmoothCache adaptively caches and reuses key features during inference. Our experiments demonstrate that SmoothCache achieves 8% to 71% speed up while maintaining or even improving generation quality across diverse modalities. We showcase its effectiveness on DiT-XL for image generation, Open-Sora for text-to-video, and Stable Audio Open for text-to-audio, highlighting its potential to enable real-time applications and broaden the accessibility of powerful DiT models.",
    "arxiv_url": "http://arxiv.org/abs/2411.10510v1",
    "pdf_url": "http://arxiv.org/pdf/2411.10510v1",
    "published_date": "2024-11-15",
    "categories": [
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Latent Space Disentanglement in Diffusion Transformers Enables Precise Zero-shot Semantic Editing",
    "authors": [
      "Zitao Shuai",
      "Chenwei Wu",
      "Zhengxu Tang",
      "Bowen Song",
      "Liyue Shen"
    ],
    "abstract": "Diffusion Transformers (DiTs) have recently achieved remarkable success in text-guided image generation. In image editing, DiTs project text and image inputs to a joint latent space, from which they decode and synthesize new images. However, it remains largely unexplored how multimodal information collectively forms this joint space and how they guide the semantics of the synthesized images. In this paper, we investigate the latent space of DiT models and uncover two key properties: First, DiT's latent space is inherently semantically disentangled, where different semantic attributes can be controlled by specific editing directions. Second, consistent semantic editing requires utilizing the entire joint latent space, as neither encoded image nor text alone contains enough semantic information. We show that these editing directions can be obtained directly from text prompts, enabling precise semantic control without additional training or mask annotations. Based on these insights, we propose a simple yet effective Encode-Identify-Manipulate (EIM) framework for zero-shot fine-grained image editing. Specifically, we first encode both the given source image and the text prompt that describes the image, to obtain the joint latent embedding. Then, using our proposed Hessian Score Distillation Sampling (HSDS) method, we identify editing directions that control specific target attributes while preserving other image features. These directions are guided by text prompts and used to manipulate the latent embeddings. Moreover, we propose a new metric to quantify the disentanglement degree of the latent space of diffusion models. Extensive experiment results on our new curated benchmark dataset and analysis demonstrate DiT's disentanglement properties and effectiveness of the EIM framework.",
    "arxiv_url": "http://arxiv.org/abs/2411.08196v1",
    "pdf_url": "http://arxiv.org/pdf/2411.08196v1",
    "published_date": "2024-11-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image editing",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Taming Rectified Flow for Inversion and Editing",
    "authors": [
      "Jiangshan Wang",
      "Junfu Pu",
      "Zhongang Qi",
      "Jiayi Guo",
      "Yue Ma",
      "Nisha Huang",
      "Yuxin Chen",
      "Xiu Li",
      "Ying Shan"
    ],
    "abstract": "Rectified-flow-based diffusion transformers like FLUX and OpenSora have demonstrated outstanding performance in the field of image and video generation. Despite their robust generative capabilities, these models often struggle with inversion inaccuracies, which could further limit their effectiveness in downstream tasks such as image and video editing. To address this issue, we propose RF-Solver, a novel training-free sampler that effectively enhances inversion precision by mitigating the errors in the ODE-solving process of rectified flow. Specifically, we derive the exact formulation of the rectified flow ODE and apply the high-order Taylor expansion to estimate its nonlinear components, significantly enhancing the precision of ODE solutions at each timestep. Building upon RF-Solver, we further propose RF-Edit, a general feature-sharing-based framework for image and video editing. By incorporating self-attention features from the inversion process into the editing process, RF-Edit effectively preserves the structural information of the source image or video while achieving high-quality editing results. Our approach is compatible with any pre-trained rectified-flow-based models for image and video tasks, requiring no additional training or optimization. Extensive experiments across generation, inversion, and editing tasks in both image and video modalities demonstrate the superiority and versatility of our method. The source code is available at https://github.com/wangjiangshan0725/RF-Solver-Edit.",
    "arxiv_url": "http://arxiv.org/abs/2411.04746v2",
    "pdf_url": "http://arxiv.org/pdf/2411.04746v2",
    "published_date": "2024-11-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/wangjiangshan0725/RF-Solver-Edit",
    "keywords": [
      "FLUX",
      "video generation",
      "video editing",
      "inversion",
      "diffusion transformer",
      "rectified flow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiT4Edit: Diffusion Transformer for Image Editing",
    "authors": [
      "Kunyu Feng",
      "Yue Ma",
      "Bingyuan Wang",
      "Chenyang Qi",
      "Haozhe Chen",
      "Qifeng Chen",
      "Zeyu Wang"
    ],
    "abstract": "Despite recent advances in UNet-based image editing, methods for shape-aware object editing in high-resolution images are still lacking. Compared to UNet, Diffusion Transformers (DiT) demonstrate superior capabilities to effectively capture the long-range dependencies among patches, leading to higher-quality image generation. In this paper, we propose DiT4Edit, the first Diffusion Transformer-based image editing framework. Specifically, DiT4Edit uses the DPM-Solver inversion algorithm to obtain the inverted latents, reducing the number of steps compared to the DDIM inversion algorithm commonly used in UNet-based frameworks. Additionally, we design unified attention control and patches merging, tailored for transformer computation streams. This integration allows our framework to generate higher-quality edited images faster. Our design leverages the advantages of DiT, enabling it to surpass UNet structures in image editing, especially in high-resolution and arbitrary-size images. Extensive experiments demonstrate the strong performance of DiT4Edit across various editing scenarios, highlighting the potential of Diffusion Transformers in supporting image editing.",
    "arxiv_url": "http://arxiv.org/abs/2411.03286v2",
    "pdf_url": "http://arxiv.org/pdf/2411.03286v2",
    "published_date": "2024-11-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image editing",
      "Control",
      "image generation",
      "inversion",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
    "authors": [
      "Kumara Kahatapitiya",
      "Haozhe Liu",
      "Sen He",
      "Ding Liu",
      "Menglin Jia",
      "Chenyang Zhang",
      "Michael S. Ryoo",
      "Tian Xie"
    ],
    "abstract": "Generating temporally-consistent high-fidelity videos can be computationally expensive, especially over longer temporal spans. More-recent Diffusion Transformers (DiTs) -- despite making significant headway in this context -- have only heightened such challenges as they rely on larger models and heavier attention mechanisms, resulting in slower inference speeds. In this paper, we introduce a training-free method to accelerate video DiTs, termed Adaptive Caching (AdaCache), which is motivated by the fact that \"not all videos are created equal\": meaning, some videos require fewer denoising steps to attain a reasonable quality than others. Building on this, we not only cache computations through the diffusion process, but also devise a caching schedule tailored to each video generation, maximizing the quality-latency trade-off. We further introduce a Motion Regularization (MoReg) scheme to utilize video information within AdaCache, essentially controlling the compute allocation based on motion content. Altogether, our plug-and-play contributions grant significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video generation) without sacrificing the generation quality, across multiple video DiT baselines.",
    "arxiv_url": "http://arxiv.org/abs/2411.02397v2",
    "pdf_url": "http://arxiv.org/pdf/2411.02397v2",
    "published_date": "2024-11-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Training-free Regional Prompting for Diffusion Transformers",
    "authors": [
      "Anthony Chen",
      "Jianjin Xu",
      "Wenzhao Zheng",
      "Gaole Dai",
      "Yida Wang",
      "Renrui Zhang",
      "Haofan Wang",
      "Shanghang Zhang"
    ],
    "abstract": "Diffusion models have demonstrated excellent capabilities in text-to-image generation. Their semantic understanding (i.e., prompt following) ability has also been greatly improved with large language models (e.g., T5, Llama). However, existing models cannot perfectly handle long and complex text prompts, especially when the text prompts contain various objects with numerous attributes and interrelated spatial relationships. While many regional prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but there are still no implementations based on the recent Diffusion Transformer (DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and implement regional prompting for FLUX.1 based on attention manipulation, which enables DiT with fined-grained compositional text-to-image generation capability in a training-free manner. Code is available at https://github.com/antonioo-c/Regional-Prompting-FLUX.",
    "arxiv_url": "http://arxiv.org/abs/2411.02395v1",
    "pdf_url": "http://arxiv.org/pdf/2411.02395v1",
    "published_date": "2024-11-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/antonioo-c/Regional-Prompting-FLUX",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GameGen-X: Interactive Open-world Game Video Generation",
    "authors": [
      "Haoxuan Che",
      "Xuanhua He",
      "Quande Liu",
      "Cheng Jin",
      "Hao Chen"
    ],
    "abstract": "We introduce GameGen-X, the first diffusion transformer model specifically designed for both generating and interactively controlling open-world game videos. This model facilitates high-quality, open-domain generation by simulating an extensive array of game engine features, such as innovative characters, dynamic environments, complex actions, and diverse events. Additionally, it provides interactive controllability, predicting and altering future content based on the current clip, thus allowing for gameplay simulation. To realize this vision, we first collected and built an Open-World Video Game Dataset from scratch. It is the first and largest dataset for open-world game video generation and control, which comprises over a million diverse gameplay video clips sampling from over 150 games with informative captions from GPT-4o. GameGen-X undergoes a two-stage training process, consisting of foundation model pre-training and instruction tuning. Firstly, the model was pre-trained via text-to-video generation and video continuation, endowing it with the capability for long-sequence, high-quality open-domain game video generation. Further, to achieve interactive controllability, we designed InstructNet to incorporate game-related multi-modal control signal experts. This allows the model to adjust latent representations based on user inputs, unifying character interaction and scene content control for the first time in video generation. During instruction tuning, only the InstructNet is updated while the pre-trained foundation model is frozen, enabling the integration of interactive controllability without loss of diversity and quality of generated video content.",
    "arxiv_url": "http://arxiv.org/abs/2411.00769v3",
    "pdf_url": "http://arxiv.org/pdf/2411.00769v3",
    "published_date": "2024-11-01",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "In-Context LoRA for Diffusion Transformers",
    "authors": [
      "Lianghua Huang",
      "Wei Wang",
      "Zhi-Fan Wu",
      "Yupeng Shi",
      "Huanzhang Dou",
      "Chen Liang",
      "Yutong Feng",
      "Yu Liu",
      "Jingren Zhou"
    ],
    "abstract": "Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., 20~100 samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems. We release our code, data, and models at https://github.com/ali-vilab/In-Context-LoRA",
    "arxiv_url": "http://arxiv.org/abs/2410.23775v3",
    "pdf_url": "http://arxiv.org/pdf/2410.23775v3",
    "published_date": "2024-10-31",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "https://github.com/ali-vilab/In-Context-LoRA",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion Beats Autoregressive: An Evaluation of Compositional Generation in Text-to-Image Models",
    "authors": [
      "Arash Marioriyad",
      "Parham Rezaei",
      "Mahdieh Soleymani Baghshah",
      "Mohammad Hossein Rohban"
    ],
    "abstract": "Text-to-image (T2I) generative models, such as Stable Diffusion and DALL-E, have shown remarkable proficiency in producing high-quality, realistic, and natural images from textual descriptions. However, these models sometimes fail to accurately capture all the details specified in the input prompts, particularly concerning entities, attributes, and spatial relationships. This issue becomes more pronounced when the prompt contains novel or complex compositions, leading to what are known as compositional generation failure modes. Recently, a new open-source diffusion-based T2I model, FLUX, has been introduced, demonstrating strong performance in high-quality image generation. Additionally, autoregressive T2I models like LlamaGen have claimed competitive visual quality performance compared to diffusion-based models. In this study, we evaluate the compositional generation capabilities of these newly introduced models against established models using the T2I-CompBench benchmark. Our findings reveal that LlamaGen, as a vanilla autoregressive model, is not yet on par with state-of-the-art diffusion models for compositional generation tasks under the same criteria, such as model size and inference time. On the other hand, the open-source diffusion-based model FLUX exhibits compositional generation capabilities comparable to the state-of-the-art closed-source model DALL-E3.",
    "arxiv_url": "http://arxiv.org/abs/2410.22775v2",
    "pdf_url": "http://arxiv.org/pdf/2410.22775v2",
    "published_date": "2024-10-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diff-Instruct*: Towards Human-Preferred One-step Text-to-image Generative Models",
    "authors": [
      "Weijian Luo",
      "Colin Zhang",
      "Debing Zhang",
      "Zhengyang Geng"
    ],
    "abstract": "In this paper, we introduce the Diff-Instruct* (DI*), an image data-free approach for building one-step text-to-image generative models that align with human preference while maintaining the ability to generate highly realistic images. We frame human preference alignment as online reinforcement learning using human feedback (RLHF), where the goal is to maximize the reward function while regularizing the generator distribution to remain close to a reference diffusion process. Unlike traditional RLHF approaches, which rely on the KL divergence for regularization, we introduce a novel score-based divergence regularization, which leads to significantly better performances. Although the direct calculation of this preference alignment objective remains intractable, we demonstrate that we can efficiently compute its gradient by deriving an equivalent yet tractable loss function. Remarkably, we used Diff-Instruct* to train a Stable Diffusion-XL-based 1-step model, the 2.6B DI*-SDXL-1step text-to-image model, which can generate images of a resolution of 1024x1024 with only 1 generation step. DI*-SDXL-1step model uses only 1.88% inference time and 29.30% GPU memory cost to outperform 12B FLUX-dev-50step significantly in PickScore, ImageReward, and CLIPScore on Parti prompt benchmark and HPSv2.1 on Human Preference Score benchmark, establishing a new state-of-the-art benchmark of human-preferred 1-step text-to-image generative models. Besides the strong quantitative performances, extensive qualitative comparisons also confirm the advantages of DI* in terms of maintaining diversity, improving image layouts, and enhancing aesthetic colors. We have released our industry-ready model on the homepage: \\url{https://github.com/pkulwj1994/diff_instruct_star}.",
    "arxiv_url": "http://arxiv.org/abs/2410.20898v2",
    "pdf_url": "http://arxiv.org/pdf/2410.20898v2",
    "published_date": "2024-10-28",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "github_url": "https://github.com/pkulwj1994/diff_instruct_star",
    "keywords": [
      "FLUX",
      "text-to-image"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation",
    "authors": [
      "Zongyi Li",
      "Shujie Hu",
      "Shujie Liu",
      "Long Zhou",
      "Jeongsoo Choi",
      "Lingwei Meng",
      "Xun Guo",
      "Jinyu Li",
      "Hefei Ling",
      "Furu Wei"
    ],
    "abstract": "Text-to-video models have recently undergone rapid and substantial advancements. Nevertheless, due to limitations in data and computational resources, achieving efficient generation of long videos with rich motion dynamics remains a significant challenge. To generate high-quality, dynamic, and temporally consistent long videos, this paper presents ARLON, a novel framework that boosts diffusion Transformers with autoregressive models for long video generation, by integrating the coarse spatial and long-range temporal information provided by the AR model to guide the DiT model. Specifically, ARLON incorporates several key innovations: 1) A latent Vector Quantized Variational Autoencoder (VQ-VAE) compresses the input latent space of the DiT model into compact visual tokens, bridging the AR and DiT models and balancing the learning complexity and information density; 2) An adaptive norm-based semantic injection module integrates the coarse discrete visual units from the AR model into the DiT model, ensuring effective guidance during video generation; 3) To enhance the tolerance capability of noise introduced from the AR inference, the DiT model is trained with coarser visual latent tokens incorporated with an uncertainty sampling module. Experimental results demonstrate that ARLON significantly outperforms the baseline OpenSora-V1.2 on eight out of eleven metrics selected from VBench, with notable improvements in dynamic degree and aesthetic quality, while delivering competitive results on the remaining three and simultaneously accelerating the generation process. In addition, ARLON achieves state-of-the-art performance in long video generation. Detailed analyses of the improvements in inference efficiency are presented, alongside a practical application that demonstrates the generation of long videos using progressive text prompts. See demos of ARLON at http://aka.ms/arlon.",
    "arxiv_url": "http://arxiv.org/abs/2410.20502v3",
    "pdf_url": "http://arxiv.org/pdf/2410.20502v3",
    "published_date": "2024-10-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation",
    "authors": [
      "Phillip Y. Lee",
      "Taehoon Yoon",
      "Minhyuk Sung"
    ],
    "abstract": "We introduce GrounDiT, a novel training-free spatial grounding technique for text-to-image generation using Diffusion Transformers (DiT). Spatial grounding with bounding boxes has gained attention for its simplicity and versatility, allowing for enhanced user control in image generation. However, prior training-free approaches often rely on updating the noisy image during the reverse diffusion process via backpropagation from custom loss functions, which frequently struggle to provide precise control over individual bounding boxes. In this work, we leverage the flexibility of the Transformer architecture, demonstrating that DiT can generate noisy patches corresponding to each bounding box, fully encoding the target object and allowing for fine-grained control over each region. Our approach builds on an intriguing property of DiT, which we refer to as semantic sharing. Due to semantic sharing, when a smaller patch is jointly denoised alongside a generatable-size image, the two become semantic clones. Each patch is denoised in its own branch of the generation process and then transplanted into the corresponding region of the original noisy image at each timestep, resulting in robust spatial grounding for each bounding box. In our experiments on the HRS and DrawBench benchmarks, we achieve state-of-the-art performance compared to previous training-free approaches.",
    "arxiv_url": "http://arxiv.org/abs/2410.20474v2",
    "pdf_url": "http://arxiv.org/pdf/2410.20474v2",
    "published_date": "2024-10-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model",
    "authors": [
      "ZiDong Wang",
      "Zeyu Lu",
      "Di Huang",
      "Cai Zhou",
      "Wanli Ouyang",
      "and Lei Bai"
    ],
    "abstract": "\\textit{Nature is infinitely resolution-free}. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To address this limitation, we conceptualize images as sequences of tokens with dynamic sizes, rather than traditional methods that perceive images as fixed-resolution grids. This perspective enables a flexible training strategy that seamlessly accommodates various aspect ratios during both training and inference, thus promoting resolution generalization and eliminating biases introduced by image cropping. On this basis, we present the \\textbf{Flexible Vision Transformer} (FiT), a transformer architecture specifically designed for generating images with \\textit{unrestricted resolutions and aspect ratios}. We further upgrade the FiT to FiTv2 with several innovative designs, includingthe Query-Key vector normalization, the AdaLN-LoRA module, a rectified flow scheduler, and a Logit-Normal sampler. Enhanced by a meticulously adjusted network structure, FiTv2 exhibits $2\\times$ convergence speed of FiT. When incorporating advanced training-free extrapolation techniques, FiTv2 demonstrates remarkable adaptability in both resolution extrapolation and diverse resolution generation. Additionally, our exploration of the scalability of the FiTv2 model reveals that larger models exhibit better computational efficiency. Furthermore, we introduce an efficient post-training strategy to adapt a pre-trained model for the high-resolution generation. Comprehensive experiments demonstrate the exceptional performance of FiTv2 across a broad range of resolutions. We have released all the codes and models at \\url{https://github.com/whlzy/FiT} to promote the exploration of diffusion transformer models for arbitrary-resolution image generation.",
    "arxiv_url": "http://arxiv.org/abs/2410.13925v1",
    "pdf_url": "http://arxiv.org/pdf/2410.13925v1",
    "published_date": "2024-10-17",
    "categories": [
      "cs.LG"
    ],
    "github_url": "https://github.com/whlzy/FiT",
    "keywords": [
      "image generation",
      "diffusion transformer",
      "rectified flow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Boosting Camera Motion Control for Video Diffusion Transformers",
    "authors": [
      "Soon Yau Cheong",
      "Duygu Ceylan",
      "Armin Mustafa",
      "Andrew Gilbert",
      "Chun-Hao Paul Huang"
    ],
    "abstract": "Recent advancements in diffusion models have significantly enhanced the quality of video generation. However, fine-grained control over camera pose remains a challenge. While U-Net-based models have shown promising results for camera control, transformer-based diffusion models (DiT)-the preferred architecture for large-scale video generation - suffer from severe degradation in camera motion accuracy. In this paper, we investigate the underlying causes of this issue and propose solutions tailored to DiT architectures. Our study reveals that camera control performance depends heavily on the choice of conditioning methods rather than camera pose representations that is commonly believed. To address the persistent motion degradation in DiT, we introduce Camera Motion Guidance (CMG), based on classifier-free guidance, which boosts camera control by over 400%. Additionally, we present a sparse camera control pipeline, significantly simplifying the process of specifying camera poses for long videos. Our method universally applies to both U-Net and DiT models, offering improved camera control for video generation tasks.",
    "arxiv_url": "http://arxiv.org/abs/2410.10802v1",
    "pdf_url": "http://arxiv.org/pdf/2410.10802v1",
    "published_date": "2024-10-14",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations",
    "authors": [
      "Litu Rout",
      "Yujia Chen",
      "Nataniel Ruiz",
      "Constantine Caramanis",
      "Sanjay Shakkottai",
      "Wen-Sheng Chu"
    ],
    "abstract": "Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.",
    "arxiv_url": "http://arxiv.org/abs/2410.10792v1",
    "pdf_url": "http://arxiv.org/pdf/2410.10792v1",
    "published_date": "2024-10-14",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "image editing",
      "Control",
      "inversion",
      "rectified flow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scaling Laws For Diffusion Transformers",
    "authors": [
      "Zhengyang Liang",
      "Hao He",
      "Ceyuan Yang",
      "Bo Dai"
    ],
    "abstract": "Diffusion transformers (DiT) have already achieved appealing synthesis and scaling properties in content recreation, e.g., image and video generation. However, scaling laws of DiT are less explored, which usually offer precise predictions regarding optimal model size and data requirements given a specific compute budget. Therefore, experiments across a broad range of compute budgets, from 1e17 to 6e18 FLOPs are conducted to confirm the existence of scaling laws in DiT for the first time. Concretely, the loss of pretraining DiT also follows a power-law relationship with the involved compute. Based on the scaling law, we can not only determine the optimal model size and required data but also accurately predict the text-to-image generation loss given a model with 1B parameters and a compute budget of 1e21 FLOPs. Additionally, we also demonstrate that the trend of pre-training loss matches the generation performances (e.g., FID), even across various datasets, which complements the mapping from compute to synthesis quality and thus provides a predictable benchmark that assesses model performance and data quality at a reduced cost.",
    "arxiv_url": "http://arxiv.org/abs/2410.08184v1",
    "pdf_url": "http://arxiv.org/pdf/2410.08184v1",
    "published_date": "2024-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation",
    "authors": [
      "Xinchen Zhang",
      "Ling Yang",
      "Guohao Li",
      "Yaqi Cai",
      "Jiake Xie",
      "Yong Tang",
      "Yujiu Yang",
      "Mengdi Wang",
      "Bin Cui"
    ],
    "abstract": "Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relationships. This disparity highlights the need for an approach that can leverage the complementary strengths of various models to comprehensively improve the composition capability. To this end, we introduce IterComp, a novel framework that aggregates composition-aware model preferences from multiple models and employs an iterative feedback learning approach to enhance compositional generation. Specifically, we curate a gallery of six powerful open-source diffusion models and evaluate their three key compositional metrics: attribute binding, spatial relationships, and non-spatial relationships. Based on these metrics, we develop a composition-aware model preference dataset comprising numerous image-rank pairs to train composition-aware reward models. Then, we propose an iterative feedback learning method to enhance compositionality in a closed-loop manner, enabling the progressive self-refinement of both the base diffusion model and reward models over multiple iterations. Theoretical proof demonstrates the effectiveness and extensive experiments show our significant superiority over previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category object composition and complex semantic alignment. IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation. Code: https://github.com/YangLing0818/IterComp",
    "arxiv_url": "http://arxiv.org/abs/2410.07171v2",
    "pdf_url": "http://arxiv.org/pdf/2410.07171v2",
    "published_date": "2024-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/YangLing0818/IterComp",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pyramidal Flow Matching for Efficient Video Generative Modeling",
    "authors": [
      "Yang Jin",
      "Zhicheng Sun",
      "Ningyuan Li",
      "Kun Xu",
      "Kun Xu",
      "Hao Jiang",
      "Nan Zhuang",
      "Quzhe Huang",
      "Yang Song",
      "Yadong Mu",
      "Zhouchen Lin"
    ],
    "abstract": "Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution latent. Despite reducing computational demands, the separate optimization of each sub-stage hinders knowledge sharing and sacrifices flexibility. This work introduces a unified pyramidal flow matching algorithm. It reinterprets the original denoising trajectory as a series of pyramid stages, where only the final stage operates at the full resolution, thereby enabling more efficient video generative modeling. Through our sophisticated design, the flows of different pyramid stages can be interlinked to maintain continuity. Moreover, we craft autoregressive video generation with a temporal pyramid to compress the full-resolution history. The entire framework can be optimized in an end-to-end manner and with a single unified Diffusion Transformer (DiT). Extensive experiments demonstrate that our method supports generating high-quality 5-second (up to 10-second) videos at 768p resolution and 24 FPS within 20.7k A100 GPU training hours. All code and models are open-sourced at https://pyramid-flow.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2410.05954v2",
    "pdf_url": "http://arxiv.org/pdf/2410.05954v2",
    "published_date": "2024-10-08",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
    "authors": [
      "Chang Zou",
      "Xuyang Liu",
      "Ting Liu",
      "Siteng Huang",
      "Linfeng Zhang"
    ],
    "abstract": "Diffusion transformers have shown significant effectiveness in both image and video synthesis at the expense of huge computation costs. To address this problem, feature caching methods have been introduced to accelerate diffusion transformers by caching the features in previous timesteps and reusing them in the following timesteps. However, previous caching methods ignore that different tokens exhibit different sensitivities to feature caching, and feature caching on some tokens may lead to 10$\\times$ more destruction to the overall generation quality compared with other tokens. In this paper, we introduce token-wise feature caching, allowing us to adaptively select the most suitable tokens for caching, and further enable us to apply different caching ratios to neural layers in different types and depths. Extensive experiments on PixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image and video generation with no requirements for training. For instance, 2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and PixArt-$\\alpha$ with almost no drop in generation quality.",
    "arxiv_url": "http://arxiv.org/abs/2410.05317v4",
    "pdf_url": "http://arxiv.org/pdf/2410.05317v4",
    "published_date": "2024-10-05",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Diffusion Transformer",
    "authors": [
      "Wangbo Zhao",
      "Yizeng Han",
      "Jiasheng Tang",
      "Kai Wang",
      "Yibing Song",
      "Gao Huang",
      "Fan Wang",
      "Yang You"
    ],
    "abstract": "Diffusion Transformer (DiT), an emerging diffusion model for image generation, has demonstrated superior performance but suffers from substantial computational costs. Our investigations reveal that these costs stem from the static inference paradigm, which inevitably introduces redundant computation in certain diffusion timesteps and spatial regions. To address this inefficiency, we propose Dynamic Diffusion Transformer (DyDiT), an architecture that dynamically adjusts its computation along both timestep and spatial dimensions during generation. Specifically, we introduce a Timestep-wise Dynamic Width (TDW) approach that adapts model width conditioned on the generation timesteps. In addition, we design a Spatial-wise Dynamic Token (SDT) strategy to avoid redundant computation at unnecessary spatial locations. Extensive experiments on various datasets and different-sized models verify the superiority of DyDiT. Notably, with <3% additional fine-tuning iterations, our method reduces the FLOPs of DiT-XL by 51%, accelerates generation by 1.73, and achieves a competitive FID score of 2.07 on ImageNet. The code is publicly available at https://github.com/NUS-HPC-AI-Lab/ Dynamic-Diffusion-Transformer.",
    "arxiv_url": "http://arxiv.org/abs/2410.03456v2",
    "pdf_url": "http://arxiv.org/pdf/2410.03456v2",
    "published_date": "2024-10-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice Routing",
    "authors": [
      "Haotian Sun",
      "Tao Lei",
      "Bowen Zhang",
      "Yanghao Li",
      "Haoshuo Huang",
      "Ruoming Pang",
      "Bo Dai",
      "Nan Du"
    ],
    "abstract": "Diffusion transformers have been widely adopted for text-to-image synthesis. While scaling these models up to billions of parameters shows promise, the effectiveness of scaling beyond current sizes remains underexplored and challenging. By explicitly exploiting the computational heterogeneity of image generations, we develop a new family of Mixture-of-Experts (MoE) models (EC-DIT) for diffusion transformers with expert-choice routing. EC-DIT learns to adaptively optimize the compute allocated to understand the input texts and generate the respective image patches, enabling heterogeneous computation aligned with varying text-image complexities. This heterogeneity provides an efficient way of scaling EC-DIT up to 97 billion parameters and achieving significant improvements in training convergence, text-to-image alignment, and overall generation quality over dense models and conventional MoE models. Through extensive ablations, we show that EC-DIT demonstrates superior scalability and adaptive compute allocation by recognizing varying textual importance through end-to-end training. Notably, in text-to-image alignment evaluation, our largest models achieve a state-of-the-art GenEval score of 71.68% and still maintain competitive inference speed with intuitive interpretability.",
    "arxiv_url": "http://arxiv.org/abs/2410.02098v5",
    "pdf_url": "http://arxiv.org/pdf/2410.02098v5",
    "published_date": "2024-10-02",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Effective Diffusion Transformer Architecture for Image Super-Resolution",
    "authors": [
      "Kun Cheng",
      "Lei Yu",
      "Zhijun Tu",
      "Xiao He",
      "Liyu Chen",
      "Yong Guo",
      "Mingrui Zhu",
      "Nannan Wang",
      "Xinbo Gao",
      "Jie Hu"
    ],
    "abstract": "Recent advances indicate that diffusion models hold great promise in image super-resolution. While the latest methods are primarily based on latent diffusion models with convolutional neural networks, there are few attempts to explore transformers, which have demonstrated remarkable performance in image generation. In this work, we design an effective diffusion transformer for image super-resolution (DiT-SR) that achieves the visual quality of prior-based methods, but through a training-from-scratch manner. In practice, DiT-SR leverages an overall U-shaped architecture, and adopts a uniform isotropic design for all the transformer blocks across different stages. The former facilitates multi-scale hierarchical feature extraction, while the latter reallocates the computational resources to critical layers to further enhance performance. Moreover, we thoroughly analyze the limitation of the widely used AdaLN, and present a frequency-adaptive time-step conditioning module, enhancing the model's capacity to process distinct frequency information at different time steps. Extensive experiments demonstrate that DiT-SR outperforms the existing training-from-scratch diffusion-based SR methods significantly, and even beats some of the prior-based methods on pretrained Stable Diffusion, proving the superiority of diffusion transformer in image super-resolution.",
    "arxiv_url": "http://arxiv.org/abs/2409.19589v1",
    "pdf_url": "http://arxiv.org/pdf/2409.19589v1",
    "published_date": "2024-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer",
      "image super-resolution"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language Instructions",
    "authors": [
      "Weifeng Lin",
      "Xinyu Wei",
      "Renrui Zhang",
      "Le Zhuo",
      "Shitian Zhao",
      "Siyuan Huang",
      "Huan Teng",
      "Junlin Xie",
      "Yu Qiao",
      "Peng Gao",
      "Hongsheng Li"
    ],
    "abstract": "This paper presents a versatile image-to-image visual assistant, PixWizard, designed for image generation, manipulation, and translation based on free-from language instructions. To this end, we tackle a variety of vision tasks into a unified image-text-to-image generation framework and curate an Omni Pixel-to-Pixel Instruction-Tuning Dataset. By constructing detailed instruction templates in natural language, we comprehensively include a large set of diverse vision tasks such as text-to-image generation, image restoration, image grounding, dense image prediction, image editing, controllable generation, inpainting/outpainting, and more. Furthermore, we adopt Diffusion Transformers (DiT) as our foundation model and extend its capabilities with a flexible any resolution mechanism, enabling the model to dynamically process images based on the aspect ratio of the input, closely aligning with human perceptual processes. The model also incorporates structure-aware and semantic-aware guidance to facilitate effective fusion of information from the input image. Our experiments demonstrate that PixWizard not only shows impressive generative and understanding abilities for images with diverse resolutions but also exhibits promising generalization capabilities with unseen tasks and human instructions. The code and related resources are available at https://github.com/AFeng-x/PixWizard",
    "arxiv_url": "http://arxiv.org/abs/2409.15278v4",
    "pdf_url": "http://arxiv.org/pdf/2409.15278v4",
    "published_date": "2024-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/AFeng-x/PixWizard",
    "keywords": [
      "text-to-image",
      "image editing",
      "Control",
      "image generation",
      "diffusion transformer",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LoVA: Long-form Video-to-Audio Generation",
    "authors": [
      "Xin Cheng",
      "Xihua Wang",
      "Yihan Wu",
      "Yuyue Wang",
      "Ruihua Song"
    ],
    "abstract": "Video-to-audio (V2A) generation is important for video editing and post-processing, enabling the creation of semantics-aligned audio for silent video. However, most existing methods focus on generating short-form audio for short video segment (less than 10 seconds), while giving little attention to the scenario of long-form video inputs. For current UNet-based diffusion V2A models, an inevitable problem when handling long-form audio generation is the inconsistencies within the final concatenated audio. In this paper, we first highlight the importance of long-form V2A problem. Besides, we propose LoVA, a novel model for Long-form Video-to-Audio generation. Based on the Diffusion Transformer (DiT) architecture, LoVA proves to be more effective at generating long-form audio compared to existing autoregressive models and UNet-based diffusion models. Extensive objective and subjective experiments demonstrate that LoVA achieves comparable performance on 10-second V2A benchmark and outperforms all other baselines on a benchmark with long-form video input.",
    "arxiv_url": "http://arxiv.org/abs/2409.15157v2",
    "pdf_url": "http://arxiv.org/pdf/2409.15157v2",
    "published_date": "2024-09-23",
    "categories": [
      "cs.SD",
      "cs.MM",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "video editing",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EDGE-Rec: Efficient and Data-Guided Edge Diffusion For Recommender Systems Graphs",
    "authors": [
      "Utkarsh Priyam",
      "Hemit Shah",
      "Edoardo Botta"
    ],
    "abstract": "Most recommender systems research focuses on binary historical user-item interaction encodings to predict future interactions. User features, item features, and interaction strengths remain largely under-utilized in this space or only indirectly utilized, despite proving largely effective in large-scale production recommendation systems. We propose a new attention mechanism, loosely based on the principles of collaborative filtering, called Row-Column Separable Attention RCSA to take advantage of real-valued interaction weights as well as user and item features directly. Building on this mechanism, we additionally propose a novel Graph Diffusion Transformer GDiT architecture which is trained to iteratively denoise the weighted interaction matrix of the user-item interaction graph directly. The weighted interaction matrix is built from the bipartite structure of the user-item interaction graph and corresponding edge weights derived from user-item rating interactions. Inspired by the recent progress in text-conditioned image generation, our method directly produces user-item rating predictions on the same scale as the original ratings by conditioning the denoising process on user and item features with a principled approach.",
    "arxiv_url": "http://arxiv.org/abs/2409.14689v1",
    "pdf_url": "http://arxiv.org/pdf/2409.14689v1",
    "published_date": "2024-09-23",
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Qihoo-T2X: An Efficient Proxy-Tokenized Diffusion Transformer for Text-to-Any-Task",
    "authors": [
      "Jing Wang",
      "Ao Ma",
      "Jiasong Feng",
      "Dawei Leng",
      "Yuhui Yin",
      "Xiaodan Liang"
    ],
    "abstract": "The global self-attention mechanism in diffusion transformers involves redundant computation due to the sparse and redundant nature of visual information, and the attention map of tokens within a spatial window shows significant similarity. To address this redundancy, we propose the Proxy-Tokenized Diffusion Transformer (PT-DiT), which employs sparse representative token attention (where the number of representative tokens is much smaller than the total number of tokens) to model global visual information efficiently. Specifically, within each transformer block, we compute an averaging token from each spatial-temporal window to serve as a proxy token for that region. The global semantics are captured through the self-attention of these proxy tokens and then injected into all latent tokens via cross-attention. Simultaneously, we introduce window and shift window attention to address the limitations in detail modeling caused by the sparse attention mechanism. Building on the well-designed PT-DiT, we further develop the Qihoo-T2X family, which includes a variety of models for T2I, T2V, and T2MV tasks. Experimental results show that PT-DiT achieves competitive performance while reducing the computational complexity in both image and video generation tasks (e.g., a 49% reduction compared to DiT and a 34% reduction compared to PixArt-$\\alpha$). The visual exhibition and source code of Qihoo-T2X is available at https://360cvgroup.github.io/Qihoo-T2X/.",
    "arxiv_url": "http://arxiv.org/abs/2409.04005v2",
    "pdf_url": "http://arxiv.org/pdf/2409.04005v2",
    "published_date": "2024-09-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiVE: DiT-based Video Generation with Enhanced Control",
    "authors": [
      "Junpeng Jiang",
      "Gangyi Hong",
      "Lijun Zhou",
      "Enhui Ma",
      "Hengtong Hu",
      "Xia Zhou",
      "Jie Xiang",
      "Fan Liu",
      "Kaicheng Yu",
      "Haiyang Sun",
      "Kun Zhan",
      "Peng Jia",
      "Miao Zhang"
    ],
    "abstract": "Generating high-fidelity, temporally consistent videos in autonomous driving scenarios faces a significant challenge, e.g. problematic maneuvers in corner cases. Despite recent video generation works are proposed to tackcle the mentioned problem, i.e. models built on top of Diffusion Transformers (DiT), works are still missing which are targeted on exploring the potential for multi-view videos generation scenarios. Noticeably, we propose the first DiT-based framework specifically designed for generating temporally and multi-view consistent videos which precisely match the given bird's-eye view layouts control. Specifically, the proposed framework leverages a parameter-free spatial view-inflated attention mechanism to guarantee the cross-view consistency, where joint cross-attention modules and ControlNet-Transformer are integrated to further improve the precision of control. To demonstrate our advantages, we extensively investigate the qualitative comparisons on nuScenes dataset, particularly in some most challenging corner cases. In summary, the effectiveness of our proposed method in producing long, controllable, and highly consistent videos under difficult conditions is proven to be effective.",
    "arxiv_url": "http://arxiv.org/abs/2409.01595v1",
    "pdf_url": "http://arxiv.org/pdf/2409.01595v1",
    "published_date": "2024-09-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers",
    "authors": [
      "Juncan Deng",
      "Shuaiting Li",
      "Zeyu Wang",
      "Hong Gu",
      "Kedong Xu",
      "Kejie Huang"
    ],
    "abstract": "The Diffusion Transformers Models (DiTs) have transitioned the network architecture from traditional UNets to transformers, demonstrating exceptional capabilities in image generation. Although DiTs have been widely applied to high-definition video generation tasks, their large parameter size hinders inference on edge devices. Vector quantization (VQ) can decompose model weight into a codebook and assignments, allowing extreme weight quantization and significantly reducing memory usage. In this paper, we propose VQ4DiT, a fast post-training vector quantization method for DiTs. We found that traditional VQ methods calibrate only the codebook without calibrating the assignments. This leads to weight sub-vectors being incorrectly assigned to the same assignment, providing inconsistent gradients to the codebook and resulting in a suboptimal result. To address this challenge, VQ4DiT calculates the candidate assignment set for each weight sub-vector based on Euclidean distance and reconstructs the sub-vector based on the weighted average. Then, using the zero-data and block-wise calibration method, the optimal assignment from the set is efficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT XL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending on the different quantization settings. Experiments show that VQ4DiT establishes a new state-of-the-art in model size and performance trade-offs, quantizing weights to 2-bit precision while retaining acceptable image generation quality.",
    "arxiv_url": "http://arxiv.org/abs/2408.17131v1",
    "pdf_url": "http://arxiv.org/pdf/2408.17131v1",
    "published_date": "2024-08-30",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2; I.4"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Alfie: Democratising RGBA Image Generation With No $$$",
    "authors": [
      "Fabio Quattrini",
      "Vittorio Pippi",
      "Silvia Cascianelli",
      "Rita Cucchiara"
    ],
    "abstract": "Designs and artworks are ubiquitous across various creative fields, requiring graphic design skills and dedicated software to create compositions that include many graphical elements, such as logos, icons, symbols, and art scenes, which are integral to visual storytelling. Automating the generation of such visual elements improves graphic designers' productivity, democratizes and innovates the creative industry, and helps generate more realistic synthetic data for related tasks. These illustration elements are mostly RGBA images with irregular shapes and cutouts, facilitating blending and scene composition. However, most image generation models are incapable of generating such images and achieving this capability requires expensive computational resources, specific training recipes, or post-processing solutions. In this work, we propose a fully-automated approach for obtaining RGBA illustrations by modifying the inference-time behavior of a pre-trained Diffusion Transformer model, exploiting the prompt-guided controllability and visual quality offered by such models with no additional computational cost. We force the generation of entire subjects without sharp croppings, whose background is easily removed for seamless integration into design projects or artistic scenes. We show with a user study that, in most cases, users prefer our solution over generating and then matting an image, and we show that our generated illustrations yield good results when used as inputs for composite scene generation pipelines. We release the code at https://github.com/aimagelab/Alfie.",
    "arxiv_url": "http://arxiv.org/abs/2408.14826v1",
    "pdf_url": "http://arxiv.org/pdf/2408.14826v1",
    "published_date": "2024-08-27",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "https://github.com/aimagelab/Alfie",
    "keywords": [
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Latent Space Disentanglement in Diffusion Transformers Enables Zero-shot Fine-grained Semantic Editing",
    "authors": [
      "Zitao Shuai",
      "Chenwei Wu",
      "Zhengxu Tang",
      "Bowen Song",
      "Liyue Shen"
    ],
    "abstract": "Diffusion Transformers (DiTs) have achieved remarkable success in diverse and high-quality text-to-image(T2I) generation. However, how text and image latents individually and jointly contribute to the semantics of generated images, remain largely unexplored. Through our investigation of DiT's latent space, we have uncovered key findings that unlock the potential for zero-shot fine-grained semantic editing: (1) Both the text and image spaces in DiTs are inherently decomposable. (2) These spaces collectively form a disentangled semantic representation space, enabling precise and fine-grained semantic control. (3) Effective image editing requires the combined use of both text and image latent spaces. Leveraging these insights, we propose a simple and effective Extract-Manipulate-Sample (EMS) framework for zero-shot fine-grained image editing. Our approach first utilizes a multi-modal Large Language Model to convert input images and editing targets into text descriptions. We then linearly manipulate text embeddings based on the desired editing degree and employ constrained score distillation sampling to manipulate image embeddings. We quantify the disentanglement degree of the latent space of diffusion models by proposing a new metric. To evaluate fine-grained editing performance, we introduce a comprehensive benchmark incorporating both human annotations, manual evaluation, and automatic metrics. We have conducted extensive experimental results and in-depth analysis to thoroughly uncover the semantic disentanglement properties of the diffusion transformer, as well as the effectiveness of our proposed method. Our annotated benchmark dataset is publicly available at https://anonymous.com/anonymous/EMS-Benchmark, facilitating reproducible research in this domain.",
    "arxiv_url": "http://arxiv.org/abs/2408.13335v1",
    "pdf_url": "http://arxiv.org/pdf/2408.13335v1",
    "published_date": "2024-08-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "text-to-image",
      "diffusion transformer",
      "image editing"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations",
    "authors": [
      "Can Qin",
      "Congying Xia",
      "Krithika Ramakrishnan",
      "Michael Ryoo",
      "Lifu Tu",
      "Yihao Feng",
      "Manli Shu",
      "Honglu Zhou",
      "Anas Awadalla",
      "Jun Wang",
      "Senthil Purushwalkam",
      "Le Xue",
      "Yingbo Zhou",
      "Huan Wang",
      "Silvio Savarese",
      "Juan Carlos Niebles",
      "Zeyuan Chen",
      "Ran Xu",
      "Caiming Xiong"
    ],
    "abstract": "We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of producing realistic scenes from textual descriptions. Building on recent advancements, such as OpenAI's Sora, we explore the latent diffusion model (LDM) architecture and introduce a video variational autoencoder (VidVAE). VidVAE compresses video data both spatially and temporally, significantly reducing the length of visual tokens and the computational demands associated with generating long-sequence videos. To further address the computational costs, we propose a divide-and-merge strategy that maintains temporal consistency across video segments. Our Diffusion Transformer (DiT) model incorporates spatial and temporal self-attention layers, enabling robust generalization across different timeframes and aspect ratios. We have devised a data processing pipeline from the very beginning and collected over 13M high-quality video-text pairs. The pipeline includes multiple steps such as clipping, text detection, motion estimation, aesthetics scoring, and dense captioning based on our in-house video-LLM model. Training the VidVAE and DiT models required approximately 40 and 642 H100 days, respectively. Our model supports over 14-second 720p video generation in an end-to-end way and demonstrates competitive performance against state-of-the-art T2V models.",
    "arxiv_url": "http://arxiv.org/abs/2408.12590v2",
    "pdf_url": "http://arxiv.org/pdf/2408.12590v2",
    "published_date": "2024-08-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for Dynamic Medical Image Generation in Virtual Simulated Patient",
    "authors": [
      "Yanzeng Li",
      "Cheng Zeng",
      "Jinchao Zhang",
      "Jie Zhou",
      "Lei Zou"
    ],
    "abstract": "Medical education relies heavily on Simulated Patients (SPs) to provide a safe environment for students to practice clinical skills, including medical image analysis. However, the high cost of recruiting qualified SPs and the lack of diverse medical imaging datasets have presented significant challenges. To address these issues, this paper introduces MedDiT, a novel knowledge-controlled conversational framework that can dynamically generate plausible medical images aligned with simulated patient symptoms, enabling diverse diagnostic skill training. Specifically, MedDiT integrates various patient Knowledge Graphs (KGs), which describe the attributes and symptoms of patients, to dynamically prompt Large Language Models' (LLMs) behavior and control the patient characteristics, mitigating hallucination during medical conversation. Additionally, a well-tuned Diffusion Transformer (DiT) model is incorporated to generate medical images according to the specified patient attributes in the KG. In this paper, we present the capabilities of MedDiT through a practical demonstration, showcasing its ability to act in diverse simulated patient cases and generate the corresponding medical images. This can provide an abundant and interactive learning experience for students, advancing medical education by offering an immersive simulation platform for future healthcare professionals. The work sheds light on the feasibility of incorporating advanced technologies like LLM, KG, and DiT in education applications, highlighting their potential to address the challenges faced in simulated patient-based medical education.",
    "arxiv_url": "http://arxiv.org/abs/2408.12236v1",
    "pdf_url": "http://arxiv.org/pdf/2408.12236v1",
    "published_date": "2024-08-22",
    "categories": [
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer",
    "authors": [
      "Zhuoyi Yang",
      "Jiayan Teng",
      "Wendi Zheng",
      "Ming Ding",
      "Shiyu Huang",
      "Jiazheng Xu",
      "Yuanming Yang",
      "Wenyi Hong",
      "Xiaohan Zhang",
      "Guanyu Feng",
      "Da Yin",
      "Yuxuan Zhang",
      "Weihan Wang",
      "Yean Cheng",
      "Bin Xu",
      "Xiaotao Gu",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "abstract": "We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.",
    "arxiv_url": "http://arxiv.org/abs/2408.06072v3",
    "pdf_url": "http://arxiv.org/pdf/2408.06072v3",
    "published_date": "2024-08-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/THUDM/CogVideo",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "An Object is Worth 64x64 Pixels: Generating 3D Object via Image Diffusion",
    "authors": [
      "Xingguang Yan",
      "Han-Hung Lee",
      "Ziyu Wan",
      "Angel X. Chang"
    ],
    "abstract": "We introduce a new approach for generating realistic 3D models with UV maps through a representation termed \"Object Images.\" This approach encapsulates surface geometry, appearance, and patch structures within a 64x64 pixel image, effectively converting complex 3D shapes into a more manageable 2D format. By doing so, we address the challenges of both geometric and semantic irregularity inherent in polygonal meshes. This method allows us to use image generation models, such as Diffusion Transformers, directly for 3D shape generation. Evaluated on the ABO dataset, our generated shapes with patch structures achieve point cloud FID comparable to recent 3D generative models, while naturally supporting PBR material generation.",
    "arxiv_url": "http://arxiv.org/abs/2408.03178v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03178v1",
    "published_date": "2024-08-06",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tora: Trajectory-oriented Diffusion Transformer for Video Generation",
    "authors": [
      "Zhenghao Zhang",
      "Junchao Liao",
      "Menghao Li",
      "Zuozhuo Dai",
      "Bingxue Qiu",
      "Siyu Zhu",
      "Long Qin",
      "Weizhi Wang"
    ],
    "abstract": "Recent advancements in Diffusion Transformer (DiT) have demonstrated remarkable proficiency in producing high-quality video content. Nonetheless, the potential of transformer-based diffusion models for effectively generating videos with controllable motion remains an area of limited exploration. This paper introduces Tora, the first trajectory-oriented DiT framework that concurrently integrates textual, visual, and trajectory conditions, thereby enabling scalable video generation with effective motion guidance. Specifically, Tora consists of a Trajectory Extractor (TE), a Spatial-Temporal DiT, and a Motion-guidance Fuser (MGF). The TE encodes arbitrary trajectories into hierarchical spacetime motion patches with a 3D motion compression network. The MGF integrates the motion patches into the DiT blocks to generate consistent videos that accurately follow designated trajectories. Our design aligns seamlessly with DiT's scalability, allowing precise control of video content's dynamics with diverse durations, aspect ratios, and resolutions. Extensive experiments demonstrate that Tora excels in achieving high motion fidelity compared to the foundational DiT model, while also accurately simulating the complex movements of the physical world. Code is made available at https://github.com/alibaba/Tora .",
    "arxiv_url": "http://arxiv.org/abs/2407.21705v4",
    "pdf_url": "http://arxiv.org/pdf/2407.21705v4",
    "published_date": "2024-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/alibaba/Tora",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MotionCraft: Crafting Whole-Body Motion with Plug-and-Play Multimodal Controls",
    "authors": [
      "Yuxuan Bian",
      "Ailing Zeng",
      "Xuan Ju",
      "Xian Liu",
      "Zhaoyang Zhang",
      "Wei Liu",
      "Qiang Xu"
    ],
    "abstract": "Whole-body multimodal motion generation, controlled by text, speech, or music, has numerous applications including video generation and character animation. However, employing a unified model to achieve various generation tasks with different condition modalities presents two main challenges: motion distribution drifts across different tasks (e.g., co-speech gestures and text-driven daily actions) and the complex optimization of mixed conditions with varying granularities (e.g., text and audio). Additionally, inconsistent motion formats across different tasks and datasets hinder effective training toward multimodal motion generation. In this paper, we propose MotionCraft, a unified diffusion transformer that crafts whole-body motion with plug-and-play multimodal control. Our framework employs a coarse-to-fine training strategy, starting with the first stage of text-to-motion semantic pre-training, followed by the second stage of multimodal low-level control adaptation to handle conditions of varying granularities. To effectively learn and transfer motion knowledge across different distributions, we design MC-Attn for parallel modeling of static and dynamic human topology graphs. To overcome the motion format inconsistency of existing benchmarks, we introduce MC-Bench, the first available multimodal whole-body motion generation benchmark based on the unified SMPL-X format. Extensive experiments show that MotionCraft achieves state-of-the-art performance on various standard motion generation tasks.",
    "arxiv_url": "http://arxiv.org/abs/2407.21136v3",
    "pdf_url": "http://arxiv.org/pdf/2407.21136v3",
    "published_date": "2024-07-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion Transformer Captures Spatial-Temporal Dependencies: A Theory for Gaussian Process Data",
    "authors": [
      "Hengyu Fu",
      "Zehao Dou",
      "Jiawei Guo",
      "Mengdi Wang",
      "Minshuo Chen"
    ],
    "abstract": "Diffusion Transformer, the backbone of Sora for video generation, successfully scales the capacity of diffusion models, pioneering new avenues for high-fidelity sequential data generation. Unlike static data such as images, sequential data consists of consecutive data frames indexed by time, exhibiting rich spatial and temporal dependencies. These dependencies represent the underlying dynamic model and are critical to validate the generated data. In this paper, we make the first theoretical step towards bridging diffusion transformers for capturing spatial-temporal dependencies. Specifically, we establish score approximation and distribution estimation guarantees of diffusion transformers for learning Gaussian process data with covariance functions of various decay patterns. We highlight how the spatial-temporal dependencies are captured and affect learning efficiency. Our study proposes a novel transformer approximation theory, where the transformer acts to unroll an algorithm. We support our theoretical results by numerical experiments, providing strong evidence that spatial-temporal dependencies are captured within attention layers, aligning with our approximation theory.",
    "arxiv_url": "http://arxiv.org/abs/2407.16134v2",
    "pdf_url": "http://arxiv.org/pdf/2407.16134v2",
    "published_date": "2024-07-23",
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Anchored Diffusion for Video Face Reenactment",
    "authors": [
      "Idan Kligvasser",
      "Regev Cohen",
      "George Leifman",
      "Ehud Rivlin",
      "Michael Elad"
    ],
    "abstract": "Video generation has drawn significant interest recently, pushing the development of large-scale models capable of producing realistic videos with coherent motion. Due to memory constraints, these models typically generate short video segments that are then combined into long videos. The merging process poses a significant challenge, as it requires ensuring smooth transitions and overall consistency. In this paper, we introduce Anchored Diffusion, a novel method for synthesizing relatively long and seamless videos. We extend Diffusion Transformers (DiTs) to incorporate temporal information, creating our sequence-DiT (sDiT) model for generating short video segments. Unlike previous works, we train our model on video sequences with random non-uniform temporal spacing and incorporate temporal information via external guidance, increasing flexibility and allowing it to capture both short and long-term relationships. Furthermore, during inference, we leverage the transformer architecture to modify the diffusion process, generating a batch of non-uniform sequences anchored to a common frame, ensuring consistency regardless of temporal distance. To demonstrate our method, we focus on face reenactment, the task of creating a video from a source image that replicates the facial expressions and movements from a driving video. Through comprehensive experiments, we show our approach outperforms current techniques in producing longer consistent high-quality videos while offering editing capabilities.",
    "arxiv_url": "http://arxiv.org/abs/2407.15153v1",
    "pdf_url": "http://arxiv.org/pdf/2407.15153v1",
    "published_date": "2024-07-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control",
    "authors": [
      "Sherwin Bahmani",
      "Ivan Skorokhodov",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Guocheng Qian",
      "Michael Vasilkovsky",
      "Hsin-Ying Lee",
      "Chaoyang Wang",
      "Jiaxu Zou",
      "Andrea Tagliasacchi",
      "David B. Lindell",
      "Sergey Tulyakov"
    ],
    "abstract": "Modern text-to-video synthesis models demonstrate coherent, photorealistic generation of complex videos from a text description. However, most existing models lack fine-grained control over camera movement, which is critical for downstream applications related to content creation, visual effects, and 3D vision. Recently, new methods demonstrate the ability to generate videos with controllable camera poses these techniques leverage pre-trained U-Net-based diffusion models that explicitly disentangle spatial and temporal generation. Still, no existing approach enables camera control for new, transformer-based video diffusion models that process spatial and temporal information jointly. Here, we propose to tame video transformers for 3D camera control using a ControlNet-like conditioning mechanism that incorporates spatiotemporal camera embeddings based on Pl\\\"ucker coordinates. The approach demonstrates state-of-the-art performance for controllable video generation after fine-tuning on the RealEstate10K dataset. To the best of our knowledge, our work is the first to enable camera control for transformer-based video diffusion models.",
    "arxiv_url": "http://arxiv.org/abs/2407.12781v3",
    "pdf_url": "http://arxiv.org/pdf/2407.12781v3",
    "published_date": "2024-07-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation",
      "Controllable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scaling Diffusion Transformers to 16 Billion Parameters",
    "authors": [
      "Zhengcong Fei",
      "Mingyuan Fan",
      "Changqian Yu",
      "Debang Li",
      "Junshi Huang"
    ],
    "abstract": "In this paper, we present DiT-MoE, a sparse version of the diffusion Transformer, that is scalable and competitive with dense networks while exhibiting highly optimized inference. The DiT-MoE includes two simple designs: shared expert routing and expert-level balance loss, thereby capturing common knowledge and reducing redundancy among the different routed experts. When applied to conditional image generation, a deep analysis of experts specialization gains some interesting observations: (i) Expert selection shows preference with spatial position and denoising time step, while insensitive with different class-conditional information; (ii) As the MoE layers go deeper, the selection of experts gradually shifts from specific spacial position to dispersion and balance. (iii) Expert specialization tends to be more concentrated at the early time step and then gradually uniform after half. We attribute it to the diffusion process that first models the low-frequency spatial information and then high-frequency complex information. Based on the above guidance, a series of DiT-MoE experimentally achieves performance on par with dense networks yet requires much less computational load during inference. More encouragingly, we demonstrate the potential of DiT-MoE with synthesized image data, scaling diffusion model at a 16.5B parameter that attains a new SoTA FID-50K score of 1.80 in 512$\\times$512 resolution settings. The project page: https://github.com/feizc/DiT-MoE.",
    "arxiv_url": "http://arxiv.org/abs/2407.11633v3",
    "pdf_url": "http://arxiv.org/pdf/2407.11633v3",
    "published_date": "2024-07-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/feizc/DiT-MoE",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation",
    "authors": [
      "Kepan Nan",
      "Rui Xie",
      "Penghao Zhou",
      "Tiehan Fan",
      "Zhenheng Yang",
      "Zhijie Chen",
      "Xiang Li",
      "Jian Yang",
      "Ying Tai"
    ],
    "abstract": "Text-to-video (T2V) generation has recently garnered significant attention thanks to the large multi-modality model Sora. However, T2V generation still faces two important challenges: 1) Lacking a precise open sourced high-quality dataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M, are either with low quality or too large for most research institutions. Therefore, it is challenging but crucial to collect a precise high-quality text-video pairs for T2V generation. 2) Ignoring to fully utilize textual information. Recent T2V methods have focused on vision transformers, using a simple cross attention module for video generation, which falls short of thoroughly extracting semantic information from text prompt. To address these issues, we introduce OpenVid-1M, a precise high-quality dataset with expressive captions. This open-scenario dataset contains over 1 million text-video pairs, facilitating research on T2V generation. Furthermore, we curate 433K 1080p videos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definition video generation. Additionally, we propose a novel Multi-modal Video Diffusion Transformer (MVDiT) capable of mining both structure information from visual tokens and semantic information from text tokens. Extensive experiments and ablation studies verify the superiority of OpenVid-1M over previous datasets and the effectiveness of our MVDiT.",
    "arxiv_url": "http://arxiv.org/abs/2407.02371v3",
    "pdf_url": "http://arxiv.org/pdf/2407.02371v3",
    "published_date": "2024-07-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers",
    "authors": [
      "Lei Chen",
      "Yuan Meng",
      "Chen Tang",
      "Xinzhu Ma",
      "Jingyan Jiang",
      "Xin Wang",
      "Zhi Wang",
      "Wenwu Zhu"
    ],
    "abstract": "Recent advancements in diffusion models, particularly the architectural transformation from UNet-based models to Diffusion Transformers (DiTs), significantly improve the quality and scalability of image and video generation. However, despite their impressive capabilities, the substantial computational costs of these large-scale models pose significant challenges for real-world deployment. Post-Training Quantization (PTQ) emerges as a promising solution, enabling model compression and accelerated inference for pretrained models, without the costly retraining. However, research on DiT quantization remains sparse, and existing PTQ frameworks, primarily designed for traditional diffusion models, tend to suffer from biased quantization, leading to notable performance degradation. In this work, we identify that DiTs typically exhibit significant spatial variance in both weights and activations, along with temporal variance in activations. To address these issues, we propose Q-DiT, a novel approach that seamlessly integrates two key techniques: automatic quantization granularity allocation to handle the significant variance of weights and activations across input channels, and sample-wise dynamic activation quantization to adaptively capture activation changes across both timesteps and samples. Extensive experiments conducted on ImageNet and VBench demonstrate the effectiveness of the proposed Q-DiT. Specifically, when quantizing DiT-XL/2 to W6A8 on ImageNet ($256 \\times 256$), Q-DiT achieves a remarkable reduction in FID by 1.09 compared to the baseline. Under the more challenging W4A8 setting, it maintains high fidelity in image and video generation, establishing a new benchmark for efficient, high-quality quantization in DiTs. Code is available at \\href{https://github.com/Juanerx/Q-DiT}{https://github.com/Juanerx/Q-DiT}.",
    "arxiv_url": "http://arxiv.org/abs/2406.17343v2",
    "pdf_url": "http://arxiv.org/pdf/2406.17343v2",
    "published_date": "2024-06-25",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/Juanerx/Q-DiT",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models",
    "authors": [
      "Bingqi Ma",
      "Zhuofan Zong",
      "Guanglu Song",
      "Hongsheng Li",
      "Yu Liu"
    ],
    "abstract": "Large language models (LLMs) based on decoder-only transformers have demonstrated superior text understanding capabilities compared to CLIP and T5-series models. However, the paradigm for utilizing current advanced LLMs in text-to-image diffusion models remains to be explored. We observed an unusual phenomenon: directly using a large language model as the prompt encoder significantly degrades the prompt-following ability in image generation. We identified two main obstacles behind this issue. One is the misalignment between the next token prediction training in LLM and the requirement for discriminative prompt features in diffusion models. The other is the intrinsic positional bias introduced by the decoder-only architecture. To deal with this issue, we propose a novel framework to fully harness the capabilities of LLMs. Through the carefully designed usage guidance, we effectively enhance the text representation capability for prompt encoding and eliminate its inherent positional bias. This allows us to integrate state-of-the-art LLMs into the text-to-image generation model flexibly. Furthermore, we also provide an effective manner to fuse multiple LLMs into our framework. Considering the excellent performance and scaling capabilities demonstrated by the transformer architecture, we further design an LLM-Infused Diffusion Transformer (LI-DiT) based on the framework. We conduct extensive experiments to validate LI-DiT across model size and data size. Benefiting from the inherent ability of the LLMs and our innovative designs, the prompt understanding performance of LI-DiT easily surpasses state-of-the-art open-source models as well as mainstream closed-source commercial models including Stable Diffusion 3, DALL-E 3, and Midjourney V6. The LLM-Infused Diffuser framework is also one of the core technologies powering SenseMirage, a highly advanced text-to-image model.",
    "arxiv_url": "http://arxiv.org/abs/2406.11831v3",
    "pdf_url": "http://arxiv.org/pdf/2406.11831v3",
    "published_date": "2024-06-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "An Analysis on Quantizing Diffusion Transformers",
    "authors": [
      "Yuewei Yang",
      "Jialiang Wang",
      "Xiaoliang Dai",
      "Peizhao Zhang",
      "Hongbo Zhang"
    ],
    "abstract": "Diffusion Models (DMs) utilize an iterative denoising process to transform random noise into synthetic data. Initally proposed with a UNet structure, DMs excel at producing images that are virtually indistinguishable with or without conditioned text prompts. Later transformer-only structure is composed with DMs to achieve better performance. Though Latent Diffusion Models (LDMs) reduce the computational requirement by denoising in a latent space, it is extremely expensive to inference images for any operating devices due to the shear volume of parameters and feature sizes. Post Training Quantization (PTQ) offers an immediate remedy for a smaller storage size and more memory-efficient computation during inferencing. Prior works address PTQ of DMs on UNet structures have addressed the challenges in calibrating parameters for both activations and weights via moderate optimization. In this work, we pioneer an efficient PTQ on transformer-only structure without any optimization. By analysing challenges in quantizing activations and weights for diffusion transformers, we propose a single-step sampling calibration on activations and adapt group-wise quantization on weights for low-bit quantization. We demonstrate the efficiency and effectiveness of proposed methods with preliminary experiments on conditional image generation.",
    "arxiv_url": "http://arxiv.org/abs/2406.11100v1",
    "pdf_url": "http://arxiv.org/pdf/2406.11100v1",
    "published_date": "2024-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Complex Image-Generative Diffusion Transformer for Audio Denoising",
    "authors": [
      "Junhui Li",
      "Pu Wang",
      "Jialu Li",
      "Youshan Zhang"
    ],
    "abstract": "The audio denoising technique has captured widespread attention in the deep neural network field. Recently, the audio denoising problem has been converted into an image generation task, and deep learning-based approaches have been applied to tackle this problem. However, its performance is still limited, leaving room for further improvement. In order to enhance audio denoising performance, this paper introduces a complex image-generative diffusion transformer that captures more information from the complex Fourier domain. We explore a novel diffusion transformer by integrating the transformer with a diffusion model. Our proposed model demonstrates the scalability of the transformer and expands the receptive field of sparse attention using attention diffusion. Our work is among the first to utilize diffusion transformers to deal with the image generation task for audio denoising. Extensive experiments on two benchmark datasets demonstrate that our proposed model outperforms state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2406.09161v1",
    "pdf_url": "http://arxiv.org/pdf/2406.09161v1",
    "published_date": "2024-06-13",
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiTFastAttn: Attention Compression for Diffusion Transformer Models",
    "authors": [
      "Zhihang Yuan",
      "Hanling Zhang",
      "Pu Lu",
      "Xuefei Ning",
      "Linfeng Zhang",
      "Tianchen Zhao",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ],
    "abstract": "Diffusion Transformers (DiT) excel at image and video generation but face computational challenges due to the quadratic complexity of self-attention operators. We propose DiTFastAttn, a post-training compression method to alleviate the computational bottleneck of DiT. We identify three key redundancies in the attention computation during DiT inference: (1) spatial redundancy, where many attention heads focus on local information; (2) temporal redundancy, with high similarity between the attention outputs of neighboring steps; (3) conditional redundancy, where conditional and unconditional inferences exhibit significant similarity. We propose three techniques to reduce these redundancies: (1) Window Attention with Residual Sharing to reduce spatial redundancy; (2) Attention Sharing across Timesteps to exploit the similarity between steps; (3) Attention Sharing across CFG to skip redundant computations during conditional generation. We apply DiTFastAttn to DiT, PixArt-Sigma for image generation tasks, and OpenSora for video generation tasks. Our results show that for image generation, our method reduces up to 76% of the attention FLOPs and achieves up to 1.8x end-to-end speedup at high-resolution (2k x 2k) generation.",
    "arxiv_url": "http://arxiv.org/abs/2406.08552v2",
    "pdf_url": "http://arxiv.org/pdf/2406.08552v2",
    "published_date": "2024-06-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "What If We Recaption Billions of Web Images with LLaMA-3?",
    "authors": [
      "Xianhang Li",
      "Haoqin Tu",
      "Mude Hui",
      "Zeyu Wang",
      "Bingchen Zhao",
      "Junfei Xiao",
      "Sucheng Ren",
      "Jieru Mei",
      "Qing Liu",
      "Huangjie Zheng",
      "Yuyin Zhou",
      "Cihang Xie"
    ],
    "abstract": "Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate that semantically aligning and enriching textual descriptions of these pairs can significantly enhance model training across various vision-language tasks, particularly text-to-image generation. However, large-scale investigations in this area remain predominantly closed-source. Our paper aims to bridge this community effort, leveraging the powerful and \\textit{open-sourced} LLaMA-3, a GPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images from the DataComp-1B dataset. Our empirical results confirm that this enhanced dataset, Recap-DataComp-1B, offers substantial benefits in training advanced vision-language models. For discriminative models like CLIP, we observe enhanced zero-shot performance in cross-modal retrieval tasks. For generative models like text-to-image Diffusion Transformers, the generated images exhibit a significant improvement in alignment with users' text instructions, especially in following complex queries. Our project page is https://www.haqtu.me/Recap-Datacomp-1B/",
    "arxiv_url": "http://arxiv.org/abs/2406.08478v2",
    "pdf_url": "http://arxiv.org/pdf/2406.08478v2",
    "published_date": "2024-06-12",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation",
    "authors": [
      "Kai Wang",
      "Shijian Deng",
      "Jing Shi",
      "Dimitrios Hatzinakos",
      "Yapeng Tian"
    ],
    "abstract": "Recent Diffusion Transformers (DiTs) have shown impressive capabilities in generating high-quality single-modality content, including images, videos, and audio. However, it is still under-explored whether the transformer-based diffuser can efficiently denoise the Gaussian noises towards superb multimodal content creation. To bridge this gap, we introduce AV-DiT, a novel and efficient audio-visual diffusion transformer designed to generate high-quality, realistic videos with both visual and audio tracks. To minimize model complexity and computational costs, AV-DiT utilizes a shared DiT backbone pre-trained on image-only data, with only lightweight, newly inserted adapters being trainable. This shared backbone facilitates both audio and video generation. Specifically, the video branch incorporates a trainable temporal attention layer into a frozen pre-trained DiT block for temporal consistency. Additionally, a small number of trainable parameters adapt the image-based DiT block for audio generation. An extra shared DiT block, equipped with lightweight parameters, facilitates feature interaction between audio and visual modalities, ensuring alignment. Extensive experiments on the AIST++ and Landscape datasets demonstrate that AV-DiT achieves state-of-the-art performance in joint audio-visual generation with significantly fewer tunable parameters. Furthermore, our results highlight that a single shared image generative backbone with modality-specific adaptations is sufficient for constructing a joint audio-video generator. Our source code and pre-trained models will be released.",
    "arxiv_url": "http://arxiv.org/abs/2406.07686v1",
    "pdf_url": "http://arxiv.org/pdf/2406.07686v1",
    "published_date": "2024-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lumina-Next: Making Lumina-T2X Stronger and Faster with Next-DiT",
    "authors": [
      "Le Zhuo",
      "Ruoyi Du",
      "Han Xiao",
      "Yangguang Li",
      "Dongyang Liu",
      "Rongjie Huang",
      "Wenze Liu",
      "Lirui Zhao",
      "Fu-Yun Wang",
      "Zhanyu Ma",
      "Xu Luo",
      "Zehan Wang",
      "Kaipeng Zhang",
      "Xiangyang Zhu",
      "Si Liu",
      "Xiangyu Yue",
      "Dingning Liu",
      "Wanli Ouyang",
      "Ziwei Liu",
      "Yu Qiao",
      "Hongsheng Li",
      "Peng Gao"
    ],
    "abstract": "Lumina-T2X is a nascent family of Flow-based Large Diffusion Transformers that establishes a unified framework for transforming noise into various modalities, such as images and videos, conditioned on text instructions. Despite its promising capabilities, Lumina-T2X still encounters challenges including training instability, slow inference, and extrapolation artifacts. In this paper, we present Lumina-Next, an improved version of Lumina-T2X, showcasing stronger generation performance with increased training and inference efficiency. We begin with a comprehensive analysis of the Flag-DiT architecture and identify several suboptimal components, which we address by introducing the Next-DiT architecture with 3D RoPE and sandwich normalizations. To enable better resolution extrapolation, we thoroughly compare different context extrapolation methods applied to text-to-image generation with 3D RoPE, and propose Frequency- and Time-Aware Scaled RoPE tailored for diffusion transformers. Additionally, we introduced a sigmoid time discretization schedule to reduce sampling steps in solving the Flow ODE and the Context Drop method to merge redundant visual tokens for faster network evaluation, effectively boosting the overall sampling speed. Thanks to these improvements, Lumina-Next not only improves the quality and efficiency of basic text-to-image generation but also demonstrates superior resolution extrapolation capabilities and multilingual generation using decoder-based LLMs as the text encoder, all in a zero-shot manner. To further validate Lumina-Next as a versatile generative framework, we instantiate it on diverse tasks including visual recognition, multi-view, audio, music, and point cloud generation, showcasing strong performance across these domains. By releasing all codes and model weights, we aim to advance the development of next-generation generative AI capable of universal modeling.",
    "arxiv_url": "http://arxiv.org/abs/2406.18583v1",
    "pdf_url": "http://arxiv.org/pdf/2406.18583v1",
    "published_date": "2024-06-05",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation",
    "authors": [
      "Tianchen Zhao",
      "Tongcheng Fang",
      "Haofeng Huang",
      "Enshu Liu",
      "Rui Wan",
      "Widyadewi Soedarmadji",
      "Shiyao Li",
      "Zinan Lin",
      "Guohao Dai",
      "Shengen Yan",
      "Huazhong Yang",
      "Xuefei Ning",
      "Yu Wang"
    ],
    "abstract": "Diffusion transformers have demonstrated remarkable performance in visual generation tasks, such as generating realistic images or videos based on textual instructions. However, larger model sizes and multi-frame processing for video generation lead to increased computational and memory costs, posing challenges for practical deployment on edge devices. Post-Training Quantization (PTQ) is an effective method for reducing memory costs and computational complexity. When quantizing diffusion transformers, we find that existing quantization methods face challenges when applied to text-to-image and video tasks. To address these challenges, we begin by systematically analyzing the source of quantization error and conclude with the unique challenges posed by DiT quantization. Accordingly, we design an improved quantization scheme: ViDiT-Q (Video & Image Diffusion Transformer Quantization), tailored specifically for DiT models. We validate the effectiveness of ViDiT-Q across a variety of text-to-image and video models, achieving W8A8 and W4A8 with negligible degradation in visual quality and metrics. Additionally, we implement efficient GPU kernels to achieve practical 2-2.5x memory saving and a 1.4-1.7x end-to-end latency speedup.",
    "arxiv_url": "http://arxiv.org/abs/2406.02540v3",
    "pdf_url": "http://arxiv.org/pdf/2406.02540v3",
    "published_date": "2024-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "$Δ$-DiT: A Training-Free Acceleration Method Tailored for Diffusion Transformers",
    "authors": [
      "Pengtao Chen",
      "Mingzhu Shen",
      "Peng Ye",
      "Jianjian Cao",
      "Chongjun Tu",
      "Christos-Savvas Bouganis",
      "Yiren Zhao",
      "Tao Chen"
    ],
    "abstract": "Diffusion models are widely recognized for generating high-quality and diverse images, but their poor real-time performance has led to numerous acceleration works, primarily focusing on UNet-based structures. With the more successful results achieved by diffusion transformers (DiT), there is still a lack of exploration regarding the impact of DiT structure on generation, as well as the absence of an acceleration framework tailored to the DiT architecture. To tackle these challenges, we conduct an investigation into the correlation between DiT blocks and image generation. Our findings reveal that the front blocks of DiT are associated with the outline of the generated images, while the rear blocks are linked to the details. Based on this insight, we propose an overall training-free inference acceleration framework $\\Delta$-DiT: using a designed cache mechanism to accelerate the rear DiT blocks in the early sampling stages and the front DiT blocks in the later stages. Specifically, a DiT-specific cache mechanism called $\\Delta$-Cache is proposed, which considers the inputs of the previous sampling image and reduces the bias in the inference. Extensive experiments on PIXART-$\\alpha$ and DiT-XL demonstrate that the $\\Delta$-DiT can achieve a $1.6\\times$ speedup on the 20-step generation and even improves performance in most cases. In the scenario of 4-step consistent model generation and the more challenging $1.12\\times$ acceleration, our method significantly outperforms existing methods. Our code will be publicly available.",
    "arxiv_url": "http://arxiv.org/abs/2406.01125v1",
    "pdf_url": "http://arxiv.org/pdf/2406.01125v1",
    "published_date": "2024-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VITON-DiT: Learning In-the-Wild Video Try-On from Human Dance Videos via Diffusion Transformers",
    "authors": [
      "Jun Zheng",
      "Fuwei Zhao",
      "Youjiang Xu",
      "Xin Dong",
      "Xiaodan Liang"
    ],
    "abstract": "Video try-on stands as a promising area for its tremendous real-world potential. Prior works are limited to transferring product clothing images onto person videos with simple poses and backgrounds, while underperforming on casually captured videos. Recently, Sora revealed the scalability of Diffusion Transformer (DiT) in generating lifelike videos featuring real-world scenarios. Inspired by this, we explore and propose the first DiT-based video try-on framework for practical in-the-wild applications, named VITON-DiT. Specifically, VITON-DiT consists of a garment extractor, a Spatial-Temporal denoising DiT, and an identity preservation ControlNet. To faithfully recover the clothing details, the extracted garment features are fused with the self-attention outputs of the denoising DiT and the ControlNet. We also introduce novel random selection strategies during training and an Interpolated Auto-Regressive (IAR) technique at inference to facilitate long video generation. Unlike existing attempts that require the laborious and restrictive construction of a paired training dataset, severely limiting their scalability, VITON-DiT alleviates this by relying solely on unpaired human dance videos and a carefully designed multi-stage training strategy. Furthermore, we curate a challenging benchmark dataset to evaluate the performance of casual video try-on. Extensive experiments demonstrate the superiority of VITON-DiT in generating spatio-temporal consistent try-on results for in-the-wild videos with complicated human poses.",
    "arxiv_url": "http://arxiv.org/abs/2405.18326v2",
    "pdf_url": "http://arxiv.org/pdf/2405.18326v2",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Human4DiT: 360-degree Human Video Generation with 4D Diffusion Transformer",
    "authors": [
      "Ruizhi Shao",
      "Youxin Pang",
      "Zerong Zheng",
      "Jingxiang Sun",
      "Yebin Liu"
    ],
    "abstract": "We present a novel approach for generating 360-degree high-quality, spatio-temporally coherent human videos from a single image. Our framework combines the strengths of diffusion transformers for capturing global correlations across viewpoints and time, and CNNs for accurate condition injection. The core is a hierarchical 4D transformer architecture that factorizes self-attention across views, time steps, and spatial dimensions, enabling efficient modeling of the 4D space. Precise conditioning is achieved by injecting human identity, camera parameters, and temporal signals into the respective transformers. To train this model, we collect a multi-dimensional dataset spanning images, videos, multi-view data, and limited 4D footage, along with a tailored multi-dimensional training strategy. Our approach overcomes the limitations of previous methods based on generative adversarial networks or vanilla diffusion models, which struggle with complex motions, viewpoint changes, and generalization. Through extensive experiments, we demonstrate our method's ability to synthesize 360-degree realistic, coherent human motion videos, paving the way for advanced multimedia applications in areas such as virtual reality and animation.",
    "arxiv_url": "http://arxiv.org/abs/2405.17405v2",
    "pdf_url": "http://arxiv.org/pdf/2405.17405v2",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PTQ4DiT: Post-training Quantization for Diffusion Transformers",
    "authors": [
      "Junyi Wu",
      "Haoxuan Wang",
      "Yuzhang Shang",
      "Mubarak Shah",
      "Yan Yan"
    ],
    "abstract": "The recent introduction of Diffusion Transformers (DiTs) has demonstrated exceptional capabilities in image generation by using a different backbone architecture, departing from traditional U-Nets and embracing the scalable nature of transformers. Despite their advanced capabilities, the wide deployment of DiTs, particularly for real-time applications, is currently hampered by considerable computational demands at the inference stage. Post-training Quantization (PTQ) has emerged as a fast and data-efficient solution that can significantly reduce computation and memory footprint by using low-bit weights and activations. However, its applicability to DiTs has not yet been explored and faces non-trivial difficulties due to the unique design of DiTs. In this paper, we propose PTQ4DiT, a specifically designed PTQ method for DiTs. We discover two primary quantization challenges inherent in DiTs, notably the presence of salient channels with extreme magnitudes and the temporal variability in distributions of salient activation over multiple timesteps. To tackle these challenges, we propose Channel-wise Salience Balancing (CSB) and Spearmen's $\\rho$-guided Salience Calibration (SSC). CSB leverages the complementarity property of channel magnitudes to redistribute the extremes, alleviating quantization errors for both activations and weights. SSC extends this approach by dynamically adjusting the balanced salience to capture the temporal variations in activation. Additionally, to eliminate extra computational costs caused by PTQ4DiT during inference, we design an offline re-parameterization strategy for DiTs. Experiments demonstrate that our PTQ4DiT successfully quantizes DiTs to 8-bit precision (W8A8) while preserving comparable generation ability and further enables effective quantization to 4-bit weight precision (W4A8) for the first time.",
    "arxiv_url": "http://arxiv.org/abs/2405.16005v3",
    "pdf_url": "http://arxiv.org/pdf/2405.16005v3",
    "published_date": "2024-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scaling Diffusion Mamba with Bidirectional SSMs for Efficient Image and Video Generation",
    "authors": [
      "Shentong Mo",
      "Yapeng Tian"
    ],
    "abstract": "In recent developments, the Mamba architecture, known for its selective state space approach, has shown potential in the efficient modeling of long sequences. However, its application in image generation remains underexplored. Traditional diffusion transformers (DiT), which utilize self-attention blocks, are effective but their computational complexity scales quadratically with the input length, limiting their use for high-resolution images. To address this challenge, we introduce a novel diffusion architecture, Diffusion Mamba (DiM), which foregoes traditional attention mechanisms in favor of a scalable alternative. By harnessing the inherent efficiency of the Mamba architecture, DiM achieves rapid inference times and reduced computational load, maintaining linear complexity with respect to sequence length. Our architecture not only scales effectively but also outperforms existing diffusion transformers in both image and video generation tasks. The results affirm the scalability and efficiency of DiM, establishing a new benchmark for image and video generation techniques. This work advances the field of generative models and paves the way for further applications of scalable architectures.",
    "arxiv_url": "http://arxiv.org/abs/2405.15881v1",
    "pdf_url": "http://arxiv.org/pdf/2405.15881v1",
    "published_date": "2024-05-24",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TerDiT: Ternary Diffusion Models with Transformers",
    "authors": [
      "Xudong Lu",
      "Aojun Zhou",
      "Ziyi Lin",
      "Qi Liu",
      "Yuhui Xu",
      "Renrui Zhang",
      "Xue Yang",
      "Junchi Yan",
      "Peng Gao",
      "Hongsheng Li"
    ],
    "abstract": "Recent developments in large-scale pre-trained text-to-image diffusion models have significantly improved the generation of high-fidelity images, particularly with the emergence of diffusion transformer models (DiTs). Among diffusion models, diffusion transformers have demonstrated superior image-generation capabilities, boosting lower FID scores and higher scalability. However, deploying large-scale DiT models can be expensive due to their excessive parameter numbers. Although existing research has explored efficient deployment techniques for diffusion models, such as model quantization, there is still little work concerning DiT-based models. To tackle this research gap, we propose TerDiT, the first quantization-aware training (QAT) and efficient deployment scheme for extremely low-bit diffusion transformer models. We focus on the ternarization of DiT networks, with model sizes ranging from 600M to 4.2B, and image resolution from 256$\\times$256 to 512$\\times$512. Our work contributes to the exploration of efficient deployment of large-scale DiT models, demonstrating the feasibility of training extremely low-bit DiT models from scratch while maintaining competitive image generation capacities compared to full-precision models. Our code and pre-trained TerDiT checkpoints have been released at https://github.com/Lucky-Lance/TerDiT.",
    "arxiv_url": "http://arxiv.org/abs/2405.14854v2",
    "pdf_url": "http://arxiv.org/pdf/2405.14854v2",
    "published_date": "2024-05-23",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "https://github.com/Lucky-Lance/TerDiT",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding",
    "authors": [
      "Zhimin Li",
      "Jianwei Zhang",
      "Qin Lin",
      "Jiangfeng Xiong",
      "Yanxin Long",
      "Xinchi Deng",
      "Yingfang Zhang",
      "Xingchao Liu",
      "Minbin Huang",
      "Zedong Xiao",
      "Dayou Chen",
      "Jiajun He",
      "Jiahao Li",
      "Wenyue Li",
      "Chen Zhang",
      "Rongwei Quan",
      "Jianxiang Lu",
      "Jiabin Huang",
      "Xiaoyan Yuan",
      "Xiaoxiao Zheng",
      "Yixuan Li",
      "Jihong Zhang",
      "Chao Zhang",
      "Meng Chen",
      "Jie Liu",
      "Zheng Fang",
      "Weiyan Wang",
      "Jinbao Xue",
      "Yangyu Tao",
      "Jianchen Zhu",
      "Kai Liu",
      "Sihuan Lin",
      "Yifu Sun",
      "Yun Li",
      "Dongdong Wang",
      "Mingtao Chen",
      "Zhichao Hu",
      "Xiao Xiao",
      "Yan Chen",
      "Yuhong Liu",
      "Wei Liu",
      "Di Wang",
      "Yong Yang",
      "Jie Jiang",
      "Qinglin Lu"
    ],
    "abstract": "We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT",
    "arxiv_url": "http://arxiv.org/abs/2405.08748v1",
    "pdf_url": "http://arxiv.org/pdf/2405.08748v1",
    "published_date": "2024-05-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Tencent/HunyuanDiT",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Inf-DiT: Upsampling Any-Resolution Image with Memory-Efficient Diffusion Transformer",
    "authors": [
      "Zhuoyi Yang",
      "Heyang Jiang",
      "Wenyi Hong",
      "Jiayan Teng",
      "Wendi Zheng",
      "Yuxiao Dong",
      "Ming Ding",
      "Jie Tang"
    ],
    "abstract": "Diffusion models have shown remarkable performance in image generation in recent years. However, due to a quadratic increase in memory during generating ultra-high-resolution images (e.g. 4096*4096), the resolution of generated images is often limited to 1024*1024. In this work. we propose a unidirectional block attention mechanism that can adaptively adjust the memory overhead during the inference process and handle global dependencies. Building on this module, we adopt the DiT structure for upsampling and develop an infinite super-resolution model capable of upsampling images of various shapes and resolutions. Comprehensive experiments show that our model achieves SOTA performance in generating ultra-high-resolution images in both machine and human evaluation. Compared to commonly used UNet structures, our model can save more than 5x memory when generating 4096*4096 images. The project URL is https://github.com/THUDM/Inf-DiT.",
    "arxiv_url": "http://arxiv.org/abs/2405.04312v2",
    "pdf_url": "http://arxiv.org/pdf/2405.04312v2",
    "published_date": "2024-05-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/THUDM/Inf-DiT",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers",
    "authors": [
      "Yuchuan Tian",
      "Zhijun Tu",
      "Hanting Chen",
      "Jie Hu",
      "Chao Xu",
      "Yunhe Wang"
    ],
    "abstract": "Diffusion Transformers (DiTs) introduce the transformer architecture to diffusion tasks for latent-space image generation. With an isotropic architecture that chains a series of transformer blocks, DiTs demonstrate competitive performance and good scalability; but meanwhile, the abandonment of U-Net by DiTs and their following improvements is worth rethinking. To this end, we conduct a simple toy experiment by comparing a U-Net architectured DiT with an isotropic one. It turns out that the U-Net architecture only gain a slight advantage amid the U-Net inductive bias, indicating potential redundancies within the U-Net-style DiT. Inspired by the discovery that U-Net backbone features are low-frequency-dominated, we perform token downsampling on the query-key-value tuple for self-attention that bring further improvements despite a considerable amount of reduction in computation. Based on self-attention with downsampled tokens, we propose a series of U-shaped DiTs (U-DiTs) in the paper and conduct extensive experiments to demonstrate the extraordinary performance of U-DiT models. The proposed U-DiT could outperform DiT-XL/2 with only 1/6 of its computation cost. Codes are available at https://github.com/YuchuanTian/U-DiT.",
    "arxiv_url": "http://arxiv.org/abs/2405.02730v3",
    "pdf_url": "http://arxiv.org/pdf/2405.02730v3",
    "published_date": "2024-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/YuchuanTian/U-DiT",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lazy Diffusion Transformer for Interactive Image Editing",
    "authors": [
      "Yotam Nitzan",
      "Zongze Wu",
      "Richard Zhang",
      "Eli Shechtman",
      "Daniel Cohen-Or",
      "Taesung Park",
      "Michaël Gharbi"
    ],
    "abstract": "We introduce a novel diffusion transformer, LazyDiffusion, that generates partial image updates efficiently. Our approach targets interactive image editing applications in which, starting from a blank canvas or an image, a user specifies a sequence of localized image modifications using binary masks and text prompts. Our generator operates in two phases. First, a context encoder processes the current canvas and user mask to produce a compact global context tailored to the region to generate. Second, conditioned on this context, a diffusion-based transformer decoder synthesizes the masked pixels in a \"lazy\" fashion, i.e., it only generates the masked region. This contrasts with previous works that either regenerate the full canvas, wasting time and computation, or confine processing to a tight rectangular crop around the mask, ignoring the global image context altogether. Our decoder's runtime scales with the mask size, which is typically small, while our encoder introduces negligible overhead. We demonstrate that our approach is competitive with state-of-the-art inpainting methods in terms of quality and fidelity while providing a 10x speedup for typical user interactions, where the editing mask represents 10% of the image.",
    "arxiv_url": "http://arxiv.org/abs/2404.12382v1",
    "pdf_url": "http://arxiv.org/pdf/2404.12382v1",
    "published_date": "2024-04-18",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "image editing",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffscaler: Enhancing the Generative Prowess of Diffusion Transformers",
    "authors": [
      "Nithin Gopalakrishnan Nair",
      "Jeya Maria Jose Valanarasu",
      "Vishal M. Patel"
    ],
    "abstract": "Recently, diffusion transformers have gained wide attention with its excellent performance in text-to-image and text-to-vidoe models, emphasizing the need for transformers as backbone for diffusion models. Transformer-based models have shown better generalization capability compared to CNN-based models for general vision tasks. However, much less has been explored in the existing literature regarding the capabilities of transformer-based diffusion backbones and expanding their generative prowess to other datasets. This paper focuses on enabling a single pre-trained diffusion transformer model to scale across multiple datasets swiftly, allowing for the completion of diverse generative tasks using just one model. To this end, we propose DiffScaler, an efficient scaling strategy for diffusion models where we train a minimal amount of parameters to adapt to different tasks. In particular, we learn task-specific transformations at each layer by incorporating the ability to utilize the learned subspaces of the pre-trained model, as well as the ability to learn additional task-specific subspaces, which may be absent in the pre-training dataset. As these parameters are independent, a single diffusion model with these task-specific parameters can be used to perform multiple tasks simultaneously. Moreover, we find that transformer-based diffusion models significantly outperform CNN-based diffusion models methods while performing fine-tuning over smaller datasets. We perform experiments on four unconditional image generation datasets. We show that using our proposed method, a single pre-trained model can scale up to perform these conditional and unconditional tasks, respectively, with minimal parameter tuning while performing as close as fine-tuning an entire diffusion model for that particular task.",
    "arxiv_url": "http://arxiv.org/abs/2404.09976v1",
    "pdf_url": "http://arxiv.org/pdf/2404.09976v1",
    "published_date": "2024-04-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction",
    "authors": [
      "Keyu Tian",
      "Yi Jiang",
      "Zehuan Yuan",
      "Bingyue Peng",
      "Liwei Wang"
    ],
    "abstract": "We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine \"next-scale prediction\" or \"next-resolution prediction\", diverging from the standard raster-scan \"next-token prediction\". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes GPT-like AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.",
    "arxiv_url": "http://arxiv.org/abs/2404.02905v2",
    "pdf_url": "http://arxiv.org/pdf/2404.02905v2",
    "published_date": "2024-04-03",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Condition-Aware Neural Network for Controlled Image Generation",
    "authors": [
      "Han Cai",
      "Muyang Li",
      "Zhuoyang Zhang",
      "Qinsheng Zhang",
      "Ming-Yu Liu",
      "Song Han"
    ],
    "abstract": "We present Condition-Aware Neural Network (CAN), a new method for adding control to image generative models. In parallel to prior conditional control methods, CAN controls the image generation process by dynamically manipulating the weight of the neural network. This is achieved by introducing a condition-aware weight generation module that generates conditional weight for convolution/linear layers based on the input condition. We test CAN on class-conditional image generation on ImageNet and text-to-image generation on COCO. CAN consistently delivers significant improvements for diffusion transformer models, including DiT and UViT. In particular, CAN combined with EfficientViT (CaT) achieves 2.78 FID on ImageNet 512x512, surpassing DiT-XL/2 while requiring 52x fewer MACs per sampling step.",
    "arxiv_url": "http://arxiv.org/abs/2404.01143v1",
    "pdf_url": "http://arxiv.org/pdf/2404.01143v1",
    "published_date": "2024-04-01",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SD-DiT: Unleashing the Power of Self-supervised Discrimination in Diffusion Transformer",
    "authors": [
      "Rui Zhu",
      "Yingwei Pan",
      "Yehao Li",
      "Ting Yao",
      "Zhenglong Sun",
      "Tao Mei",
      "Chang Wen Chen"
    ],
    "abstract": "Diffusion Transformer (DiT) has emerged as the new trend of generative diffusion models on image generation. In view of extremely slow convergence in typical DiT, recent breakthroughs have been driven by mask strategy that significantly improves the training efficiency of DiT with additional intra-image contextual learning. Despite this progress, mask strategy still suffers from two inherent limitations: (a) training-inference discrepancy and (b) fuzzy relations between mask reconstruction & generative diffusion process, resulting in sub-optimal training of DiT. In this work, we address these limitations by novelly unleashing the self-supervised discrimination knowledge to boost DiT training. Technically, we frame our DiT in a teacher-student manner. The teacher-student discriminative pairs are built on the diffusion noises along the same Probability Flow Ordinary Differential Equation (PF-ODE). Instead of applying mask reconstruction loss over both DiT encoder and decoder, we decouple DiT encoder and decoder to separately tackle discriminative and generative objectives. In particular, by encoding discriminative pairs with student and teacher DiT encoders, a new discriminative loss is designed to encourage the inter-image alignment in the self-supervised embedding space. After that, student samples are fed into student DiT decoder to perform the typical generative diffusion task. Extensive experiments are conducted on ImageNet dataset, and our method achieves a competitive balance between training cost and generative capacity.",
    "arxiv_url": "http://arxiv.org/abs/2403.17004v1",
    "pdf_url": "http://arxiv.org/pdf/2403.17004v1",
    "published_date": "2024-03-25",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation",
    "authors": [
      "Junsong Chen",
      "Chongjian Ge",
      "Enze Xie",
      "Yue Wu",
      "Lewei Yao",
      "Xiaozhe Ren",
      "Zhongdao Wang",
      "Ping Luo",
      "Huchuan Lu",
      "Zhenguo Li"
    ],
    "abstract": "In this paper, we introduce PixArt-\\Sigma, a Diffusion Transformer model~(DiT) capable of directly generating images at 4K resolution. PixArt-\\Sigma represents a significant advancement over its predecessor, PixArt-\\alpha, offering images of markedly higher fidelity and improved alignment with text prompts. A key feature of PixArt-\\Sigma is its training efficiency. Leveraging the foundational pre-training of PixArt-\\alpha, it evolves from the `weaker' baseline to a `stronger' model via incorporating higher quality data, a process we term \"weak-to-strong training\". The advancements in PixArt-\\Sigma are twofold: (1) High-Quality Training Data: PixArt-\\Sigma incorporates superior-quality image data, paired with more precise and detailed image captions. (2) Efficient Token Compression: we propose a novel attention module within the DiT framework that compresses both keys and values, significantly improving efficiency and facilitating ultra-high-resolution image generation. Thanks to these improvements, PixArt-\\Sigma achieves superior image quality and user prompt adherence capabilities with significantly smaller model size (0.6B parameters) than existing text-to-image diffusion models, such as SDXL (2.6B parameters) and SD Cascade (5.1B parameters). Moreover, PixArt-\\Sigma's capability to generate 4K images supports the creation of high-resolution posters and wallpapers, efficiently bolstering the production of high-quality visual content in industries such as film and gaming.",
    "arxiv_url": "http://arxiv.org/abs/2403.04692v2",
    "pdf_url": "http://arxiv.org/pdf/2403.04692v2",
    "published_date": "2024-03-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Structure-Guided Adversarial Training of Diffusion Models",
    "authors": [
      "Ling Yang",
      "Haotian Qian",
      "Zhilong Zhang",
      "Jingwei Liu",
      "Bin Cui"
    ],
    "abstract": "Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transformers (DiT) and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively.",
    "arxiv_url": "http://arxiv.org/abs/2402.17563v2",
    "pdf_url": "http://arxiv.org/pdf/2402.17563v2",
    "published_date": "2024-02-27",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Cross-view Masked Diffusion Transformers for Person Image Synthesis",
    "authors": [
      "Trung X. Pham",
      "Zhang Kang",
      "Chang D. Yoo"
    ],
    "abstract": "We present X-MDPT ($\\underline{Cross}$-view $\\underline{M}$asked $\\underline{D}$iffusion $\\underline{P}$rediction $\\underline{T}$ransformers), a novel diffusion model designed for pose-guided human image generation. X-MDPT distinguishes itself by employing masked diffusion transformers that operate on latent patches, a departure from the commonly-used Unet structures in existing works. The model comprises three key modules: 1) a denoising diffusion Transformer, 2) an aggregation network that consolidates conditions into a single vector for the diffusion process, and 3) a mask cross-prediction module that enhances representation learning with semantic information from the reference image. X-MDPT demonstrates scalability, improving FID, SSIM, and LPIPS with larger models. Despite its simple design, our model outperforms state-of-the-art approaches on the DeepFashion dataset while exhibiting efficiency in terms of training parameters, training time, and inference speed. Our compact 33MB model achieves an FID of 7.42, surpassing a prior Unet latent diffusion approach (FID 8.07) using only $11\\times$ fewer parameters. Our best model surpasses the pixel-based diffusion with $\\frac{2}{3}$ of the parameters and achieves $5.43 \\times$ faster inference. The code is available at https://github.com/trungpx/xmdpt.",
    "arxiv_url": "http://arxiv.org/abs/2402.01516v2",
    "pdf_url": "http://arxiv.org/pdf/2402.01516v2",
    "published_date": "2024-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/trungpx/xmdpt",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers",
    "authors": [
      "Katherine Crowson",
      "Stefan Andreas Baumann",
      "Alex Birch",
      "Tanishq Mathew Abraham",
      "Daniel Z. Kaplan",
      "Enrico Shippole"
    ],
    "abstract": "We present the Hourglass Diffusion Transformer (HDiT), an image generative model that exhibits linear scaling with pixel count, supporting training at high-resolution (e.g. $1024 \\times 1024$) directly in pixel-space. Building on the Transformer architecture, which is known to scale to billions of parameters, it bridges the gap between the efficiency of convolutional U-Nets and the scalability of Transformers. HDiT trains successfully without typical high-resolution training techniques such as multiscale architectures, latent autoencoders or self-conditioning. We demonstrate that HDiT performs competitively with existing models on ImageNet $256^2$, and sets a new state-of-the-art for diffusion models on FFHQ-$1024^2$.",
    "arxiv_url": "http://arxiv.org/abs/2401.11605v1",
    "pdf_url": "http://arxiv.org/pdf/2401.11605v1",
    "published_date": "2024-01-21",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Latte: Latent Diffusion Transformer for Video Generation",
    "authors": [
      "Xin Ma",
      "Yaohui Wang",
      "Xinyuan Chen",
      "Gengyun Jia",
      "Ziwei Liu",
      "Yuan-Fang Li",
      "Cunjian Chen",
      "Yu Qiao"
    ],
    "abstract": "We propose Latte, a novel Latent Diffusion Transformer for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to the text-to-video generation (T2V) task, where Latte achieves results that are competitive with recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation.",
    "arxiv_url": "http://arxiv.org/abs/2401.03048v3",
    "pdf_url": "http://arxiv.org/pdf/2401.03048v3",
    "published_date": "2024-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lightning-Fast Image Inversion and Editing for Text-to-Image Diffusion Models",
    "authors": [
      "Dvir Samuel",
      "Barak Meiri",
      "Haggai Maron",
      "Yoad Tewel",
      "Nir Darshan",
      "Shai Avidan",
      "Gal Chechik",
      "Rami Ben-Ari"
    ],
    "abstract": "Diffusion inversion is the problem of taking an image and a text prompt that describes it and finding a noise latent that would generate the exact same image. Most current deterministic inversion techniques operate by approximately solving an implicit equation and may converge slowly or yield poor reconstructed images. We formulate the problem by finding the roots of an implicit equation and devlop a method to solve it efficiently. Our solution is based on Newton-Raphson (NR), a well-known technique in numerical analysis. We show that a vanilla application of NR is computationally infeasible while naively transforming it to a computationally tractable alternative tends to converge to out-of-distribution solutions, resulting in poor reconstruction and editing. We therefore derive an efficient guided formulation that fastly converges and provides high-quality reconstructions and editing. We showcase our method on real image editing with three popular open-sourced diffusion models: Stable Diffusion, SDXL-Turbo, and Flux with different deterministic schedulers. Our solution, Guided Newton-Raphson Inversion, inverts an image within 0.4 sec (on an A100 GPU) for few-step models (SDXL-Turbo and Flux.1), opening the door for interactive image editing. We further show improved results in image interpolation and generation of rare objects.",
    "arxiv_url": "http://arxiv.org/abs/2312.12540v5",
    "pdf_url": "http://arxiv.org/pdf/2312.12540v5",
    "published_date": "2023-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "inversion",
      "image editing"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GenTron: Diffusion Transformers for Image and Video Generation",
    "authors": [
      "Shoufa Chen",
      "Mengmeng Xu",
      "Jiawei Ren",
      "Yuren Cong",
      "Sen He",
      "Yanping Xie",
      "Animesh Sinha",
      "Ping Luo",
      "Tao Xiang",
      "Juan-Manuel Perez-Rua"
    ],
    "abstract": "In this study, we explore Transformer-based diffusion models for image and video generation. Despite the dominance of Transformer architectures in various fields due to their flexibility and scalability, the visual generative domain primarily utilizes CNN-based U-Net architectures, particularly in diffusion-based models. We introduce GenTron, a family of Generative models employing Transformer-based diffusion, to address this gap. Our initial step was to adapt Diffusion Transformers (DiTs) from class to text conditioning, a process involving thorough empirical exploration of the conditioning mechanism. We then scale GenTron from approximately 900M to over 3B parameters, observing significant improvements in visual quality. Furthermore, we extend GenTron to text-to-video generation, incorporating novel motion-free guidance to enhance video quality. In human evaluations against SDXL, GenTron achieves a 51.1% win rate in visual quality (with a 19.8% draw rate), and a 42.3% win rate in text alignment (with a 42.9% draw rate). GenTron also excels in the T2I-CompBench, underscoring its strengths in compositional generation. We believe this work will provide meaningful insights and serve as a valuable reference for future research.",
    "arxiv_url": "http://arxiv.org/abs/2312.04557v2",
    "pdf_url": "http://arxiv.org/pdf/2312.04557v2",
    "published_date": "2023-12-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PixArt-$α$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis",
    "authors": [
      "Junsong Chen",
      "Jincheng Yu",
      "Chongjian Ge",
      "Lewei Yao",
      "Enze Xie",
      "Yue Wu",
      "Zhongdao Wang",
      "James Kwok",
      "Ping Luo",
      "Huchuan Lu",
      "Zhenguo Li"
    ],
    "abstract": "The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PIXART-$\\alpha$, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024px resolution with low training cost, as shown in Figure 1 and 2. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that separately optimize pixel dependency, text-image alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, PIXART-$\\alpha$'s training speed markedly surpasses existing large-scale T2I models, e.g., PIXART-$\\alpha$ only takes 10.8% of Stable Diffusion v1.5's training time (675 vs. 6,250 A100 GPU days), saving nearly \\$300,000 (\\$26,000 vs. \\$320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%. Extensive experiments demonstrate that PIXART-$\\alpha$ excels in image quality, artistry, and semantic control. We hope PIXART-$\\alpha$ will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.",
    "arxiv_url": "http://arxiv.org/abs/2310.00426v3",
    "pdf_url": "http://arxiv.org/pdf/2310.00426v3",
    "published_date": "2023-09-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Cartoondiff: Training-free Cartoon Image Generation with Diffusion Transformer Models",
    "authors": [
      "Feihong He",
      "Gang Li",
      "Lingyu Si",
      "Leilei Yan",
      "Shimeng Hou",
      "Hongwei Dong",
      "Fanzhang Li"
    ],
    "abstract": "Image cartoonization has attracted significant interest in the field of image generation. However, most of the existing image cartoonization techniques require re-training models using images of cartoon style. In this paper, we present CartoonDiff, a novel training-free sampling approach which generates image cartoonization using diffusion transformer models. Specifically, we decompose the reverse process of diffusion models into the semantic generation phase and the detail generation phase. Furthermore, we implement the image cartoonization process by normalizing high-frequency signal of the noisy image in specific denoising steps. CartoonDiff doesn't require any additional reference images, complex model designs, or the tedious adjustment of multiple parameters. Extensive experimental results show the powerful ability of our CartoonDiff. The project page is available at: https://cartoondiff.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2309.08251v1",
    "pdf_url": "http://arxiv.org/pdf/2309.08251v1",
    "published_date": "2023-09-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VDT: General-purpose Video Diffusion Transformers via Mask Modeling",
    "authors": [
      "Haoyu Lu",
      "Guoxing Yang",
      "Nanyi Fei",
      "Yuqi Huo",
      "Zhiwu Lu",
      "Ping Luo",
      "Mingyu Ding"
    ],
    "abstract": "This work introduces Video Diffusion Transformer (VDT), which pioneers the use of transformers in diffusion-based video generation. It features transformer blocks with modularized temporal and spatial attention modules to leverage the rich spatial-temporal representation inherited in transformers. We also propose a unified spatial-temporal mask modeling mechanism, seamlessly integrated with the model, to cater to diverse video generation scenarios. VDT offers several appealing benefits. 1) It excels at capturing temporal dependencies to produce temporally consistent video frames and even simulate the physics and dynamics of 3D objects over time. 2) It facilitates flexible conditioning information, \\eg, simple concatenation in the token space, effectively unifying different token lengths and modalities. 3) Pairing with our proposed spatial-temporal mask modeling mechanism, it becomes a general-purpose video diffuser for harnessing a range of tasks, including unconditional generation, video prediction, interpolation, animation, and completion, etc. Extensive experiments on these tasks spanning various scenarios, including autonomous driving, natural weather, human action, and physics-based simulation, demonstrate the effectiveness of VDT. Additionally, we present comprehensive studies on how \\model handles conditioning information with the mask modeling mechanism, which we believe will benefit future research and advance the field. Project page: https:VDT-2023.github.io",
    "arxiv_url": "http://arxiv.org/abs/2305.13311v2",
    "pdf_url": "http://arxiv.org/pdf/2305.13311v2",
    "published_date": "2023-05-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion",
    "authors": [
      "Seongmin Lee",
      "Benjamin Hoover",
      "Hendrik Strobelt",
      "Zijie J. Wang",
      "ShengYun Peng",
      "Austin Wright",
      "Kevin Li",
      "Haekyu Park",
      "Haoyang Yang",
      "Duen Horng Chau"
    ],
    "abstract": "Diffusion-based generative models' impressive ability to create convincing images has garnered global attention. However, their complex structures and operations often pose challenges for non-experts to grasp. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex structure with explanations of the underlying operations. By comparing image generation of prompt variants, users can discover the impact of keyword changes on image generation. A 56-participant user study demonstrates that Diffusion Explainer offers substantial learning benefits to non-experts. Our tool has been used by over 10,300 users from 124 countries at https://poloclub.github.io/diffusion-explainer/.",
    "arxiv_url": "http://arxiv.org/abs/2305.03509v3",
    "pdf_url": "http://arxiv.org/pdf/2305.03509v3",
    "published_date": "2023-05-04",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Semantic-Conditional Diffusion Networks for Image Captioning",
    "authors": [
      "Jianjie Luo",
      "Yehao Li",
      "Yingwei Pan",
      "Ting Yao",
      "Jianlin Feng",
      "Hongyang Chao",
      "Tao Mei"
    ],
    "abstract": "Recent advances on text-to-image generation have witnessed the rise of diffusion models which act as powerful generative models. Nevertheless, it is not trivial to exploit such latent variable models to capture the dependency among discrete words and meanwhile pursue complex visual-language alignment in image captioning. In this paper, we break the deeply rooted conventions in learning Transformer-based encoder-decoder, and propose a new diffusion model based paradigm tailored for image captioning, namely Semantic-Conditional Diffusion Networks (SCD-Net). Technically, for each input image, we first search the semantically relevant sentences via cross-modal retrieval model to convey the comprehensive semantic information. The rich semantics are further regarded as semantic prior to trigger the learning of Diffusion Transformer, which produces the output sentence in a diffusion process. In SCD-Net, multiple Diffusion Transformer structures are stacked to progressively strengthen the output sentence with better visional-language alignment and linguistical coherence in a cascaded manner. Furthermore, to stabilize the diffusion process, a new self-critical sequence training strategy is designed to guide the learning of SCD-Net with the knowledge of a standard autoregressive Transformer model. Extensive experiments on COCO dataset demonstrate the promising potential of using diffusion models in the challenging image captioning task. Source code is available at \\url{https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/scdnet}.",
    "arxiv_url": "http://arxiv.org/abs/2212.03099v1",
    "pdf_url": "http://arxiv.org/pdf/2212.03099v1",
    "published_date": "2022-12-06",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.MM"
    ],
    "github_url": "https://github.com/YehLi/xmodaler",
    "keywords": [
      "text-to-image",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spectral and Imaging properties of Sgr A* from High-Resolution 3D GRMHD Simulations with Radiative Cooling",
    "authors": [
      "Doosoo Yoon",
      "Koushik Chatterjee",
      "Sera Markoff",
      "David van Eijnatten",
      "Ziri Younsi",
      "Matthew Liska",
      "Alexander Tchekhovskoy"
    ],
    "abstract": "The candidate supermassive black hole in the Galactic Centre, Sagittarius A* (Sgr A*), is known to be fed by a radiatively inefficient accretion flow (RIAF), inferred by its low accretion rate. Consequently, radiative cooling has in general been overlooked in the study of Sgr A*. However, the radiative properties of the plasma in RIAFs are poorly understood. In this work, using full 3D general-relativistic magneto-hydrodynamical simulations, we study the impact of radiative cooling on the dynamical evolution of the accreting plasma, presenting spectral energy distributions and synthetic sub-millimeter images generated from the accretion flow around Sgr A*. These simulations solve the approximated equations for radiative cooling processes self-consistently, including synchrotron, bremsstrahlung, and inverse Compton processes. We find that radiative cooling plays an increasingly important role in the dynamics of the accretion flow as the accretion rate increases: the mid-plane density grows and the infalling gas is less turbulent as cooling becomes stronger. The changes in the dynamical evolution become important when the accretion rate is larger than $10^{-8}\\,M_{\\odot}~{\\rm yr}^{-1}$ ($\\gtrsim 10^{-7} \\dot{M}_{\\rm Edd}$, where $\\dot{M}_{\\rm Edd}$ is the Eddington accretion rate). The resulting spectra in the cooled models also differ from those in the non-cooled models: the overall flux, including the peak values at the sub-mm and the far-UV, is slightly lower as a consequence of a decrease in the electron temperature. Our results suggest that radiative cooling should be carefully taken into account in modelling Sgr A* and other low-luminosity active galactic nuclei that have a mass accretion rate of $\\dot{M} > 10^{-7}\\,\\dot{M}_{\\rm Edd}$.",
    "arxiv_url": "http://arxiv.org/abs/2009.14227v1",
    "pdf_url": "http://arxiv.org/pdf/2009.14227v1",
    "published_date": "2020-09-29",
    "categories": [
      "astro-ph.HE"
    ],
    "github_url": "",
    "keywords": [
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Galaxy cluster mass estimation with deep learning and hydrodynamical simulations",
    "authors": [
      "Z. Yan",
      "A. J. Mead",
      "L. Van Waerbeke",
      "G. Hinshaw",
      "I. G. McCarthy"
    ],
    "abstract": "We evaluate the ability of Convolutional Neural Networks (CNNs) to predict galaxy cluster masses in the BAHAMAS hydrodynamical simulations. We train four separate single-channel networks using: stellar mass, soft X-ray flux, bolometric X-ray flux, and the Compton $y$ parameter as observational tracers, respectively. Our training set consists of $\\sim$4800 synthetic cluster images generated from the simulation, while an additional $\\sim$3200 images form a validation set and a test set, each with 1600 images. In order to mimic real observation, these images also contain uncorrelated structures located within 50 Mpc in front and behind clusters and seen in projection, as well as instrumental systematics including noise and smoothing. In addition to CNNs for all the four observables, we also train a `multi-channel' CNN by combining the four observational tracers. The learning curves of all the five CNNs converge within 1000 epochs. The resulting predictions are especially precise for halo masses in the range $10^{13.25}M_{\\odot}<M<10^{14.5}M_{\\odot}$, where all five networks produce mean mass biases of order $\\approx$1\\% with a scatter of $\\lesssim$20\\%. The network trained with Compton $y$ parameter maps yields the most precise predictions. We interpret the network's behaviour using two diagnostic tests to determine which features are used to predict cluster mass. The CNN trained with stellar mass images detect galaxies (not surprisingly), while CNNs trained with gas-based tracers utilise the shape of the signal to estimate cluster mass.",
    "arxiv_url": "http://arxiv.org/abs/2005.11819v2",
    "pdf_url": "http://arxiv.org/pdf/2005.11819v2",
    "published_date": "2020-05-24",
    "categories": [
      "astro-ph.CO"
    ],
    "github_url": "",
    "keywords": [
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Expected performances of the Characterising Exoplanet Satellite (CHEOPS) II. The CHEOPS simulator",
    "authors": [
      "David Futyan",
      "Andrea Fortier",
      "Mathias Beck",
      "David Ehrenreich",
      "Anja Bekkelien",
      "Willy Benz",
      "Nicolas Billot",
      "Vincent Bourrier",
      "Christopher Broeg",
      "Andrew Collier Cameron",
      "Adrien Deline",
      "Thibault Kuntzer",
      "Monika Lendl",
      "Didier Queloz",
      "Reiner Rohlfs",
      "Attila E. Simon",
      "Francois Wildi"
    ],
    "abstract": "The CHaracterising ExOPlanet Satellite (CHEOPS) is a mission dedicated to the search for exoplanetary transits through high precision photometry of bright stars already known to host planets. The telescope will provide the unique capability of determining accurate radii for planets whose masses have already been measured from ground-based spectroscopic surveys. This will allow a first-order characterisation of the planets' internal structure through the determination of the bulk density, providing direct insight into their composition. The CHEOPS simulator has been developed to perform detailed simulations of the data which is to be received from the CHEOPS satellite. It generates accurately simulated images that can be used to explore design options and to test the on-ground data processing, in particular, the pipeline producing the photometric time series. It is, thus, a critical tool for estimating the photometric performance expected in flight and to guide photometric analysis. It can be used to prepare observations, consolidate the noise budget, and asses the performance of CHEOPS in realistic astrophysical fields that are difficult to reproduce in the laboratory. Images generated by CHEOPSim take account of many detailed effects, including variations of the incident signal flux and backgrounds, and detailed modelling of the satellite orbit, pointing jitter and telescope optics, as well as the CCD response, noise and readout. The simulator results presented in this paper have been used in the context of validating the data reduction processing chain, in which image time series generated by CHEOPSim were used to generate light curves for simulated planetary transits across real and simulated targets. Independent analysts were successfully able to detect the planets and measure their radii to an accuracy within the science requirements of the mission.",
    "arxiv_url": "http://arxiv.org/abs/2001.05587v1",
    "pdf_url": "http://arxiv.org/pdf/2001.05587v1",
    "published_date": "2020-01-15",
    "categories": [
      "astro-ph.IM",
      "astro-ph.EP"
    ],
    "github_url": "",
    "keywords": [
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "First M87 Event Horizon Telescope Results. VI. The Shadow and Mass of the Central Black Hole",
    "authors": [
      "The Event Horizon Telescope Collaboration"
    ],
    "abstract": "We present measurements of the properties of the central radio source in M87 using Event Horizon Telescope data obtained during the 2017 campaign. We develop and fit geometric crescent models (asymmetric rings with interior brightness depressions) using two independent sampling algorithms that consider distinct representations of the visibility data. We show that the crescent family of models is statistically preferred over other comparably complex geometric models that we explore. We calibrate the geometric model parameters using general relativistic magnetohydrodynamic (GRMHD) models of the emission region and estimate physical properties of the source. We further fit images generated from GRMHD models directly to the data. We compare the derived emission region and black hole parameters from these analyses with those recovered from reconstructed images. There is a remarkable consistency among all methods and data sets. We find that >50% of the total flux at arcsecond scales comes from near the horizon, and that the emission is dramatically suppressed interior to this region by a factor >10, providing direct evidence of the predicted shadow of a black hole. Across all methods, we measure a crescent diameter of 42+/-3 micro-as and constrain its fractional width to be <0.5. Associating the crescent feature with the emission surrounding the black hole shadow, we infer an angular gravitational radius of GM/Dc2 = 3.8+/- 0.4 micro-as. Folding in a distance measurement of 16.8(+0.8,-0.7) Mpc gives a black hole mass of M = 6.5 +/- 0.2(stat) +/-0.7(sys) 10^9 Msun. This measurement from lensed emission near the event horizon is consistent with the presence of a central Kerr black hole, as predicted by the general theory of relativity.",
    "arxiv_url": "http://arxiv.org/abs/1906.11243v1",
    "pdf_url": "http://arxiv.org/pdf/1906.11243v1",
    "published_date": "2019-06-26",
    "categories": [
      "astro-ph.GA",
      "astro-ph.HE",
      "gr-qc"
    ],
    "github_url": "",
    "keywords": [
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AARTFAAC Flux Density Calibration and Northern Hemisphere Catalogue at 60 MHz",
    "authors": [
      "Mark Kuiack",
      "Folkert Huizinga",
      "Gijs Molenaar",
      "Peeyush Prasad",
      "Antonia Rowlinson",
      "Ralph A. M. J. Wijers"
    ],
    "abstract": "We present a method for calibrating the flux density scale for images generated by the Amsterdam ASTRON Radio Transient Facility And Analysis Centre (AARTFAAC). AARTFAAC produces a stream of all-sky images at a rate of one second in order to survey the Northern Hemisphere for short duration, low frequency transients, such as the prompt EM counterpart to gravitational wave events, magnetar flares, blazars, and other as of yet unobserved phenomena. Therefore, an independent flux density scaling solution per image is calculated via bootstrapping, comparing the measured apparent brightness of sources in the field to a reference catalogue. However, the lack of accurate flux density measurements of bright sources below 74 MHz necessitated the creation of the AARTFAAC source catalogue, at 60 MHz, which contains 167 sources across the Northern Hemisphere. Using this as a reference results in a sufficiently high number of detected sources in each image to calculate a stable and accurate flux scale per one second snapshot, in real-time.",
    "arxiv_url": "http://arxiv.org/abs/1810.06430v1",
    "pdf_url": "http://arxiv.org/pdf/1810.06430v1",
    "published_date": "2018-10-15",
    "categories": [
      "astro-ph.IM"
    ],
    "github_url": "",
    "keywords": [
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ALMACAL IV: A catalogue of ALMA calibrator continuum observations",
    "authors": [
      "M. Bonato",
      "E. Liuzzo",
      "A. Giannetti",
      "M. Massardi",
      "G. De Zotti",
      "S. Burkutean",
      "V. Galluzzi",
      "M. Negrello",
      "I. Baronchelli",
      "J. Brand",
      "M. A. Zwaan",
      "K. L. J. Rygl",
      "N. Marchili",
      "A. Klitsch",
      "I. Oteo"
    ],
    "abstract": "We present a catalogue of ALMA flux density measurements of 754 calibrators observed between August 2012 and September 2017, for a total of 16,263 observations in different bands and epochs. The flux densities were measured reprocessing the ALMA images generated in the framework of the ALMACAL project, with a new code developed by the Italian node of the European ALMA Regional Centre. A search in the online databases yielded redshift measurements for 589 sources ($\\sim$78 per cent of the total). Almost all sources are flat-spectrum, based on their low-frequency spectral index, and have properties consistent with being blazars of different types. To illustrate the properties of the sample we show the redshift and flux density distributions as well as the distributions of the number of observations of individual sources and of time spans in the source frame for sources observed in bands 3 (84$-$116 GHz) and 6 (211$-$275 GHz). As examples of the scientific investigations allowed by the catalogue we briefly discuss the variability properties of our sources in ALMA bands 3 and 6 and the frequency spectra between the effective frequencies of these bands. We find that the median variability index steadily increases with the source-frame time lag increasing from 100 to 800 days, and that the frequency spectra of BL Lacs are significantly flatter than those of flat-spectrum radio quasars. We also show the global spectral energy distributions of our sources over 17 orders of magnitude in frequency.",
    "arxiv_url": "http://arxiv.org/abs/1805.00024v1",
    "pdf_url": "http://arxiv.org/pdf/1805.00024v1",
    "published_date": "2018-04-30",
    "categories": [
      "astro-ph.GA"
    ],
    "github_url": "",
    "keywords": [
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ProFound: Source Extraction and Application to Modern Survey Data",
    "authors": [
      "A. S. G. Robotham",
      "L. J. M. Davies",
      "S. P. Driver",
      "S. Koushan",
      "D. S. Taranu",
      "S. Casura",
      "J. Liske"
    ],
    "abstract": "We introduce ProFound, a source finding and image analysis package. ProFound provides methods to detect sources in noisy images, generate segmentation maps identifying the pixels belonging to each source, and measure statistics like flux, size and ellipticity. These inputs are key requirements of ProFit, our recently released galaxy profiling package, where the design aim is that these two software packages will be used in unison to semi-automatically profile large samples of galaxies. The key novel feature introduced in ProFound is that all photometry is executed on dilated segmentation maps that fully contain the identifiable flux, rather than using more traditional circular or ellipse based photometry. Also, to be less sensitive to pathological segmentation issues, the de-blending is made across saddle points in flux. We apply ProFound in a number of simulated and real world cases, and demonstrate that it behaves reasonably given its stated design goals. In particular, it offers good initial parameter estimation for ProFit, and also segmentation maps that follow the sometimes complex geometry of resolved sources, whilst capturing nearly all of the flux. A number of bulge-disc decomposition projects are already making use of the ProFound and ProFit pipeline, and adoption is being encouraged by publicly releasing the software for the open source R data analysis platform under an LGPL-3 license on GitHub (github.com/asgr/ProFound).",
    "arxiv_url": "http://arxiv.org/abs/1802.00937v1",
    "pdf_url": "http://arxiv.org/pdf/1802.00937v1",
    "published_date": "2018-02-03",
    "categories": [
      "astro-ph.IM"
    ],
    "github_url": "https://github.com/asgr/ProFound",
    "keywords": [
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Observational signatures of a kink-unstable coronal flux rope using Hinode/EIS",
    "authors": [
      "Ben Snow",
      "Gert J. J. Botha",
      "Stephane Regnier",
      "Richard J. Morton",
      "Erwin Verwichte",
      "Peter R Young"
    ],
    "abstract": "The signatures of energy release and energy transport for a kink-unstable coronal flux rope are investigated via forward modelling. Synthetic intensity and Doppler maps are generated from a 3D numerical simulation. The CHIANTI database is used to compute intensities for three Hinode/EIS emission lines that cover the thermal range of the loop. The intensities and Doppler velocities at simulation resolution are spatially degraded to the Hinode/EIS pixel size (1\\arcsec), convolved using a Gaussian point-spread function (3\\arcsec), and exposed for a characteristic time of 50 seconds. The synthetic images generated for rasters (moving slit) and sit-and-stare (stationary slit) are analysed to find the signatures of the twisted flux and the associated instability. We find that there are several qualities of a kink-unstable coronal flux rope that can be detected observationally using Hinode/EIS, namely the growth of the loop radius, the increase in intensity towards the radial edge of the loop, and the Doppler velocity following an internal twisted magnetic field line. However, EIS cannot resolve the small, transient features present in the simulation, such as sites of small-scale reconnection (e.g. nanoflares)",
    "arxiv_url": "http://arxiv.org/abs/1705.05114v1",
    "pdf_url": "http://arxiv.org/pdf/1705.05114v1",
    "published_date": "2017-05-15",
    "categories": [
      "astro-ph.SR"
    ],
    "github_url": "",
    "keywords": [
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "The SOFIA Massive (SOMA) Star Formation Survey. I. Overview and First Results",
    "authors": [
      "James M. De Buizer",
      "Mengyao Liu",
      "Jonathan C. Tan",
      "Yichen Zhang",
      "Maria T. Beltran",
      "Ralph Shuping",
      "Jan E. Staff",
      "Kei E. I. Tanaka",
      "Barbara Whitney"
    ],
    "abstract": "We present an overview and first results of the Stratospheric Observatory For Infrared Astronomy Massive (SOMA) Star Formation Survey, which is using the FORCAST instrument to image massive protostars from $\\sim10$--$40\\:\\rm{\\mu}\\rm{m}$. These wavelengths trace thermal emission from warm dust, which in Core Accretion models mainly emerges from the inner regions of protostellar outflow cavities. Dust in dense core envelopes also imprints characteristic extinction patterns at these wavelengths, causing intensity peaks to shift along the outflow axis and profiles to become more symmetric at longer wavelengths. We present observational results for the first eight protostars in the survey, i.e., multiwavelength images, including some ancillary ground-based MIR observations and archival {\\it{Spitzer}} and {\\it{Herschel}} data. These images generally show extended MIR/FIR emission along directions consistent with those of known outflows and with shorter wavelength peak flux positions displaced from the protostar along the blueshifted, near-facing sides, thus confirming qualitative predictions of Core Accretion models. We then compile spectral energy distributions and use these to derive protostellar properties by fitting theoretical radiative transfer models. Zhang and Tan models, based on the Turbulent Core Model of McKee and Tan, imply the sources have protostellar masses $m_*\\sim10$--50$\\:M_\\odot$ accreting at $\\sim10^{-4}$--$10^{-3}\\:M_\\odot\\:{\\rm{yr}}^{-1}$ inside cores of initial masses $M_c\\sim30$--500$\\:M_\\odot$ embedded in clumps with mass surface densities $\\Sigma_{\\rm{cl}}\\sim0.1$--3$\\:{\\rm{g\\:cm}^{-2}}$. Fitting Robitaille et al. models typically leads to slightly higher protostellar masses, but with disk accretion rates $\\sim100\\times$ smaller. We discuss reasons for these differences and overall implications of these first survey results for massive star formation theories.",
    "arxiv_url": "http://arxiv.org/abs/1610.05373v5",
    "pdf_url": "http://arxiv.org/pdf/1610.05373v5",
    "published_date": "2016-10-17",
    "categories": [
      "astro-ph.GA",
      "astro-ph.SR"
    ],
    "github_url": "",
    "keywords": [
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "The effect of low mass substructures on the Cusp lensing relation",
    "authors": [
      "Andrea V. Maccio'",
      "Marco Miranda"
    ],
    "abstract": "It has been argued that the flux anomalies detected in gravitationally lensed QSOs are evidence for substructures in the foreground lensing haloes. In this paper we investigate this issue in greater detail focusing on the Cusp relation which corresponds to images of a source located to the cusp of the inner caustic curve. We use numerical simulations combined with a Monte Carlo approach to study the effects of the expected power law distribution of substructures within LCDM haloes on the multiple images.   Generally, the high number of anomalous flux ratios in the cusp configurations is unlikely explained by 'simple' perturbers (subhaloes) inside the lensing galaxy, either modeled by point masses or extended NFW subhaloes. We considered in our analysis a mass range of 10^5-10^7 Msun for the subhaloes. We also demonstrate that including the effects of the surrounding mass distribution, such as other galaxies close to the primary lens, does not change the results. We conclude that triple images of lensed QSOs do not show any direct evidence for dark dwarf galaxies such as cold dark matter substructure.",
    "arxiv_url": "http://arxiv.org/abs/astro-ph/0509598v2",
    "pdf_url": "http://arxiv.org/pdf/astro-ph/0509598v2",
    "published_date": "2005-09-20",
    "categories": [
      "astro-ph"
    ],
    "github_url": "",
    "keywords": [
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  }
]