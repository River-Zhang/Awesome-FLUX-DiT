[
  {
    "title": "Boosting Resolution Generalization of Diffusion Transformers with Randomized Positional Encodings",
    "authors": [
      "Cong Liu",
      "Liang Hou",
      "Mingwu Zheng",
      "Xin Tao",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai"
    ],
    "abstract": "Resolution generalization in image generation tasks enables the production of higher-resolution images with lower training resolution overhead. However, a significant challenge in resolution generalization, particularly in the widely used Diffusion Transformers, lies in the mismatch between the positional encodings encountered during testing and those used during training. While existing methods have employed techniques such as interpolation, extrapolation, or their combinations, none have fully resolved this issue. In this paper, we propose a novel two-dimensional randomized positional encodings (RPE-2D) framework that focuses on learning positional order of image patches instead of the specific distances between them, enabling seamless high- and low-resolution image generation without requiring high- and low-resolution image training. Specifically, RPE-2D independently selects positions over a broader range along both the horizontal and vertical axes, ensuring that all position encodings are trained during the inference phase, thus improving resolution generalization. Additionally, we propose a random data augmentation technique to enhance the modeling of position order. To address the issue of image cropping caused by the augmentation, we introduce corresponding micro-conditioning to enable the model to perceive the specific cropping patterns. On the ImageNet dataset, our proposed RPE-2D achieves state-of-the-art resolution generalization performance, outperforming existing competitive methods when trained at a resolution of $256 \\times 256$ and inferred at $384 \\times 384$ and $512 \\times 512$, as well as when scaling from $512 \\times 512$ to $768 \\times 768$ and $1024 \\times 1024$. And it also exhibits outstanding capabilities in low-resolution image generation, multi-stage training acceleration and multi-resolution inheritance.",
    "arxiv_url": "http://arxiv.org/abs/2503.18719v1",
    "pdf_url": "http://arxiv.org/pdf/2503.18719v1",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models",
    "authors": [
      "Jinjin Zhang",
      "Qiuyu Huang",
      "Junjie Liu",
      "Xiefan Guo",
      "Di Huang"
    ],
    "abstract": "In this paper, we present Diffusion-4K, a novel framework for direct ultra-high-resolution image synthesis using text-to-image diffusion models. The core advancements include: (1) Aesthetic-4K Benchmark: addressing the absence of a publicly available 4K image synthesis dataset, we construct Aesthetic-4K, a comprehensive benchmark for ultra-high-resolution image generation. We curated a high-quality 4K dataset with carefully selected images and captions generated by GPT-4o. Additionally, we introduce GLCM Score and Compression Ratio metrics to evaluate fine details, combined with holistic measures such as FID, Aesthetics and CLIPScore for a comprehensive assessment of ultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a wavelet-based fine-tuning approach for direct training with photorealistic 4K images, applicable to various latent diffusion models, demonstrating its effectiveness in synthesizing highly detailed 4K images. Consequently, Diffusion-4K achieves impressive performance in high-quality image synthesis and text prompt adherence, especially when powered by modern large-scale diffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results from our benchmark demonstrate the superiority of Diffusion-4K in ultra-high-resolution image synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2503.18352v1",
    "pdf_url": "http://arxiv.org/pdf/2503.18352v1",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating, Fast and Slow: Scalable Parallel Video Generation with Video Interface Networks",
    "authors": [
      "Bhishma Dedhia",
      "David Bourgin",
      "Krishna Kumar Singh",
      "Yuheng Li",
      "Yan Kang",
      "Zhan Xu",
      "Niraj K. Jha",
      "Yuchen Liu"
    ],
    "abstract": "Diffusion Transformers (DiTs) can generate short photorealistic videos, yet directly training and sampling longer videos with full attention across the video remains computationally challenging. Alternative methods break long videos down into sequential generation of short video segments, requiring multiple sampling chain iterations and specialized consistency modules. To overcome these challenges, we introduce a new paradigm called Video Interface Networks (VINs), which augment DiTs with an abstraction module to enable parallel inference of video chunks. At each diffusion step, VINs encode global semantics from the noisy input of local chunks and the encoded representations, in turn, guide DiTs in denoising chunks in parallel. The coupling of VIN and DiT is learned end-to-end on the denoising objective. Further, the VIN architecture maintains fixed-size encoding tokens that encode the input via a single cross-attention step. Disentangling the encoding tokens from the input thus enables VIN to scale to long videos and learn essential semantics. Experiments on VBench demonstrate that VINs surpass existing chunk-based methods in preserving background consistency and subject coherence. We then show via an optical flow analysis that our approach attains state-of-the-art motion smoothness while using 25-40% fewer FLOPs than full generation. Finally, human raters favorably assessed the overall video quality and temporal consistency of our method in a user study.",
    "arxiv_url": "http://arxiv.org/abs/2503.17539v1",
    "pdf_url": "http://arxiv.org/pdf/2503.17539v1",
    "published_date": "2025-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EDiT: Efficient Diffusion Transformers with Linear Compressed Attention",
    "authors": [
      "Philipp Becker",
      "Abhinav Mehrotra",
      "Ruchika Chavhan",
      "Malcolm Chadwick",
      "Luca Morreale",
      "Mehdi Noroozi",
      "Alberto Gil Ramos",
      "Sourav Bhattacharya"
    ],
    "abstract": "Diffusion Transformers (DiTs) have emerged as a leading architecture for text-to-image synthesis, producing high-quality and photorealistic images. However, the quadratic scaling properties of the attention in DiTs hinder image generation with higher resolution or on devices with limited resources. This work introduces an efficient diffusion transformer (EDiT) to alleviate these efficiency bottlenecks in conventional DiTs and Multimodal DiTs (MM-DiTs). First, we present a novel linear compressed attention method that uses a multi-layer convolutional network to modulate queries with local information while keys and values are spatially aggregated. Second, we formulate a hybrid attention scheme for multi-modal inputs that combines linear attention for image-to-image interactions and standard scaled dot-product attention for interactions involving prompts. Merging these two approaches leads to an expressive, linear-time Multimodal Efficient Diffusion Transformer (MM-EDiT). We demonstrate the effectiveness of the EDiT and MM-EDiT architectures by integrating them into PixArt-Sigma(conventional DiT) and Stable Diffusion 3.5-Medium (MM-DiT), achieving up to 2.2x speedup with comparable image quality after distillation.",
    "arxiv_url": "http://arxiv.org/abs/2503.16726v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16726v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
    "authors": [
      "Liming Jiang",
      "Qing Yan",
      "Yumin Jia",
      "Zichuan Liu",
      "Hao Kang",
      "Xin Lu"
    ],
    "abstract": "Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality and aesthetics. Central to InfU is InfuseNet, a component that injects identity features into the DiT base model via residual connections, enhancing identity similarity while maintaining generation capabilities. A multi-stage training strategy, including pretraining and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample (SPMS) data, further improves text-image alignment, ameliorates image quality, and alleviates face copy-pasting. Extensive experiments demonstrate that InfU achieves state-of-the-art performance, surpassing existing baselines. In addition, the plug-and-play design of InfU ensures compatibility with various existing methods, offering a valuable contribution to the broader community.",
    "arxiv_url": "http://arxiv.org/abs/2503.16418v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16418v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Ultra-Resolution Adaptation with Ease",
    "authors": [
      "Ruonan Yu",
      "Songhua Liu",
      "Zhenxiong Tan",
      "Xinchao Wang"
    ],
    "abstract": "Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key perspectives: data and parameter efficiency, and propose a set of key guidelines for ultra-resolution adaptation termed \\emph{URAE}. For data efficiency, we theoretically and empirically demonstrate that synthetic data generated by some teacher models can significantly promote training convergence. For parameter efficiency, we find that tuning minor components of the weight matrices outperforms widely-used low-rank adapters when synthetic data are unavailable, offering substantial performance gains while maintaining efficiency. Additionally, for models leveraging guidance distillation, such as FLUX, we show that disabling classifier-free guidance, \\textit{i.e.}, setting the guidance scale to 1 during adaptation, is crucial for satisfactory performance. Extensive experiments validate that URAE achieves comparable 2K-generation performance to state-of-the-art closed-source models like FLUX1.1 [Pro] Ultra with only 3K samples and 2K iterations, while setting new benchmarks for 4K-resolution generation. Codes are available \\href{https://github.com/Huage001/URAE}{here}.",
    "arxiv_url": "http://arxiv.org/abs/2503.16322v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16322v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Huage001/URAE",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FreeFlux: Understanding and Exploiting Layer-Specific Roles in RoPE-Based MMDiT for Versatile Image Editing",
    "authors": [
      "Tianyi Wei",
      "Yifan Zhou",
      "Dongdong Chen",
      "Xingang Pan"
    ],
    "abstract": "The integration of Rotary Position Embedding (RoPE) in Multimodal Diffusion Transformer (MMDiT) has significantly enhanced text-to-image generation quality. However, the fundamental reliance of self-attention layers on positional embedding versus query-key similarity during generation remains an intriguing question. We present the first mechanistic analysis of RoPE-based MMDiT models (e.g., FLUX), introducing an automated probing strategy that disentangles positional information versus content dependencies by strategically manipulating RoPE during generation. Our analysis reveals distinct dependency patterns that do not straightforwardly correlate with depth, offering new insights into the layer-specific roles in RoPE-based MMDiT. Based on these findings, we propose a training-free, task-specific image editing framework that categorizes editing tasks into three types: position-dependent editing (e.g., object addition), content similarity-dependent editing (e.g., non-rigid editing), and region-preserved editing (e.g., background replacement). For each type, we design tailored key-value injection strategies based on the characteristics of the editing task. Extensive qualitative and quantitative evaluations demonstrate that our method outperforms state-of-the-art approaches, particularly in preserving original semantic content and achieving seamless modifications.",
    "arxiv_url": "http://arxiv.org/abs/2503.16153v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16153v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "image editing",
      "FLUX",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation",
    "authors": [
      "Soham Roy",
      "Abhishek Mishra",
      "Shirish Karande",
      "Murari Mandal"
    ],
    "abstract": "Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog",
    "arxiv_url": "http://arxiv.org/abs/2503.16171v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16171v1",
    "published_date": "2025-03-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers",
    "authors": [
      "Minglei Shi",
      "Ziyang Yuan",
      "Haotian Yang",
      "Xintao Wang",
      "Mingwu Zheng",
      "Xin Tao",
      "Wenliang Zhao",
      "Wenzhao Zheng",
      "Jie Zhou",
      "Jiwen Lu",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai"
    ],
    "abstract": "Diffusion models have demonstrated remarkable success in various image generation tasks, but their performance is often limited by the uniform processing of inputs across varying conditions and noise levels. To address this limitation, we propose a novel approach that leverages the inherent heterogeneity of the diffusion process. Our method, DiffMoE, introduces a batch-level global token pool that enables experts to access global token distributions during training, promoting specialized expert behavior. To unleash the full potential of the diffusion process, DiffMoE incorporates a capacity predictor that dynamically allocates computational resources based on noise levels and sample complexity. Through comprehensive evaluation, DiffMoE achieves state-of-the-art performance among diffusion models on ImageNet benchmark, substantially outperforming both dense architectures with 3x activated parameters and existing MoE approaches while maintaining 1x activated parameters. The effectiveness of our approach extends beyond class-conditional generation to more challenging tasks such as text-to-image generation, demonstrating its broad applicability across different diffusion model applications. Project Page: https://shiml20.github.io/DiffMoE/",
    "arxiv_url": "http://arxiv.org/abs/2503.14487v1",
    "pdf_url": "http://arxiv.org/pdf/2503.14487v1",
    "published_date": "2025-03-18",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Personalize Anything for Free with Diffusion Transformer",
    "authors": [
      "Haoran Feng",
      "Zehuan Huang",
      "Lin Li",
      "Hairong Lv",
      "Lu Sheng"
    ],
    "abstract": "Personalized image generation aims to produce images of user-specified concepts while enabling flexible editing. Recent training-free approaches, while exhibit higher computational efficiency than training-based methods, struggle with identity preservation, applicability, and compatibility with diffusion transformers (DiTs). In this paper, we uncover the untapped potential of DiT, where simply replacing denoising tokens with those of a reference subject achieves zero-shot subject reconstruction. This simple yet effective feature injection technique unlocks diverse scenarios, from personalization to image editing. Building upon this observation, we propose \\textbf{Personalize Anything}, a training-free framework that achieves personalized image generation in DiT through: 1) timestep-adaptive token replacement that enforces subject consistency via early-stage injection and enhances flexibility through late-stage regularization, and 2) patch perturbation strategies to boost structural diversity. Our method seamlessly supports layout-guided generation, multi-subject personalization, and mask-controlled editing. Evaluations demonstrate state-of-the-art performance in identity preservation and versatility. Our work establishes new insights into DiTs while delivering a practical paradigm for efficient personalization.",
    "arxiv_url": "http://arxiv.org/abs/2503.12590v1",
    "pdf_url": "http://arxiv.org/pdf/2503.12590v1",
    "published_date": "2025-03-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image editing",
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection",
    "authors": [
      "Shufan Li",
      "Konstantinos Kallidromitis",
      "Akash Gokul",
      "Arsh Koneru",
      "Yusuke Kato",
      "Kazuki Kozuka",
      "Aditya Grover"
    ],
    "abstract": "The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and a selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping text-to-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, a method that enables Diffusion Transformers to refine their generations using in-context examples of previously generated images alongside textual feedback describing necessary improvements. Instead of passively relying on random sampling and hoping for a better result in a future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it achieves a new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach.",
    "arxiv_url": "http://arxiv.org/abs/2503.12271v1",
    "pdf_url": "http://arxiv.org/pdf/2503.12271v1",
    "published_date": "2025-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture Design in Text to Image Generation",
    "authors": [
      "Chen Chen",
      "Rui Qian",
      "Wenze Hu",
      "Tsu-Jui Fu",
      "Jialing Tong",
      "Xinze Wang",
      "Lezhi Li",
      "Bowen Zhang",
      "Alex Schwing",
      "Wei Liu",
      "Yinfei Yang"
    ],
    "abstract": "In this work, we empirically study Diffusion Transformers (DiTs) for text-to-image generation, focusing on architectural choices, text-conditioning strategies, and training protocols. We evaluate a range of DiT-based architectures--including PixArt-style and MMDiT variants--and compare them with a standard DiT variant which directly processes concatenated text and noise inputs. Surprisingly, our findings reveal that the performance of standard DiT is comparable with those specialized models, while demonstrating superior parameter-efficiency, especially when scaled up. Leveraging the layer-wise parameter sharing strategy, we achieve a further reduction of 66% in model size compared to an MMDiT architecture, with minimal performance impact. Building on an in-depth analysis of critical components such as text encoders and Variational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With supervised and reward fine-tuning, DiT-Air achieves state-of-the-art performance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly competitive, surpassing most existing models despite its compact size.",
    "arxiv_url": "http://arxiv.org/abs/2503.10618v2",
    "pdf_url": "http://arxiv.org/pdf/2503.10618v2",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Long Context Tuning for Video Generation",
    "authors": [
      "Yuwei Guo",
      "Ceyuan Yang",
      "Ziyan Yang",
      "Zhibei Ma",
      "Zhijie Lin",
      "Zhenheng Yang",
      "Dahua Lin",
      "Lu Jiang"
    ],
    "abstract": "Recent advances in video generation can produce realistic, minute-long single-shot videos with scalable diffusion transformers. However, real-world narrative videos require multi-shot scenes with visual and dynamic consistency across shots. In this work, we introduce Long Context Tuning (LCT), a training paradigm that expands the context window of pre-trained single-shot video diffusion models to learn scene-level consistency directly from data. Our method expands full attention mechanisms from individual shots to encompass all shots within a scene, incorporating interleaved 3D position embedding and an asynchronous noise strategy, enabling both joint and auto-regressive shot generation without additional parameters. Models with bidirectional attention after LCT can further be fine-tuned with context-causal attention, facilitating auto-regressive generation with efficient KV-cache. Experiments demonstrate single-shot models after LCT can produce coherent multi-shot scenes and exhibit emerging capabilities, including compositional generation and interactive shot extension, paving the way for more practical visual content creation. See https://guoyww.github.io/projects/long-context-video/ for more details.",
    "arxiv_url": "http://arxiv.org/abs/2503.10589v1",
    "pdf_url": "http://arxiv.org/pdf/2503.10589v1",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation Benchmark",
    "authors": [
      "Viktor Moskvoretskii",
      "Alina Lobanova",
      "Ekaterina Neminova",
      "Chris Biemann",
      "Alexander Panchenko",
      "Irina Nikishina"
    ],
    "abstract": "This paper explores the feasibility of using text-to-image models in a zero-shot setup to generate images for taxonomy concepts. While text-based methods for taxonomy enrichment are well-established, the potential of the visual dimension remains unexplored. To address this, we propose a comprehensive benchmark for Taxonomy Image Generation that assesses models' abilities to understand taxonomy concepts and generate relevant, high-quality images. The benchmark includes common-sense and randomly sampled WordNet concepts, alongside the LLM generated predictions. The 12 models are evaluated using 9 novel taxonomy-related text-to-image metrics and human feedback. Moreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for image generation. Experimental results show that the ranking of models differs significantly from standard T2I tasks. Playground-v2 and FLUX consistently outperform across metrics and subsets and the retrieval-based approach performs poorly. These findings highlight the potential for automating the curation of structured data resources.",
    "arxiv_url": "http://arxiv.org/abs/2503.10357v1",
    "pdf_url": "http://arxiv.org/pdf/2503.10357v1",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer",
    "authors": [
      "Haoxuan Wang",
      "Jinlong Peng",
      "Qingdong He",
      "Hao Yang",
      "Ying Jin",
      "Jiafu Wu",
      "Xiaobin Hu",
      "Yanjie Pan",
      "Zhenye Gan",
      "Mingmin Chi",
      "Bo Peng",
      "Yabiao Wang"
    ],
    "abstract": "With the rapid development of diffusion models in image generation, the demand for more powerful and flexible controllable frameworks is increasing. Although existing methods can guide generation beyond text prompts, the challenge of effectively combining multiple conditional inputs while maintaining consistency with all of them remains unsolved. To address this, we introduce UniCombine, a DiT-based multi-conditional controllable generative framework capable of handling any combination of conditions, including but not limited to text prompts, spatial maps, and subject images. Specifically, we introduce a novel Conditional MMDiT Attention mechanism and incorporate a trainable LoRA module to build both the training-free and training-based versions. Additionally, we propose a new pipeline to construct SubjectSpatial200K, the first dataset designed for multi-conditional generative tasks covering both the subject-driven and spatially-aligned conditions. Extensive experimental results on multi-conditional generation demonstrate the outstanding universality and powerful capability of our approach with state-of-the-art performance.",
    "arxiv_url": "http://arxiv.org/abs/2503.09277v1",
    "pdf_url": "http://arxiv.org/pdf/2503.09277v1",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Controllable",
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NAMI: Efficient Image Generation via Progressive Rectified Flow Transformers",
    "authors": [
      "Yuhang Ma",
      "Bo Cheng",
      "Shanyuan Liu",
      "Ao Ma",
      "Xiaoyu Wu",
      "Liebucha Wu",
      "Dawei Leng",
      "Yuhui Yin"
    ],
    "abstract": "Flow-based transformer models for image generation have achieved state-of-the-art performance with larger model parameters, but their inference deployment cost remains high. To enhance inference performance while maintaining generation quality, we propose progressive rectified flow transformers. We divide the rectified flow into different stages according to resolution, using fewer transformer layers at the low-resolution stages to generate image layouts and concept contours, and progressively adding more layers as the resolution increases. Experiments demonstrate that our approach achieves fast convergence and reduces inference time while ensuring generation quality. The main contributions of this paper are summarized as follows: (1) We introduce progressive rectified flow transformers that enable multi-resolution training, accelerating model convergence; (2) NAMI leverages piecewise flow and spatial cascading of Diffusion Transformer (DiT) to rapidly generate images, reducing inference time by 40% to generate a 1024 resolution image; (3) We propose NAMI-1K benchmark to evaluate human preference performance, aiming to mitigate distributional bias and prevent data leakage from open-source benchmarks. The results show that our model is competitive with state-of-the-art models.",
    "arxiv_url": "http://arxiv.org/abs/2503.09242v1",
    "pdf_url": "http://arxiv.org/pdf/2503.09242v1",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "rectified flow",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space",
    "authors": [
      "Jian Zhu",
      "Zhengyu Jia",
      "Tian Gao",
      "Jiaxin Deng",
      "Shidi Li",
      "Fu Liu",
      "Peng Jia",
      "Xianpeng Lang",
      "Xiaolong Sun"
    ],
    "abstract": "Advanced end-to-end autonomous driving systems predict other vehicles' motions and plan ego vehicle's trajectory. The world model that can foresee the outcome of the trajectory has been used to evaluate the end-to-end autonomous driving system. However, existing world models predominantly emphasize the trajectory of the ego vehicle and leave other vehicles uncontrollable. This limitation hinders their ability to realistically simulate the interaction between the ego vehicle and the driving scenario. In addition, it remains a challenge to match multiple trajectories with each vehicle in the video to control the video generation. To address above issues, a driving World Model named EOT-WM is proposed in this paper, unifying Ego-Other vehicle Trajectories in videos. Specifically, we first project ego and other vehicle trajectories in the BEV space into the image coordinate to match each trajectory with its corresponding vehicle in the video. Then, trajectory videos are encoded by the Spatial-Temporal Variational Auto Encoder to align with driving video latents spatially and temporally in the unified visual space. A trajectory-injected diffusion Transformer is further designed to denoise the noisy video latents for video generation with the guidance of ego-other vehicle trajectories. In addition, we propose a metric based on control latent similarity to evaluate the controllability of trajectories. Extensive experiments are conducted on the nuScenes dataset, and the proposed model outperforms the state-of-the-art method by 30% in FID and 55% in FVD. The model can also predict unseen driving scenes with self-produced trajectories.",
    "arxiv_url": "http://arxiv.org/abs/2503.09215v2",
    "pdf_url": "http://arxiv.org/pdf/2503.09215v2",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Controllable",
      "Control",
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
    "authors": [
      "Hyeonho Jeong",
      "Suhyeon Lee",
      "Jong Chul Ye"
    ],
    "abstract": "We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: https://hyeonho99.github.io/reangle-a-video/",
    "arxiv_url": "http://arxiv.org/abs/2503.09151v2",
    "pdf_url": "http://arxiv.org/pdf/2503.09151v2",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Zero-Shot Subject-Centric Generation for Creative Application Using Entropy Fusion",
    "authors": [
      "Kaifeng Zou",
      "Xiaoyi Feng",
      "Peng Wang",
      "Tao Huang",
      "Zizhou Huang",
      "Zhang Haihang",
      "Yuntao Zou",
      "Dagang Li"
    ],
    "abstract": "Generative models are widely used in visual content creation. However, current text-to-image models often face challenges in practical applications-such as textile pattern design and meme generation-due to the presence of unwanted elements that are difficult to separate with existing methods. Meanwhile, subject-reference generation has emerged as a key research trend, highlighting the need for techniques that can produce clean, high-quality subject images while effectively removing extraneous components. To address this challenge, we introduce a framework for reliable subject-centric image generation. In this work, we propose an entropy-based feature-weighted fusion method to merge the informative cross-attention features obtained from each sampling step of the pretrained text-to-image model FLUX, enabling a precise mask prediction and subject-centric generation. Additionally, we have developed an agent framework based on Large Language Models (LLMs) that translates users' casual inputs into more descriptive prompts, leading to highly detailed image generation. Simultaneously, the agents extract primary elements of prompts to guide the entropy-based feature fusion, ensuring focused primary element generation without extraneous components. Experimental results and user studies demonstrate our methods generates high-quality subject-centric images, outperform existing methods or other possible pipelines, highlighting the effectiveness of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2503.10697v1",
    "pdf_url": "http://arxiv.org/pdf/2503.10697v1",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation",
    "authors": [
      "Junsong Chen",
      "Shuchen Xue",
      "Yuyang Zhao",
      "Jincheng Yu",
      "Sayak Paul",
      "Junyu Chen",
      "Han Cai",
      "Enze Xie",
      "Song Han"
    ],
    "abstract": "This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast text-to-image (T2I) generation. SANA-Sprint is built on a pre-trained foundation model and augmented with hybrid distillation, dramatically reducing inference steps from 20 to 1-4. We introduce three key innovations: (1) We propose a training-free approach that transforms a pre-trained flow-matching model for continuous-time consistency distillation (sCM), eliminating costly training from scratch and achieving high training efficiency. Our hybrid distillation strategy combines sCM with latent adversarial distillation (LADD): sCM ensures alignment with the teacher model, while LADD enhances single-step generation fidelity. (2) SANA-Sprint is a unified step-adaptive model that achieves high-quality generation in 1-4 steps, eliminating step-specific training and improving efficiency. (3) We integrate ControlNet with SANA-Sprint for real-time interactive image generation, enabling instant visual feedback for user interaction. SANA-Sprint establishes a new Pareto frontier in speed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID and 0.74 GenEval in only 1 step - outperforming FLUX-schnell (7.94 FID / 0.71 GenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s (T2I) and 0.25s (ControlNet) latency for 1024 x 1024 images on H100, and 0.31s (T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for AI-powered consumer applications (AIPC). Code and pre-trained models will be open-sourced.",
    "arxiv_url": "http://arxiv.org/abs/2503.09641v2",
    "pdf_url": "http://arxiv.org/pdf/2503.09641v2",
    "published_date": "2025-03-12",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder",
    "authors": [
      "Yitian Zhang",
      "Long Mai",
      "Aniruddha Mahapatra",
      "David Bourgin",
      "Yicong Hong",
      "Jonah Casebeer",
      "Feng Liu",
      "Yun Fu"
    ],
    "abstract": "We present a novel perspective on learning video embedders for generative modeling: rather than requiring an exact reproduction of an input video, an effective embedder should focus on synthesizing visually plausible reconstructions. This relaxed criterion enables substantial improvements in compression ratios without compromising the quality of downstream generative models. Specifically, we propose replacing the conventional encoder-decoder video embedder with an encoder-generator framework that employs a diffusion transformer (DiT) to synthesize missing details from a compact latent space. Therein, we develop a dedicated latent conditioning module to condition the DiT decoder on the encoded video latent embedding. Our experiments demonstrate that our approach enables superior encoding-decoding performance compared to state-of-the-art methods, particularly as the compression ratio increases. To demonstrate the efficacy of our approach, we report results from our video embedders achieving a temporal compression ratio of up to 32x (8x higher than leading video embedders) and validate the robustness of this ultra-compact latent space for text-to-video generation, providing a significant efficiency boost in latent diffusion model training and inference.",
    "arxiv_url": "http://arxiv.org/abs/2503.08665v1",
    "pdf_url": "http://arxiv.org/pdf/2503.08665v1",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OminiControl2: Efficient Conditioning for Diffusion Transformers",
    "authors": [
      "Zhenxiong Tan",
      "Qiaochu Xue",
      "Xingyi Yang",
      "Songhua Liu",
      "Xinchao Wang"
    ],
    "abstract": "Fine-grained control of text-to-image diffusion transformer models (DiT) remains a critical challenge for practical deployment. While recent advances such as OminiControl and others have enabled a controllable generation of diverse control signals, these methods face significant computational inefficiency when handling long conditional inputs. We present OminiControl2, an efficient framework that achieves efficient image-conditional image generation. OminiControl2 introduces two key innovations: (1) a dynamic compression strategy that streamlines conditional inputs by preserving only the most semantically relevant tokens during generation, and (2) a conditional feature reuse mechanism that computes condition token features only once and reuses them across denoising steps. These architectural improvements preserve the original framework's parameter efficiency and multi-modal versatility while dramatically reducing computational costs. Our experiments demonstrate that OminiControl2 reduces conditional processing overhead by over 90% compared to its predecessor, achieving an overall 5.9$\\times$ speedup in multi-conditional generation scenarios. This efficiency enables the practical implementation of complex, multi-modal control for high-quality image synthesis with DiT models.",
    "arxiv_url": "http://arxiv.org/abs/2503.08280v1",
    "pdf_url": "http://arxiv.org/pdf/2503.08280v1",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "Control",
      "Controllable",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seedream 2.0: A Native Chinese-English Bilingual Image Generation Foundation Model",
    "authors": [
      "Lixue Gong",
      "Xiaoxia Hou",
      "Fanshi Li",
      "Liang Li",
      "Xiaochen Lian",
      "Fei Liu",
      "Liyang Liu",
      "Wei Liu",
      "Wei Lu",
      "Yichun Shi",
      "Shiqi Sun",
      "Yu Tian",
      "Zhi Tian",
      "Peng Wang",
      "Xun Wang",
      "Ye Wang",
      "Guofeng Wu",
      "Jie Wu",
      "Xin Xia",
      "Xuefeng Xiao",
      "Linjie Yang",
      "Zhonghua Zhai",
      "Xinyu Zhang",
      "Qi Zhang",
      "Yuwei Zhang",
      "Shijia Zhao",
      "Jianchao Yang",
      "Weilin Huang"
    ],
    "abstract": "Rapid advancement of diffusion models has catalyzed remarkable progress in the field of image generation. However, prevalent models such as Flux, SD3.5 and Midjourney, still grapple with issues like model bias, limited text rendering capabilities, and insufficient understanding of Chinese cultural nuances. To address these limitations, we present Seedream 2.0, a native Chinese-English bilingual image generation foundation model that excels across diverse dimensions, which adeptly manages text prompt in both Chinese and English, supporting bilingual image generation and text rendering. We develop a powerful data system that facilitates knowledge integration, and a caption system that balances the accuracy and richness for image description. Particularly, Seedream is integrated with a self-developed bilingual large language model as a text encoder, allowing it to learn native knowledge directly from massive data. This enable it to generate high-fidelity images with accurate cultural nuances and aesthetic expressions described in either Chinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible character-level text rendering, while a Scaled ROPE generalizes well to untrained resolutions. Multi-phase post-training optimizations, including SFT and RLHF iterations, further improve the overall capability. Through extensive experimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art performance across multiple aspects, including prompt-following, aesthetics, text rendering, and structural correctness. Furthermore, Seedream 2.0 has been optimized through multiple RLHF iterations to closely align its output with human preferences, as revealed by its outstanding ELO score. In addition, it can be readily adapted to an instruction-based image editing model, such as SeedEdit, with strong editing capability that balances instruction-following and image consistency.",
    "arxiv_url": "http://arxiv.org/abs/2503.07703v1",
    "pdf_url": "http://arxiv.org/pdf/2503.07703v1",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image editing",
      "FLUX",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VACE: All-in-One Video Creation and Editing",
    "authors": [
      "Zeyinzi Jiang",
      "Zhen Han",
      "Chaojie Mao",
      "Jingfeng Zhang",
      "Yulin Pan",
      "Yu Liu"
    ],
    "abstract": "Diffusion Transformer has demonstrated powerful capability and scalability in generating high-quality images and videos. Further pursuing the unification of generation and editing tasks has yielded significant progress in the domain of image content creation. However, due to the intrinsic demands for consistency across both temporal and spatial dynamics, achieving a unified approach for video synthesis remains challenging. We introduce VACE, which enables users to perform Video tasks within an All-in-one framework for Creation and Editing. These tasks include reference-to-video generation, video-to-video editing, and masked video-to-video editing. Specifically, we effectively integrate the requirements of various tasks by organizing video task inputs, such as editing, reference, and masking, into a unified interface referred to as the Video Condition Unit (VCU). Furthermore, by utilizing a Context Adapter structure, we inject different task concepts into the model using formalized representations of temporal and spatial dimensions, allowing it to handle arbitrary video synthesis tasks flexibly. Extensive experiments demonstrate that the unified model of VACE achieves performance on par with task-specific models across various subtasks. Simultaneously, it enables diverse applications through versatile task combinations. Project page: https://ali-vilab.github.io/VACE-Page/.",
    "arxiv_url": "http://arxiv.org/abs/2503.07598v2",
    "pdf_url": "http://arxiv.org/pdf/2503.07598v2",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video editing",
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TIDE : Temporal-Aware Sparse Autoencoders for Interpretable Diffusion Transformers in Image Generation",
    "authors": [
      "Victor Shea-Jay Huang",
      "Le Zhuo",
      "Yi Xin",
      "Zhaokai Wang",
      "Peng Gao",
      "Hongsheng Li"
    ],
    "abstract": "Diffusion Transformers (DiTs) are a powerful yet underexplored class of generative models compared to U-Net-based diffusion models. To bridge this gap, we introduce TIDE (Temporal-aware Sparse Autoencoders for Interpretable Diffusion transformErs), a novel framework that enhances temporal reconstruction within DiT activation layers across denoising steps. TIDE employs Sparse Autoencoders (SAEs) with a sparse bottleneck layer to extract interpretable and hierarchical features, revealing that diffusion models inherently learn hierarchical features at multiple levels (e.g., 3D, semantic, class) during generative pre-training. Our approach achieves state-of-the-art reconstruction performance, with a mean squared error (MSE) of 1e-3 and a cosine similarity of 0.97, demonstrating superior accuracy in capturing activation dynamics along the denoising trajectory. Beyond interpretability, we showcase TIDE's potential in downstream applications such as sparse activation-guided image editing and style transfer, enabling improved controllability for generative systems. By providing a comprehensive training and evaluation protocol tailored for DiTs, TIDE contributes to developing more interpretable, transparent, and trustworthy generative models.",
    "arxiv_url": "http://arxiv.org/abs/2503.07050v1",
    "pdf_url": "http://arxiv.org/pdf/2503.07050v1",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "image editing",
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Post-Training Quantization for Diffusion Transformer via Hierarchical Timestep Grouping",
    "authors": [
      "Ning Ding",
      "Jing Han",
      "Yuchuan Tian",
      "Chao Xu",
      "Kai Han",
      "Yehui Tang"
    ],
    "abstract": "Diffusion Transformer (DiT) has now become the preferred choice for building image generation models due to its great generation capability. Unlike previous convolution-based UNet models, DiT is purely composed of a stack of transformer blocks, which renders DiT excellent in scalability like large language models. However, the growing model size and multi-step sampling paradigm bring about considerable pressure on deployment and inference. In this work, we propose a post-training quantization framework tailored for Diffusion Transforms to tackle these challenges. We firstly locate that the quantization difficulty of DiT mainly originates from the time-dependent channel-specific outliers. We propose a timestep-aware shift-and-scale strategy to smooth the activation distribution to reduce the quantization error. Secondly, based on the observation that activations of adjacent timesteps have similar distributions, we utilize a hierarchical clustering scheme to divide the denoising timesteps into multiple groups. We further design a re-parameterization scheme which absorbs the quantization parameters into nearby module to avoid redundant computations. Comprehensive experiments demonstrate that out PTQ method successfully quantize the Diffusion Transformer into 8-bit weight and 8-bit activation (W8A8) with state-of-the-art FiD score. And our method can further quantize DiT model into 4-bit weight and 8-bit activation (W4A8) without sacrificing generation quality.",
    "arxiv_url": "http://arxiv.org/abs/2503.06930v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06930v1",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical Latent and Layer Caching for Video Generation",
    "authors": [
      "Junyi Wu",
      "Zhiteng Li",
      "Zheng Hui",
      "Yulun Zhang",
      "Linghe Kong",
      "Xiaokang Yang"
    ],
    "abstract": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant architecture in video generation, surpassing U-Net-based models in terms of performance. However, the enhanced capabilities of DiTs come with significant drawbacks, including increased computational and memory costs, which hinder their deployment on resource-constrained devices. Current acceleration techniques, such as quantization and cache mechanism, offer limited speedup and are often applied in isolation, failing to fully address the complexities of DiT architectures. In this paper, we propose QuantCache, a novel training-free inference acceleration framework that jointly optimizes hierarchical latent caching, adaptive importance-guided quantization, and structural redundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of 6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive experiments across multiple video generation benchmarks demonstrate the effectiveness of our method, setting a new standard for efficient DiT inference. The code and models will be available at https://github.com/JunyiWuCode/QuantCache.",
    "arxiv_url": "http://arxiv.org/abs/2503.06545v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06545v1",
    "published_date": "2025-03-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/JunyiWuCode/QuantCache",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Get In Video: Add Anything You Want to the Video",
    "authors": [
      "Shaobin Zhuang",
      "Zhipeng Huang",
      "Binxin Yang",
      "Ying Zhang",
      "Fangyikang Wang",
      "Canmiao Fu",
      "Chong Sun",
      "Zheng-Jun Zha",
      "Chen Li",
      "Yali Wang"
    ],
    "abstract": "Video editing increasingly demands the ability to incorporate specific real-world instances into existing footage, yet current approaches fundamentally fail to capture the unique visual characteristics of particular subjects and ensure natural instance/scene interactions. We formalize this overlooked yet critical editing paradigm as \"Get-In-Video Editing\", where users provide reference images to precisely specify visual elements they wish to incorporate into videos. Addressing this task's dual challenges, severe training data scarcity and technical challenges in maintaining spatiotemporal coherence, we introduce three key contributions. First, we develop GetIn-1M dataset created through our automated Recognize-Track-Erase pipeline, which sequentially performs video captioning, salient instance identification, object detection, temporal tracking, and instance removal to generate high-quality video editing pairs with comprehensive annotations (reference image, tracking mask, instance prompt). Second, we present GetInVideo, a novel end-to-end framework that leverages a diffusion transformer architecture with 3D full attention to process reference images, condition videos, and masks simultaneously, maintaining temporal coherence, preserving visual identity, and ensuring natural scene interactions when integrating reference objects into videos. Finally, we establish GetInBench, the first comprehensive benchmark for Get-In-Video Editing scenario, demonstrating our approach's superior performance through extensive evaluations. Our work enables accessible, high-quality incorporation of specific real-world subjects into videos, significantly advancing personalized video editing capabilities.",
    "arxiv_url": "http://arxiv.org/abs/2503.06268v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06268v1",
    "published_date": "2025-03-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video editing",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "X2I: Seamless Integration of Multimodal Understanding into Diffusion Transformer via Attention Distillation",
    "authors": [
      "Jian Ma",
      "Qirong Peng",
      "Xu Guo",
      "Chen Chen",
      "Haonan Lu",
      "Zhenyu Yang"
    ],
    "abstract": "Text-to-image (T2I) models are well known for their ability to produce highly realistic images, while multimodal large language models (MLLMs) are renowned for their proficiency in understanding and integrating multiple modalities. However, currently there is no straightforward and efficient framework to transfer the multimodal comprehension abilities of MLLMs to T2I models to enable them to understand multimodal inputs. In this paper, we propose the X2I framework, which endows Diffusion Transformer (DiT) models with the capability to comprehend various modalities, including multilingual text, screenshot documents, images, videos, and audio. X2I is trained using merely 100K English corpus with 160 GPU hours. Building on the DiT teacher model, we adopt an innovative distillation method to extract the inference capabilities of the teacher model and design a lightweight AlignNet structure to serve as an intermediate bridge. Compared to the teacher model, X2I shows a decrease in performance degradation of less than 1\\% while gaining various multimodal understanding abilities, including multilingual to image, image to image, image-text to image, video to image, audio to image, and utilizing creative fusion to enhance imagery. Furthermore, it is applicable for LoRA training in the context of image-text to image generation, filling a void in the industry in this area. We further design a simple LightControl to enhance the fidelity of instructional image editing. Finally, extensive experiments demonstrate the effectiveness, efficiency, multifunctionality, and transferability of our X2I. The open-source code and checkpoints for X2I can be found at the following link: https://github.com/OPPO-Mente-Lab/X2I.",
    "arxiv_url": "http://arxiv.org/abs/2503.06134v2",
    "pdf_url": "http://arxiv.org/pdf/2503.06134v2",
    "published_date": "2025-03-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/OPPO-Mente-Lab/X2I",
    "keywords": [
      "image generation",
      "image editing",
      "image to image",
      "Control",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MagicInfinite: Generating Infinite Talking Videos with Your Words and Voice",
    "authors": [
      "Hongwei Yi",
      "Tian Ye",
      "Shitong Shao",
      "Xuancheng Yang",
      "Jiantong Zhao",
      "Hanzhong Guo",
      "Terrance Wang",
      "Qingyu Yin",
      "Zeke Xie",
      "Lei Zhu",
      "Wei Li",
      "Michael Lingelbach",
      "Daquan Zhou"
    ],
    "abstract": "We present MagicInfinite, a novel diffusion Transformer (DiT) framework that overcomes traditional portrait animation limitations, delivering high-fidelity results across diverse character types-realistic humans, full-body figures, and stylized anime characters. It supports varied facial poses, including back-facing views, and animates single or multiple characters with input masks for precise speaker designation in multi-character scenes. Our approach tackles key challenges with three innovations: (1) 3D full-attention mechanisms with a sliding window denoising strategy, enabling infinite video generation with temporal coherence and visual quality across diverse character styles; (2) a two-stage curriculum learning scheme, integrating audio for lip sync, text for expressive dynamics, and reference images for identity preservation, enabling flexible multi-modal control over long sequences; and (3) region-specific masks with adaptive loss functions to balance global textual control and local audio guidance, supporting speaker-specific animations. Efficiency is enhanced via our innovative unified step and cfg distillation techniques, achieving a 20x inference speed boost over the basemodel: generating a 10 second 540x540p video in 10 seconds or 720x720p in 30 seconds on 8 H100 GPUs, without quality loss. Evaluations on our new benchmark demonstrate MagicInfinite's superiority in audio-lip synchronization, identity preservation, and motion naturalness across diverse scenarios. It is publicly available at https://www.hedra.com/, with examples at https://magicinfinite.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2503.05978v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05978v1",
    "published_date": "2025-03-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Video Super-Resolution: All You Need is a Video Diffusion Model",
    "authors": [
      "Zhihao Zhan",
      "Wang Pang",
      "Xiang Zhu",
      "Yechao Bai"
    ],
    "abstract": "We present a generic video super-resolution algorithm in this paper, based on the Diffusion Posterior Sampling framework with an unconditional video generation model in latent space. The video generation model, a diffusion transformer, functions as a space-time model. We argue that a powerful model, which learns the physics of the real world, can easily handle various kinds of motion patterns as prior knowledge, thus eliminating the need for explicit estimation of optical flows or motion parameters for pixel alignment. Furthermore, a single instance of the proposed video diffusion transformer model can adapt to different sampling conditions without re-training. Empirical results on synthetic and real-world datasets demonstrate that our method has strong capabilities to address video super-resolution challenges.",
    "arxiv_url": "http://arxiv.org/abs/2503.03355v3",
    "pdf_url": "http://arxiv.org/pdf/2503.03355v3",
    "published_date": "2025-03-05",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Q&C: When Quantization Meets Cache in Efficient Image Generation",
    "authors": [
      "Xin Ding",
      "Xin Li",
      "Haotong Qin",
      "Zhibo Chen"
    ],
    "abstract": "Quantization and cache mechanisms are typically applied individually for efficient Diffusion Transformers (DiTs), each demonstrating notable potential for acceleration. However, the promoting effect of combining the two mechanisms on efficient generation remains under-explored. Through empirical investigation, we find that the combination of quantization and cache mechanisms for DiT is not straightforward, and two key challenges lead to severe catastrophic performance degradation: (i) the sample efficacy of calibration datasets in post-training quantization (PTQ) is significantly eliminated by cache operation; (ii) the combination of the above mechanisms introduces more severe exposure bias within sampling distribution, resulting in amplified error accumulation in the image generation process. In this work, we take advantage of these two acceleration mechanisms and propose a hybrid acceleration method by tackling the above challenges, aiming to further improve the efficiency of DiTs while maintaining excellent generation capability. Concretely, a temporal-aware parallel clustering (TAP) is designed to dynamically improve the sample selection efficacy for the calibration within PTQ for different diffusion steps. A variance compensation (VC) strategy is derived to correct the sampling distribution. It mitigates exposure bias through an adaptive correction factor generation. Extensive experiments have shown that our method has accelerated DiTs by 12.7x while preserving competitive generation capability. The code will be available at https://github.com/xinding-sys/Quant-Cache.",
    "arxiv_url": "http://arxiv.org/abs/2503.02508v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02508v1",
    "published_date": "2025-03-04",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "https://github.com/xinding-sys/Quant-Cache",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video Generation",
    "authors": [
      "Yifei Xia",
      "Suhan Ling",
      "Fangcheng Fu",
      "Yujie Wang",
      "Huixia Li",
      "Xuefeng Xiao",
      "Bin Cui"
    ],
    "abstract": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is often hindered by significant latency, primarily due to the computational demands of attention mechanisms. For instance, generating an 8-second 720p video (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500 PFLOPs consumed by attention computations. To address this issue, we propose AdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention method. Firstly, to realize the Dynamic Pattern, we introduce a blockified pattern to efficiently capture the hierarchical sparsity inherent in DiTs. This is based on our observation that sparse characteristics of DiTs exhibit hierarchical and blockified structures between and within different modalities. This blockified approach significantly reduces the complexity of attention computation while maintaining high fidelity in the generated videos. Secondly, to enable Online Precise Search, we propose the Fused LSE-Cached Search with Head-adaptive Hierarchical Block Sparse Attention. This method is motivated by our finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and heads, but remain invariant across denoising steps. By leveraging this invariance across denoising steps, it adapts to the dynamic nature of DiTs and allows for precise, real-time identification of sparse indices with minimal overhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can be integrated seamlessly with existing DiTs, requiring neither additional fine-tuning nor a dataset-dependent profiling. Extensive experiments validate that AdaSpa delivers substantial acceleration across various models while preserving video quality, establishing itself as a robust and scalable approach to efficient video generation.",
    "arxiv_url": "http://arxiv.org/abs/2502.21079v1",
    "pdf_url": "http://arxiv.org/pdf/2502.21079v1",
    "published_date": "2025-02-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think",
    "authors": [
      "Liang Chen",
      "Shuai Bai",
      "Wenhao Chai",
      "Weichu Xie",
      "Haozhe Zhao",
      "Leon Vinci",
      "Junyang Lin",
      "Baobao Chang"
    ],
    "abstract": "The field of advanced text-to-image generation is witnessing the emergence of unified frameworks that integrate powerful text encoders, such as CLIP and T5, with Diffusion Transformer backbones. Although there have been efforts to control output images with additional conditions, like canny and depth map, a comprehensive framework for arbitrary text-image interleaved control is still lacking. This gap is especially evident when attempting to merge concepts or visual elements from multiple images in the generation process. To mitigate the gap, we conducted preliminary experiments showing that large multimodal models (LMMs) offer an effective shared representation space, where image and text can be well-aligned to serve as a condition for external diffusion models. Based on this discovery, we propose Dream Engine, an efficient and unified framework designed for arbitrary text-image interleaved control in image generation models. Building on powerful text-to-image models like SD3.5, we replace the original text-only encoders by incorporating versatile multimodal information encoders such as QwenVL. Our approach utilizes a two-stage training paradigm, consisting of joint text-image alignment and multimodal interleaved instruction tuning. Our experiments demonstrate that this training method is effective, achieving a 0.69 overall score on the GenEval benchmark, and matching the performance of state-of-the-art text-to-image models like SD3.5 and FLUX.",
    "arxiv_url": "http://arxiv.org/abs/2502.20172v1",
    "pdf_url": "http://arxiv.org/pdf/2502.20172v1",
    "published_date": "2025-02-27",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "Control",
      "FLUX",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute",
    "authors": [
      "Sotiris Anagnostidis",
      "Gregor Bachmann",
      "Yeongmin Kim",
      "Jonas Kohler",
      "Markos Georgopoulos",
      "Artsiom Sanakoyeu",
      "Yuming Du",
      "Albert Pumarola",
      "Ali Thabet",
      "Edgar Schnfeld"
    ],
    "abstract": "Despite their remarkable performance, modern Diffusion Transformers are hindered by substantial resource requirements during inference, stemming from the fixed and large amount of compute needed for each denoising step. In this work, we revisit the conventional static paradigm that allocates a fixed compute budget per denoising iteration and propose a dynamic strategy instead. Our simple and sample-efficient framework enables pre-trained DiT models to be converted into \\emph{flexible} ones -- dubbed FlexiDiT -- allowing them to process inputs at varying compute budgets. We demonstrate how a single \\emph{flexible} model can generate images without any drop in quality, while reducing the required FLOPs by more than $40$\\% compared to their static counterparts, for both class-conditioned and text-conditioned image generation. Our method is general and agnostic to input and conditioning modalities. We show how our approach can be readily extended for video generation, where FlexiDiT models generate samples with up to $75$\\% less compute without compromising performance.",
    "arxiv_url": "http://arxiv.org/abs/2502.20126v1",
    "pdf_url": "http://arxiv.org/pdf/2502.20126v1",
    "published_date": "2025-02-27",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers",
    "authors": [
      "Min Zhao",
      "Guande He",
      "Yixiao Chen",
      "Hongzhou Zhu",
      "Chongxuan Li",
      "Jun Zhu"
    ],
    "abstract": "Recent advancements in video generation have enabled models to synthesize high-quality, minute-long videos. However, generating even longer videos with temporal coherence remains a major challenge, and existing length extrapolation methods lead to temporal repetition or motion deceleration. In this work, we systematically analyze the role of frequency components in positional embeddings and identify an intrinsic frequency that primarily governs extrapolation behavior. Based on this insight, we propose RIFLEx, a minimal yet effective approach that reduces the intrinsic frequency to suppress repetition while preserving motion consistency, without requiring any additional modifications. RIFLEx offers a true free lunch--achieving high-quality $2\\times$ extrapolation on state-of-the-art video diffusion transformers in a completely training-free manner. Moreover, it enhances quality and enables $3\\times$ extrapolation by minimal fine-tuning without long videos. Project page and codes: \\href{https://riflex-video.github.io/}{https://riflex-video.github.io/.}",
    "arxiv_url": "http://arxiv.org/abs/2502.15894v1",
    "pdf_url": "http://arxiv.org/pdf/2502.15894v1",
    "published_date": "2025-02-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hardware-Friendly Static Quantization Method for Video Diffusion Transformers",
    "authors": [
      "Sanghyun Yi",
      "Qingfeng Liu",
      "Mostafa El-Khamy"
    ],
    "abstract": "Diffusion Transformers for video generation have gained significant research interest since the impressive performance of SORA. Efficient deployment of such generative-AI models on GPUs has been demonstrated with dynamic quantization. However, resource-constrained devices cannot support dynamic quantization, and need static quantization of the models for their efficient deployment on AI processors. In this paper, we propose a novel method for the post-training quantization of OpenSora\\cite{opensora}, a Video Diffusion Transformer, without relying on dynamic quantization techniques. Our approach employs static quantization, achieving video quality comparable to FP16 and dynamically quantized ViDiT-Q methods, as measured by CLIP, and VQA metrics. In particular, we utilize per-step calibration data to adequately provide a post-training statically quantized model for each time step, incorporating channel-wise quantization for weights and tensor-wise quantization for activations. By further applying the smooth-quantization technique, we can obtain high-quality video outputs with the statically quantized models. Extensive experimental results demonstrate that static quantization can be a viable alternative to dynamic quantization for video diffusion transformers, offering a more efficient approach without sacrificing performance.",
    "arxiv_url": "http://arxiv.org/abs/2502.15077v1",
    "pdf_url": "http://arxiv.org/pdf/2502.15077v1",
    "published_date": "2025-02-20",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers",
    "authors": [
      "Ke Cao",
      "Jing Wang",
      "Ao Ma",
      "Jiasong Feng",
      "Zhanjie Zhang",
      "Xuanhua He",
      "Shanyuan Liu",
      "Bo Cheng",
      "Dawei Leng",
      "Yuhui Yin",
      "Jie Zhang"
    ],
    "abstract": "The Diffusion Transformer plays a pivotal role in advancing text-to-image and text-to-video generation, owing primarily to its inherent scalability. However, existing controlled diffusion transformer methods incur significant parameter and computational overheads and suffer from inefficient resource allocation due to their failure to account for the varying relevance of control information across different transformer layers. To address this, we propose the Relevance-Guided Efficient Controllable Generation framework, RelaCtrl, enabling efficient and resource-optimized integration of control signals into the Diffusion Transformer. First, we evaluate the relevance of each layer in the Diffusion Transformer to the control information by assessing the \"ControlNet Relevance Score\"-i.e., the impact of skipping each control layer on both the quality of generation and the control effectiveness during inference. Based on the strength of the relevance, we then tailor the positioning, parameter scale, and modeling capacity of the control layers to reduce unnecessary parameters and redundant computations. Additionally, to further improve efficiency, we replace the self-attention and FFN in the commonly used copy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM), enabling efficient implementation of both the token mixer and channel mixer. Both qualitative and quantitative experimental results demonstrate that our approach achieves superior performance with only 15% of the parameters and computational complexity compared to PixArt-delta.",
    "arxiv_url": "http://arxiv.org/abs/2502.14377v4",
    "pdf_url": "http://arxiv.org/pdf/2502.14377v4",
    "published_date": "2025-02-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "Control",
      "Controllable",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Designing Parameter and Compute Efficient Diffusion Transformers using Distillation",
    "authors": [
      "Vignesh Sundaresha"
    ],
    "abstract": "Diffusion Transformers (DiTs) with billions of model parameters form the backbone of popular image and video generation models like DALL.E, Stable-Diffusion and SORA. Though these models are necessary in many low-latency applications like Augmented/Virtual Reality, they cannot be deployed on resource-constrained Edge devices (like Apple Vision Pro or Meta Ray-Ban glasses) due to their huge computational complexity. To overcome this, we turn to knowledge distillation and perform a thorough design-space exploration to achieve the best DiT for a given parameter size. In particular, we provide principles for how to choose design knobs such as depth, width, attention heads and distillation setup for a DiT. During the process, a three-way trade-off emerges between model performance, size and speed that is crucial for Edge implementation of diffusion. We also propose two distillation approaches - Teaching Assistant (TA) method and Multi-In-One (MI1) method - to perform feature distillation in the DiT context. Unlike existing solutions, we demonstrate and benchmark the efficacy of our approaches on practical Edge devices such as NVIDIA Jetson Orin Nano.",
    "arxiv_url": "http://arxiv.org/abs/2502.14226v1",
    "pdf_url": "http://arxiv.org/pdf/2502.14226v1",
    "published_date": "2025-02-20",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation",
    "authors": [
      "Yunpeng Zhang",
      "Qiang Wang",
      "Fan Jiang",
      "Yaqi Fan",
      "Mu Xu",
      "Yonggang Qi"
    ],
    "abstract": "Tuning-free approaches adapting large-scale pre-trained video diffusion models for identity-preserving text-to-video generation (IPT2V) have gained popularity recently due to their efficacy and scalability. However, significant challenges remain to achieve satisfied facial dynamics while keeping the identity unchanged. In this work, we present a novel tuning-free IPT2V framework by enhancing face knowledge of the pre-trained video model built on diffusion transformers (DiT), dubbed FantasyID. Essentially, 3D facial geometry prior is incorporated to ensure plausible facial structures during video synthesis. To prevent the model from learning copy-paste shortcuts that simply replicate reference face across frames, a multi-view face augmentation strategy is devised to capture diverse 2D facial appearance features, hence increasing the dynamics over the facial expressions and head poses. Additionally, after blending the 2D and 3D features as guidance, instead of naively employing cross-attention to inject guidance cues into DiT layers, a learnable layer-aware adaptive mechanism is employed to selectively inject the fused features into each individual DiT layers, facilitating balanced modeling of identity preservation and motion dynamics. Experimental results validate our model's superiority over the current tuning-free IPT2V methods.",
    "arxiv_url": "http://arxiv.org/abs/2502.13995v1",
    "pdf_url": "http://arxiv.org/pdf/2502.13995v1",
    "published_date": "2025-02-19",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AnyRefill: A Unified, Data-Efficient Framework for Left-Prompt-Guided Vision Tasks",
    "authors": [
      "Ming Xie",
      "Chenjie Cao",
      "Yunuo Cai",
      "Xiangyang Xue",
      "Yu-Gang Jiang",
      "Yanwei Fu"
    ],
    "abstract": "In this paper, we present a novel Left-Prompt-Guided (LPG) paradigm to address a diverse range of reference-based vision tasks. Inspired by the human creative process, we reformulate these tasks using a left-right stitching formulation to construct contextual input. Building upon this foundation, we propose AnyRefill, an extension of LeftRefill, that effectively adapts Text-to-Image (T2I) models to various vision tasks. AnyRefill leverages the inpainting priors of advanced T2I model based on the Diffusion Transformer (DiT) architecture, and incorporates flexible components to enhance its capabilities. By combining task-specific LoRAs with the stitching input, AnyRefill unlocks its potential across diverse tasks, including conditional generation, visual perception, and image editing, without requiring additional visual encoders. Meanwhile, AnyRefill exhibits remarkable data efficiency, requiring minimal task-specific fine-tuning while maintaining high generative performance. Through extensive ablation studies, we demonstrate that AnyRefill outperforms other image condition injection methods and achieves competitive results compared to state-of-the-art open-source methods. Notably, AnyRefill delivers results comparable to advanced commercial tools, such as IC-Light and SeedEdit, even in challenging scenarios. Comprehensive experiments and ablation studies across versatile tasks validate the strong generation of the proposed simple yet effective LPG formulation, establishing AnyRefill as a unified, highly data-efficient solution for reference-based vision tasks.",
    "arxiv_url": "http://arxiv.org/abs/2502.11158v2",
    "pdf_url": "http://arxiv.org/pdf/2502.11158v2",
    "published_date": "2025-02-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image editing",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BCDDM: Branch-Corrected Denoising Diffusion Model for Black Hole Image Generation",
    "authors": [
      "Ao liu",
      "Zelin Zhang",
      "Songbai Chen",
      "Cuihong Wen"
    ],
    "abstract": "The properties of black holes and accretion flows can be inferred by fitting Event Horizon Telescope (EHT) data to simulated images generated through general relativistic ray tracing (GRRT). However, due to the computationally intensive nature of GRRT, the efficiency of generating specific radiation flux images needs to be improved. This paper introduces the Branch Correction Denoising Diffusion Model (BCDDM), which uses a branch correction mechanism and a weighted mixed loss function to improve the accuracy of generated black hole images based on seven physical parameters of the radiatively inefficient accretion flow (RIAF) model. Our experiments show a strong correlation between the generated images and their physical parameters. By enhancing the GRRT dataset with BCDDM-generated images and using ResNet50 for parameter regression, we achieve significant improvements in parameter prediction performance. This approach reduces computational costs and provides a faster, more efficient method for dataset expansion, parameter estimation, and model fitting.",
    "arxiv_url": "http://arxiv.org/abs/2502.08528v1",
    "pdf_url": "http://arxiv.org/pdf/2502.08528v1",
    "published_date": "2025-02-12",
    "categories": [
      "astro-ph.GA",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT",
    "authors": [
      "Dongyang Liu",
      "Shicheng Li",
      "Yutong Liu",
      "Zhen Li",
      "Kai Wang",
      "Xinyue Li",
      "Qi Qin",
      "Yufei Liu",
      "Yi Xin",
      "Zhongyu Li",
      "Bin Fu",
      "Chenyang Si",
      "Yuewen Cao",
      "Conghui He",
      "Ziwei Liu",
      "Yu Qiao",
      "Qibin Hou",
      "Hongsheng Li",
      "Peng Gao"
    ],
    "abstract": "Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data. To address this, we introduce Lumina-Video, a framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis. Lumina-Video incorporates a Multi-scale Next-DiT architecture, which jointly learns multiple patchifications to enhance both efficiency and flexibility. By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos' dynamic degree. Combined with a progressive training scheme with increasingly higher resolution and FPS, and a multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency. We additionally propose Lumina-V2A, a video-to-audio model based on Next-DiT, to create synchronized sounds for generated videos. Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.",
    "arxiv_url": "http://arxiv.org/abs/2502.06782v2",
    "pdf_url": "http://arxiv.org/pdf/2502.06782v2",
    "published_date": "2025-02-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Alpha-VLLM/Lumina-Video",
    "keywords": [
      "Control",
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers",
    "authors": [
      "D. She",
      "Mushui Liu",
      "Jingxuan Pang",
      "Jin Wang",
      "Zhen Yang",
      "Wanggui He",
      "Guanghao Zhang",
      "Yi Wang",
      "Qihan Huang",
      "Haobin Tang",
      "Yunlong Yu",
      "Siming Fu"
    ],
    "abstract": "Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffusion transformer for personalized video generation from a reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish a new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality.",
    "arxiv_url": "http://arxiv.org/abs/2502.06527v2",
    "pdf_url": "http://arxiv.org/pdf/2502.06527v2",
    "published_date": "2025-02-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Universal Approximation of Visual Autoregressive Transformers",
    "authors": [
      "Yifang Chen",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song"
    ],
    "abstract": "We investigate the fundamental limits of transformer-based foundation models, extending our analysis to include Visual Autoregressive (VAR) transformers. VAR represents a big step toward generating images using a novel, scalable, coarse-to-fine ``next-scale prediction'' framework. These models set a new quality bar, outperforming all previous methods, including Diffusion Transformers, while having state-of-the-art performance for image synthesis tasks. Our primary contributions establish that, for single-head VAR transformers with a single self-attention layer and single interpolation layer, the VAR Transformer is universal. From the statistical perspective, we prove that such simple VAR transformers are universal approximators for any image-to-image Lipschitz functions. Furthermore, we demonstrate that flow-based autoregressive transformers inherit similar approximation capabilities. Our results provide important design principles for effective and computationally efficient VAR Transformer strategies that can be used to extend their utility to more sophisticated VAR models in image generation and other related areas.",
    "arxiv_url": "http://arxiv.org/abs/2502.06167v1",
    "pdf_url": "http://arxiv.org/pdf/2502.06167v1",
    "published_date": "2025-02-10",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile",
    "authors": [
      "Hangliang Ding",
      "Dacheng Li",
      "Runlong Su",
      "Peiyuan Zhang",
      "Zhijie Deng",
      "Ion Stoica",
      "Hao Zhang"
    ],
    "abstract": "Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.",
    "arxiv_url": "http://arxiv.org/abs/2502.06155v2",
    "pdf_url": "http://arxiv.org/pdf/2502.06155v2",
    "published_date": "2025-02-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation",
    "authors": [
      "Qijun Gan",
      "Yi Ren",
      "Chen Zhang",
      "Zhenhui Ye",
      "Pan Xie",
      "Xiang Yin",
      "Zehuan Yuan",
      "Bingyue Peng",
      "Jianke Zhu"
    ],
    "abstract": "Human motion video generation has advanced significantly, while existing methods still struggle with accurately rendering detailed body parts like hands and faces, especially in long sequences and intricate motions. Current approaches also rely on fixed resolution and struggle to maintain visual consistency. To address these limitations, we propose HumanDiT, a pose-guided Diffusion Transformer (DiT)-based framework trained on a large and wild dataset containing 14,000 hours of high-quality video to produce high-fidelity videos with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT, supports numerous video resolutions and variable sequence lengths, facilitating learning for long-sequence video generation; (ii) we introduce a prefix-latent reference strategy to maintain personalized characteristics across extended sequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to generate subsequent pose sequences, facilitating video continuation from static images or existing videos. It also utilizes a Pose Adapter to enable pose transfer with given sequences. Extensive experiments demonstrate its superior performance in generating long-form, pose-accurate videos across diverse scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2502.04847v2",
    "pdf_url": "http://arxiv.org/pdf/2502.04847v2",
    "published_date": "2025-02-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fast Video Generation with Sliding Tile Attention",
    "authors": [
      "Peiyuan Zhang",
      "Yongqi Chen",
      "Runlong Su",
      "Hangliang Ding",
      "Ion Stoica",
      "Zhenghong Liu",
      "Hao Zhang"
    ],
    "abstract": "Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being hardware-efficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09% drop on VBench.",
    "arxiv_url": "http://arxiv.org/abs/2502.04507v1",
    "pdf_url": "http://arxiv.org/pdf/2502.04507v1",
    "published_date": "2025-02-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniForm: A Unified Diffusion Transformer for Audio-Video Generation",
    "authors": [
      "Lei Zhao",
      "Linfeng Feng",
      "Dongxu Ge",
      "Fangqiu Yi",
      "Chi Zhang",
      "Xiao-Lei Zhang",
      "Xuelong Li"
    ],
    "abstract": "As a natural multimodal content, audible video delivers an immersive sensory experience. Consequently, audio-video generation systems have substantial potential. However, existing diffusion-based studies mainly employ relatively independent modules for generating each modality, which lack exploration of shared-weight generative modules. This approach may under-use the intrinsic correlations between audio and visual modalities, potentially resulting in sub-optimal generation quality. To address this, we propose UniForm, a unified diffusion transformer designed to enhance cross-modal consistency. By concatenating auditory and visual information, UniForm learns to generate audio and video simultaneously within a unified latent space, facilitating the creation of high-quality and well-aligned audio-visual pairs. Extensive experiments demonstrate the superior performance of our method in joint audio-video generation, audio-guided video generation, and video-guided audio generation tasks. Our demos are available at https://uniform-t2av.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2502.03897v2",
    "pdf_url": "http://arxiv.org/pdf/2502.03897v2",
    "published_date": "2025-02-06",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniCP: A Unified Caching and Pruning Framework for Efficient Video Generation",
    "authors": [
      "Wenzhang Sun",
      "Qirui Hou",
      "Donglin Di",
      "Jiahui Yang",
      "Yongjia Ma",
      "Jianxun Cui"
    ],
    "abstract": "Diffusion Transformers (DiT) excel in video generation but encounter significant computational challenges due to the quadratic complexity of attention. Notably, attention differences between adjacent diffusion steps follow a U-shaped pattern. Current methods leverage this property by caching attention blocks, however, they still struggle with sudden error spikes and large discrepancies. To address these issues, we propose UniCP a unified caching and pruning framework for efficient video generation. UniCP optimizes both temporal and spatial dimensions through. Error Aware Dynamic Cache Window (EDCW): Dynamically adjusts cache window sizes for different blocks at various timesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and Dynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS integrates caching and pruning by enabling dynamic switching between pruned and cached outputs. By adjusting cache windows and pruning redundant components, UniCP enhances computational efficiency and maintains video detail fidelity. Experimental results show that UniCP outperforms existing methods in both performance and efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2502.04393v1",
    "pdf_url": "http://arxiv.org/pdf/2502.04393v1",
    "published_date": "2025-02-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity",
    "authors": [
      "Haocheng Xi",
      "Shuo Yang",
      "Yilong Zhao",
      "Chenfeng Xu",
      "Muyang Li",
      "Xiuyu Li",
      "Yujun Lin",
      "Han Cai",
      "Jintao Zhang",
      "Dacheng Li",
      "Jianfei Chen",
      "Ion Stoica",
      "Kurt Keutzer",
      "Song Han"
    ],
    "abstract": "Diffusion Transformers (DiTs) dominate video generation but their high computational cost severely limits real-world applicability, usually requiring tens of minutes to generate a few seconds of video even on high-performance GPUs. This inefficiency primarily arises from the quadratic computational complexity of 3D Full Attention with respect to the context length. In this paper, we propose a training-free framework termed Sparse VideoGen (SVG) that leverages the inherent sparsity in 3D Full Attention to boost inference efficiency. We reveal that the attention heads can be dynamically classified into two groups depending on distinct sparse patterns: (1) Spatial Head, where only spatially-related tokens within each frame dominate the attention output, and (2) Temporal Head, where only temporally-related tokens across different frames dominate. Based on this insight, SVG proposes an online profiling strategy to capture the dynamic sparse patterns and predicts the type of attention head. Combined with a novel hardware-efficient tensor layout transformation and customized kernel implementations, SVG achieves up to 2.28x and 2.33x end-to-end speedup on CogVideoX-v1.5 and HunyuanVideo, respectively, while preserving generation quality.",
    "arxiv_url": "http://arxiv.org/abs/2502.01776v1",
    "pdf_url": "http://arxiv.org/pdf/2502.01776v1",
    "published_date": "2025-02-03",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MakeAnything: Harnessing Diffusion Transformers for Multi-Domain Procedural Sequence Generation",
    "authors": [
      "Yiren Song",
      "Cheng Liu",
      "Mike Zheng Shou"
    ],
    "abstract": "A hallmark of human intelligence is the ability to create complex artifacts through structured multi-step processes. Generating procedural tutorials with AI is a longstanding but challenging goal, facing three key obstacles: (1) scarcity of multi-task procedural datasets, (2) maintaining logical continuity and visual consistency between steps, and (3) generalizing across multiple domains. To address these challenges, we propose a multi-domain dataset covering 21 tasks with over 24,000 procedural sequences. Building upon this foundation, we introduce MakeAnything, a framework based on the diffusion transformer (DIT), which leverages fine-tuning to activate the in-context capabilities of DIT for generating consistent procedural sequences. We introduce asymmetric low-rank adaptation (LoRA) for image generation, which balances generalization capabilities and task-specific performance by freezing encoder parameters while adaptively tuning decoder layers. Additionally, our ReCraft model enables image-to-process generation through spatiotemporal consistency constraints, allowing static images to be decomposed into plausible creation sequences. Extensive experiments demonstrate that MakeAnything surpasses existing methods, setting new performance benchmarks for procedural generation tasks.",
    "arxiv_url": "http://arxiv.org/abs/2502.01572v2",
    "pdf_url": "http://arxiv.org/pdf/2502.01572v2",
    "published_date": "2025-02-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
    "authors": [
      "Gaojie Lin",
      "Jianwen Jiang",
      "Jiaqi Yang",
      "Zerong Zheng",
      "Chao Liang"
    ],
    "abstract": "End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io)",
    "arxiv_url": "http://arxiv.org/abs/2502.01061v2",
    "pdf_url": "http://arxiv.org/pdf/2502.01061v2",
    "published_date": "2025-02-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning",
    "authors": [
      "Yuanhuiyi Lyu",
      "Xu Zheng",
      "Lutao Jiang",
      "Yibo Yan",
      "Xin Zou",
      "Huiyu Zhou",
      "Linfeng Zhang",
      "Xuming Hu"
    ],
    "abstract": "Recent text-to-image generative models, e.g., Stable Diffusion V3 and Flux, have achieved notable progress. However, these models are strongly restricted to their limited knowledge, a.k.a., their own fixed parameters, that are trained with closed datasets. This leads to significant hallucinations or distortions when facing fine-grained and unseen novel real-world objects, e.g., the appearance of the Tesla Cybertruck. To this end, we present the first real-object-based retrieval-augmented generation framework (RealRAG), which augments fine-grained and unseen novel object generation by learning and retrieving real-world images to overcome the knowledge gaps of generative models. Specifically, to integrate missing memory for unseen novel object generation, we train a reflective retriever by self-reflective contrastive learning, which injects the generator's knowledge into the sef-reflective negatives, ensuring that the retrieved augmented images compensate for the model's missing knowledge. Furthermore, the real-object-based framework integrates fine-grained visual knowledge for the generative models, tackling the distortion problem and improving the realism for fine-grained object generation. Our Real-RAG is superior in its modular application to all types of state-of-the-art text-to-image generative models and also delivers remarkable performance boosts with all of them, such as a gain of 16.18% FID score with the auto-regressive model on the Stanford Car benchmark.",
    "arxiv_url": "http://arxiv.org/abs/2502.00848v1",
    "pdf_url": "http://arxiv.org/pdf/2502.00848v1",
    "published_date": "2025-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer",
    "authors": [
      "Enze Xie",
      "Junsong Chen",
      "Yuyang Zhao",
      "Jincheng Yu",
      "Ligeng Zhu",
      "Chengyue Wu",
      "Yujun Lin",
      "Zhekai Zhang",
      "Muyang Li",
      "Junyu Chen",
      "Han Cai",
      "Bingchen Liu",
      "Daquan Zhou",
      "Song Han"
    ],
    "abstract": "This paper presents SANA-1.5, a linear Diffusion Transformer for efficient scaling in text-to-image generation. Building upon SANA-1.0, we introduce three key innovations: (1) Efficient Training Scaling: A depth-growth paradigm that enables scaling from 1.6B to 4.8B parameters with significantly reduced computational resources, combined with a memory-efficient 8-bit optimizer. (2) Model Depth Pruning: A block importance analysis technique for efficient model compression to arbitrary sizes with minimal quality loss. (3) Inference-time Scaling: A repeated sampling strategy that trades computation for model capacity, enabling smaller models to match larger model quality at inference time. Through these strategies, SANA-1.5 achieves a text-image alignment score of 0.81 on GenEval, which can be further improved to 0.96 through inference scaling with VILA-Judge, establishing a new SoTA on GenEval benchmark. These innovations enable efficient model scaling across different compute budgets while maintaining high quality, making high-quality image generation more accessible. Our code and pre-trained models are released.",
    "arxiv_url": "http://arxiv.org/abs/2501.18427v3",
    "pdf_url": "http://arxiv.org/pdf/2501.18427v3",
    "published_date": "2025-01-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Accelerate High-Quality Diffusion Models with Inner Loop Feedback",
    "authors": [
      "Matthew Gwilliam",
      "Han Cai",
      "Di Wu",
      "Abhinav Shrivastava",
      "Zhiyu Cheng"
    ],
    "abstract": "We propose Inner Loop Feedback (ILF), a novel approach to accelerate diffusion models' inference. ILF trains a lightweight module to predict future features in the denoising process by leveraging the outputs from a chosen diffusion backbone block at a given time step. This approach exploits two key intuitions; (1) the outputs of a given block at adjacent time steps are similar, and (2) performing partial computations for a step imposes a lower burden on the model than skipping the step entirely. Our method is highly flexible, since we find that the feedback module itself can simply be a block from the diffusion backbone, with all settings copied. Its influence on the diffusion forward can be tempered with a learnable scaling factor from zero initialization. We train this module using distillation losses; however, unlike some prior work where a full diffusion backbone serves as the student, our model freezes the backbone, training only the feedback module. While many efforts to optimize diffusion models focus on achieving acceptable image quality in extremely few steps (1-4 steps), our emphasis is on matching best case results (typically achieved in 20 steps) while significantly reducing runtime. ILF achieves this balance effectively, demonstrating strong performance for both class-to-image generation with diffusion transformer (DiT) and text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The quality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP Image Quality Assessment, ImageReward, and qualitative comparisons. Project information is available at https://mgwillia.github.io/ilf.",
    "arxiv_url": "http://arxiv.org/abs/2501.13107v2",
    "pdf_url": "http://arxiv.org/pdf/2501.13107v2",
    "published_date": "2025-01-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LiT: Delving into a Simplified Linear Diffusion Transformer for Image Generation",
    "authors": [
      "Jiahao Wang",
      "Ning Kang",
      "Lewei Yao",
      "Mengzhao Chen",
      "Chengyue Wu",
      "Songyang Zhang",
      "Shuchen Xue",
      "Yong Liu",
      "Taiqiang Wu",
      "Xihui Liu",
      "Kaipeng Zhang",
      "Shifeng Zhang",
      "Wenqi Shao",
      "Zhenguo Li",
      "Ping Luo"
    ],
    "abstract": "In commonly used sub-quadratic complexity modules, linear attention benefits from simplicity and high parallelism, making it promising for image synthesis tasks. However, the architectural design and learning strategy for linear attention remain underexplored in this field. In this paper, we offer a suite of ready-to-use solutions for efficient linear diffusion Transformers. Our core contributions include: (1) Simplified Linear Attention using few heads, observing the free-lunch effect of performance without latency increase. (2) Weight inheritance from a fully pre-trained diffusion Transformer: initializing linear Transformer using pre-trained diffusion Transformer and loading all parameters except for those related to linear attention. (3) Hybrid knowledge distillation objective: using a pre-trained diffusion Transformer to help the training of the student linear Transformer, supervising not only the predicted noise but also the variance of the reverse diffusion process. These guidelines lead to our proposed Linear Diffusion Transformer (LiT), an efficient text-to-image Transformer that can be deployed offline on a laptop. Experiments show that in class-conditional 256*256 and 512*512 ImageNet benchmark LiT achieves highly competitive FID while reducing training steps by 80% and 77% compared to DiT. LiT also rivals methods based on Mamba or Gated Linear Attention. Besides, for text-to-image generation, LiT allows for the rapid synthesis of up to 1K resolution photorealistic images. Project page: https://techmonsterwang.github.io/LiT/.",
    "arxiv_url": "http://arxiv.org/abs/2501.12976v1",
    "pdf_url": "http://arxiv.org/pdf/2501.12976v1",
    "published_date": "2025-01-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CatV2TON: Taming Diffusion Transformers for Vision-Based Virtual Try-On with Temporal Concatenation",
    "authors": [
      "Zheng Chong",
      "Wenqing Zhang",
      "Shiyue Zhang",
      "Jun Zheng",
      "Xiao Dong",
      "Haoxiang Li",
      "Yiling Wu",
      "Dongmei Jiang",
      "Xiaodan Liang"
    ],
    "abstract": "Virtual try-on (VTON) technology has gained attention due to its potential to transform online retail by enabling realistic clothing visualization of images and videos. However, most existing methods struggle to achieve high-quality results across image and video try-on tasks, especially in long video scenarios. In this work, we introduce CatV2TON, a simple and effective vision-based virtual try-on (V2TON) method that supports both image and video try-on tasks with a single diffusion transformer model. By temporally concatenating garment and person inputs and training on a mix of image and video datasets, CatV2TON achieves robust try-on performance across static and dynamic settings. For efficient long-video generation, we propose an overlapping clip-based inference strategy that uses sequential frame guidance and Adaptive Clip Normalization (AdaCN) to maintain temporal consistency with reduced resource demands. We also present ViViD-S, a refined video try-on dataset, achieved by filtering back-facing frames and applying 3D mask smoothing for enhanced temporal consistency. Comprehensive experiments demonstrate that CatV2TON outperforms existing methods in both image and video try-on tasks, offering a versatile and reliable solution for realistic virtual try-ons across diverse scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2501.11325v1",
    "pdf_url": "http://arxiv.org/pdf/2501.11325v1",
    "published_date": "2025-01-20",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T42 (Primary) 168T45 (Secondary)",
      "I.4.9"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learnings from Scaling Visual Tokenizers for Reconstruction and Generation",
    "authors": [
      "Philippe Hansen-Estruch",
      "David Yan",
      "Ching-Yao Chung",
      "Orr Zohar",
      "Jialiang Wang",
      "Tingbo Hou",
      "Tao Xu",
      "Sriram Vishwanath",
      "Peter Vajda",
      "Xinlei Chen"
    ],
    "abstract": "Visual tokenization via auto-encoding empowers state-of-the-art image and video generative models by compressing pixels into a latent space. Although scaling Transformer-based generators has been central to recent advances, the tokenizer component itself is rarely scaled, leaving open questions about how auto-encoder design choices influence both its objective of reconstruction and downstream generative performance. Our work aims to conduct an exploration of scaling in auto-encoders to fill in this blank. To facilitate this exploration, we replace the typical convolutional backbone with an enhanced Vision Transformer architecture for Tokenization (ViTok). We train ViTok on large-scale image and video datasets far exceeding ImageNet-1K, removing data constraints on tokenizer scaling. We first study how scaling the auto-encoder bottleneck affects both reconstruction and generation -- and find that while it is highly correlated with reconstruction, its relationship with generation is more complex. We next explored the effect of separately scaling the auto-encoders' encoder and decoder on reconstruction and generation performance. Crucially, we find that scaling the encoder yields minimal gains for either reconstruction or generation, while scaling the decoder boosts reconstruction but the benefits for generation are mixed. Building on our exploration, we design ViTok as a lightweight auto-encoder that achieves competitive performance with state-of-the-art auto-encoders on ImageNet-1K and COCO reconstruction tasks (256p and 512p) while outperforming existing auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates competitive performance on image generation for ImageNet-1K and sets new state-of-the-art benchmarks for class-conditional video generation on UCF-101.",
    "arxiv_url": "http://arxiv.org/abs/2501.09755v1",
    "pdf_url": "http://arxiv.org/pdf/2501.09755v1",
    "published_date": "2025-01-16",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.10; I.4.2; I.4.5"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing Image Generation Fidelity via Progressive Prompts",
    "authors": [
      "Zhen Xiong",
      "Yuqi Li",
      "Chuanguang Yang",
      "Tiao Tan",
      "Zhihong Zhu",
      "Siyuan Li",
      "Yue Ma"
    ],
    "abstract": "The diffusion transformer (DiT) architecture has attracted significant attention in image generation, achieving better fidelity, performance, and diversity. However, most existing DiT - based image generation methods focus on global - aware synthesis, and regional prompt control has been less explored. In this paper, we propose a coarse - to - fine generation pipeline for regional prompt - following generation. Specifically, we first utilize the powerful large language model (LLM) to generate both high - level descriptions of the image (such as content, topic, and objects) and low - level descriptions (such as details and style). Then, we explore the influence of cross - attention layers at different depths. We find that deeper layers are always responsible for high - level content control, while shallow layers handle low - level content control. Various prompts are injected into the proposed regional cross - attention control for coarse - to - fine generation. By using the proposed pipeline, we enhance the controllability of DiT - based image generation. Extensive quantitative and qualitative results show that our pipeline can improve the performance of the generated images.",
    "arxiv_url": "http://arxiv.org/abs/2501.07070v1",
    "pdf_url": "http://arxiv.org/pdf/2501.07070v1",
    "published_date": "2025-01-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-subject Open-set Personalization in Video Generation",
    "authors": [
      "Tsai-Shien Chen",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Yuwei Fang",
      "Kwot Sin Lee",
      "Ivan Skorokhodov",
      "Kfir Aberman",
      "Jun-Yan Zhu",
      "Ming-Hsuan Yang",
      "Sergey Tulyakov"
    ],
    "abstract": "Video personalization methods allow us to synthesize videos with specific concepts such as people, pets, and places. However, existing methods often focus on limited domains, require time-consuming optimization per subject, or support only a single subject. We present Video Alchemist $-$ a video model with built-in multi-subject, open-set personalization capabilities for both foreground objects and background, eliminating the need for time-consuming test-time optimization. Our model is built on a new Diffusion Transformer module that fuses each conditional reference image and its corresponding subject-level text prompt with cross-attention layers. Developing such a large model presents two main challenges: dataset and evaluation. First, as paired datasets of reference images and videos are extremely hard to collect, we sample selected video frames as reference images and synthesize a clip of the target video. However, while models can easily denoise training videos given reference frames, they fail to generalize to new contexts. To mitigate this issue, we design a new automatic data construction pipeline with extensive image augmentations. Second, evaluating open-set video personalization is a challenge in itself. To address this, we introduce a personalization benchmark that focuses on accurate subject fidelity and supports diverse personalization scenarios. Finally, our extensive experiments show that our method significantly outperforms existing personalization methods in both quantitative and qualitative evaluations.",
    "arxiv_url": "http://arxiv.org/abs/2501.06187v2",
    "pdf_url": "http://arxiv.org/pdf/2501.06187v2",
    "published_date": "2025-01-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DIS-FLUX: simple and efficient multi-instance generation with DiT rendering",
    "authors": [
      "Dewei Zhou",
      "Ji Xie",
      "Zongxin Yang",
      "Yi Yang"
    ],
    "abstract": "The growing demand for controllable outputs in text-to-image generation has driven significant advancements in multi-instance generation (MIG), enabling users to define both instance layouts and attributes. Currently, the state-of-the-art methods in MIG are primarily adapter-based. However, these methods necessitate retraining a new adapter each time a more advanced model is released, resulting in significant resource consumption. A methodology named Depth-Driven Decoupled Instance Synthesis (3DIS) has been introduced, which decouples MIG into two distinct phases: 1) depth-based scene construction and 2) detail rendering with widely pre-trained depth control models. The 3DIS method requires adapter training solely during the scene construction phase, while enabling various models to perform training-free detail rendering. Initially, 3DIS focused on rendering techniques utilizing U-Net architectures such as SD1.5, SD2, and SDXL, without exploring the potential of recent DiT-based models like FLUX. In this paper, we present 3DIS-FLUX, an extension of the 3DIS framework that integrates the FLUX model for enhanced rendering capabilities. Specifically, we employ the FLUX.1-Depth-dev model for depth map controlled image generation and introduce a detail renderer that manipulates the Attention Mask in FLUX's Joint Attention mechanism based on layout information. This approach allows for the precise rendering of fine-grained attributes of each instance. Our experimental results indicate that 3DIS-FLUX, leveraging the FLUX model, outperforms the original 3DIS method, which utilized SD2 and SDXL, and surpasses current state-of-the-art adapter-based methods in terms of both performance and image quality. Project Page: https://limuloo.github.io/3DIS/.",
    "arxiv_url": "http://arxiv.org/abs/2501.05131v1",
    "pdf_url": "http://arxiv.org/pdf/2501.05131v1",
    "published_date": "2025-01-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "Control",
      "FLUX",
      "Controllable",
      "text-to-image"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning",
    "authors": [
      "Yuzhou Huang",
      "Ziyang Yuan",
      "Quande Liu",
      "Qiulin Wang",
      "Xintao Wang",
      "Ruimao Zhang",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai"
    ],
    "abstract": "Text-to-video generation has made remarkable advancements through diffusion models. However, Multi-Concept Video Customization (MCVC) remains a significant challenge. We identify two key challenges in this task: 1) the identity decoupling problem, where directly adopting existing customization methods inevitably mix attributes when handling multiple concepts simultaneously, and 2) the scarcity of high-quality video-entity pairs, which is crucial for training such a model that represents and decouples various concepts well. To address these challenges, we introduce ConceptMaster, an innovative framework that effectively tackles the critical issues of identity decoupling while maintaining concept fidelity in customized videos. Specifically, we introduce a novel strategy of learning decoupled multi-concept embeddings that are injected into the diffusion models in a standalone manner, which effectively guarantees the quality of customized videos with multiple identities, even for highly similar visual concepts. To further overcome the scarcity of high-quality MCVC data, we carefully establish a data construction pipeline, which enables systematic collection of precise multi-concept video-entity data across diverse concepts. A comprehensive benchmark is designed to validate the effectiveness of our model from three critical dimensions: concept fidelity, identity decoupling ability, and video generation quality across six different concept composition scenarios. Extensive experiments demonstrate that our ConceptMaster significantly outperforms previous approaches for this task, paving the way for generating personalized and semantically accurate videos across multiple concepts.",
    "arxiv_url": "http://arxiv.org/abs/2501.04698v1",
    "pdf_url": "http://arxiv.org/pdf/2501.04698v1",
    "published_date": "2025-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Circuit Complexity Bounds for Visual Autoregressive Model",
    "authors": [
      "Yekun Ke",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song"
    ],
    "abstract": "Understanding the expressive ability of a specific model is essential for grasping its capacity limitations. Recently, several studies have established circuit complexity bounds for Transformer architecture. Besides, the Visual AutoRegressive (VAR) model has risen to be a prominent method in the field of image generation, outperforming previous techniques, such as Diffusion Transformers, in generating high-quality images. We investigate the circuit complexity of the VAR model and establish a bound in this study. Our primary result demonstrates that the VAR model is equivalent to a simulation by a uniform $\\mathsf{TC}^0$ threshold circuit with hidden dimension $d \\leq O(n)$ and $\\mathrm{poly}(n)$ precision. This is the first study to rigorously highlight the limitations in the expressive power of VAR models despite their impressive performance. We believe our findings will offer valuable insights into the inherent constraints of these models and guide the development of more efficient and expressive architectures in the future.",
    "arxiv_url": "http://arxiv.org/abs/2501.04299v1",
    "pdf_url": "http://arxiv.org/pdf/2501.04299v1",
    "published_date": "2025-01-08",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CC",
      "cs.CL",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers",
    "authors": [
      "Yuechen Zhang",
      "Yaoyang Liu",
      "Bin Xia",
      "Bohao Peng",
      "Zexin Yan",
      "Eric Lo",
      "Jiaya Jia"
    ],
    "abstract": "We present Magic Mirror, a framework for generating identity-preserved videos with cinematic-level quality and dynamic motion. While recent advances in video diffusion models have shown impressive capabilities in text-to-video generation, maintaining consistent identity while producing natural motion remains challenging. Previous methods either require person-specific fine-tuning or struggle to balance identity preservation with motion diversity. Built upon Video Diffusion Transformers, our method introduces three key components: (1) a dual-branch facial feature extractor that captures both identity and structural features, (2) a lightweight cross-modal adapter with Conditioned Adaptive Normalization for efficient identity integration, and (3) a two-stage training strategy combining synthetic identity pairs with video data. Extensive experiments demonstrate that Magic Mirror effectively balances identity consistency with natural motion, outperforming existing methods across multiple metrics while requiring minimal parameters added. The code and model will be made publicly available at: https://github.com/dvlab-research/MagicMirror/",
    "arxiv_url": "http://arxiv.org/abs/2501.03931v1",
    "pdf_url": "http://arxiv.org/pdf/2501.03931v1",
    "published_date": "2025-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/dvlab-research/MagicMirror",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TransPixeler: Advancing Text-to-Video Generation with Transparency",
    "authors": [
      "Luozhou Wang",
      "Yijun Li",
      "Zhifei Chen",
      "Jui-Hsien Wang",
      "Zhifei Zhang",
      "He Zhang",
      "Zhe Lin",
      "Yingcong Chen"
    ],
    "abstract": "Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixeler, a method to extend pretrained video models for RGBA generation while retaining the original RGB capabilities. TransPixar leverages a diffusion transformer (DiT) architecture, incorporating alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB and alpha channels with high consistency. By optimizing attention mechanisms, TransPixar preserves the strengths of the original RGB model and achieves strong alignment between RGB and alpha channels despite limited training data. Our approach effectively generates diverse and consistent RGBA videos, advancing the possibilities for VFX and interactive content creation.",
    "arxiv_url": "http://arxiv.org/abs/2501.03006v2",
    "pdf_url": "http://arxiv.org/pdf/2501.03006v2",
    "published_date": "2025-01-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking",
    "authors": [
      "Weikang Bian",
      "Zhaoyang Huang",
      "Xiaoyu Shi",
      "Yijin Li",
      "Fu-Yun Wang",
      "Hongsheng Li"
    ],
    "abstract": "4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.",
    "arxiv_url": "http://arxiv.org/abs/2501.02690v1",
    "pdf_url": "http://arxiv.org/pdf/2501.02690v1",
    "published_date": "2025-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EliGen: Entity-Level Controlled Image Generation with Regional Attention",
    "authors": [
      "Hong Zhang",
      "Zhongjie Duan",
      "Xingjun Wang",
      "Yingda Chen",
      "Yu Zhang"
    ],
    "abstract": "Recent advancements in diffusion models have significantly advanced text-to-image generation, yet global text prompts alone remain insufficient for achieving fine-grained control over individual entities within an image. To address this limitation, we present EliGen, a novel framework for Entity-level controlled image Generation. Firstly, we put forward regional attention, a mechanism for diffusion transformers that requires no additional parameters, seamlessly integrating entity prompts and arbitrary-shaped spatial masks. By contributing a high-quality dataset with fine-grained spatial and semantic entity-level annotations, we train EliGen to achieve robust and accurate entity-level manipulation, surpassing existing methods in both spatial precision and image quality. Additionally, we propose an inpainting fusion pipeline, extending its capabilities to multi-entity image inpainting tasks. We further demonstrate its flexibility by integrating it with other open-source models such as IP-Adapter, In-Context LoRA and MLLM, unlocking new creative possibilities. The source code, model, and dataset are published at https://github.com/modelscope/DiffSynth-Studio.git.",
    "arxiv_url": "http://arxiv.org/abs/2501.01097v3",
    "pdf_url": "http://arxiv.org/pdf/2501.01097v3",
    "published_date": "2025-01-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/modelscope/DiffSynth-Studio.git",
    "keywords": [
      "image inpainting",
      "image generation",
      "Control",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dual Diffusion for Unified Image Generation and Understanding",
    "authors": [
      "Zijie Li",
      "Henry Li",
      "Yichun Shi",
      "Amir Barati Farimani",
      "Yuval Kluger",
      "Linjie Yang",
      "Peng Wang"
    ],
    "abstract": "Diffusion models have gained tremendous success in text-to-image generation, yet still lag behind with visual understanding tasks, an area dominated by autoregressive vision-language models. We propose a large-scale and fully end-to-end diffusion model for multi-modal understanding and generation that significantly improves on existing diffusion-based multimodal models, and is the first of its kind to support the full suite of vision-language modeling capabilities. Inspired by the multimodal diffusion transformer (MM-DiT) and recent advances in discrete diffusion language modeling, we leverage a cross-modal maximum likelihood estimation framework that simultaneously trains the conditional likelihoods of both images and text jointly under a single loss function, which is back-propagated through both branches of the diffusion transformer. The resulting model is highly flexible and capable of a wide range of tasks including image generation, captioning, and visual question answering. Our model attained competitive performance compared to recent unified image understanding and generation models, demonstrating the potential of multimodal diffusion modeling as a promising alternative to autoregressive next-token prediction models.",
    "arxiv_url": "http://arxiv.org/abs/2501.00289v1",
    "pdf_url": "http://arxiv.org/pdf/2501.00289v1",
    "published_date": "2024-12-31",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Open-Sora: Democratizing Efficient Video Production for All",
    "authors": [
      "Zangwei Zheng",
      "Xiangyu Peng",
      "Tianji Yang",
      "Chenhui Shen",
      "Shenggui Li",
      "Hongxin Liu",
      "Yukun Zhou",
      "Tianyi Li",
      "Yang You"
    ],
    "abstract": "Vision and language are the two foundational senses for humans, and they build up our cognitive ability and intelligence. While significant breakthroughs have been made in AI language ability, artificial visual intelligence, especially the ability to generate and simulate the world we see, is far lagging behind. To facilitate the development and accessibility of artificial visual intelligence, we created Open-Sora, an open-source video generation model designed to produce high-fidelity video content. Open-Sora supports a wide spectrum of visual generation tasks, including text-to-image generation, text-to-video generation, and image-to-video generation. The model leverages advanced deep learning architectures and training/inference techniques to enable flexible video synthesis, which could generate video content of up to 15 seconds, up to 720p resolution, and arbitrary aspect ratios. Specifically, we introduce Spatial-Temporal Diffusion Transformer (STDiT), an efficient diffusion framework for videos that decouples spatial and temporal attention. We also introduce a highly compressive 3D autoencoder to make representations compact and further accelerate training with an ad hoc training strategy. Through this initiative, we aim to foster innovation, creativity, and inclusivity within the community of AI content creation. By embracing the open-source principle, Open-Sora democratizes full access to all the training/inference/data preparation codes as well as model weights. All resources are publicly available at: https://github.com/hpcaitech/Open-Sora.",
    "arxiv_url": "http://arxiv.org/abs/2412.20404v1",
    "pdf_url": "http://arxiv.org/pdf/2412.20404v1",
    "published_date": "2024-12-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/hpcaitech/Open-Sora",
    "keywords": [
      "video generation",
      "image generation",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UNIC-Adapter: Unified Image-instruction Adapter with Multi-modal Transformer for Image Generation",
    "authors": [
      "Lunhao Duan",
      "Shanshan Zhao",
      "Wenjun Yan",
      "Yinglun Li",
      "Qing-Guo Chen",
      "Zhao Xu",
      "Weihua Luo",
      "Kaifu Zhang",
      "Mingming Gong",
      "Gui-Song Xia"
    ],
    "abstract": "Recently, text-to-image generation models have achieved remarkable advancements, particularly with diffusion models facilitating high-quality image synthesis from textual descriptions. However, these models often struggle with achieving precise control over pixel-level layouts, object appearances, and global styles when using text prompts alone. To mitigate this issue, previous works introduce conditional images as auxiliary inputs for image generation, enhancing control but typically necessitating specialized models tailored to different types of reference inputs. In this paper, we explore a new approach to unify controllable generation within a single framework. Specifically, we propose the unified image-instruction adapter (UNIC-Adapter) built on the Multi-Modal-Diffusion Transformer architecture, to enable flexible and controllable generation across diverse conditions without the need for multiple specialized models. Our UNIC-Adapter effectively extracts multi-modal instruction information by incorporating both conditional images and task instructions, injecting this information into the image generation process through a cross-attention mechanism enhanced by Rotary Position Embedding. Experimental results across a variety of tasks, including pixel-level spatial control, subject-driven image generation, and style-image-based image synthesis, demonstrate the effectiveness of our UNIC-Adapter in unified controllable image generation.",
    "arxiv_url": "http://arxiv.org/abs/2412.18928v1",
    "pdf_url": "http://arxiv.org/pdf/2412.18928v1",
    "published_date": "2024-12-25",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "Control",
      "Controllable",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
    "authors": [
      "Chang Zou",
      "Evelyn Zhang",
      "Runlin Guo",
      "Haohang Xu",
      "Conghui He",
      "Xuming Hu",
      "Linfeng Zhang"
    ],
    "abstract": "Diffusion Transformers (DiT) have become the dominant methods in image and video generation yet still suffer substantial computational costs. As an effective approach for DiT acceleration, feature caching methods are designed to cache the features of DiT in previous timesteps and reuse them in the next timesteps, allowing us to skip the computation in the next timesteps. However, on the one hand, aggressively reusing all the features cached in previous timesteps leads to a severe drop in generation quality. On the other hand, conservatively caching only the features in the redundant layers or tokens but still computing the important ones successfully preserves the generation quality but results in reductions in acceleration ratios. Observing such a tradeoff between generation quality and acceleration performance, this paper begins by quantitatively studying the accumulated error from cached features. Surprisingly, we find that aggressive caching does not introduce significantly more caching errors in the caching step, and the conservative feature caching can fix the error introduced by aggressive caching. Thereby, we propose a dual caching strategy that adopts aggressive and conservative caching iteratively, leading to significant acceleration and high generation quality at the same time. Besides, we further introduce a V-caching strategy for token-wise conservative caching, which is compatible with flash attention and requires no training and calibration data.   Our codes have been released in Github: \\textbf{Code: \\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
    "arxiv_url": "http://arxiv.org/abs/2412.18911v1",
    "pdf_url": "http://arxiv.org/pdf/2412.18911v1",
    "published_date": "2024-12-25",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "https://github.com/Shenyi-Z/DuCa",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "1.58-bit FLUX",
    "authors": [
      "Chenglin Yang",
      "Celong Liu",
      "Xueqing Deng",
      "Dongwon Kim",
      "Xing Mei",
      "Xiaohui Shen",
      "Liang-Chieh Chen"
    ],
    "abstract": "We present 1.58-bit FLUX, the first successful approach to quantizing the state-of-the-art text-to-image generation model, FLUX.1-dev, using 1.58-bit weights (i.e., values in {-1, 0, +1}) while maintaining comparable performance for generating 1024 x 1024 images. Notably, our quantization method operates without access to image data, relying solely on self-supervision from the FLUX.1-dev model. Additionally, we develop a custom kernel optimized for 1.58-bit operations, achieving a 7.7x reduction in model storage, a 5.1x reduction in inference memory, and improved inference latency. Extensive evaluations on the GenEval and T2I Compbench benchmarks demonstrate the effectiveness of 1.58-bit FLUX in maintaining generation quality while significantly enhancing computational efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2412.18653v1",
    "pdf_url": "http://arxiv.org/pdf/2412.18653v1",
    "published_date": "2024-12-24",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation",
    "authors": [
      "Minghong Cai",
      "Xiaodong Cun",
      "Xiaoyu Li",
      "Wenze Liu",
      "Zhaoyang Zhang",
      "Yong Zhang",
      "Ying Shan",
      "Xiangyu Yue"
    ],
    "abstract": "Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer MM-DiT architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiT's attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training.",
    "arxiv_url": "http://arxiv.org/abs/2412.18597v1",
    "pdf_url": "http://arxiv.org/pdf/2412.18597v1",
    "published_date": "2024-12-24",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "video editing",
      "Control",
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FFA Sora, video generation as fundus fluorescein angiography simulator",
    "authors": [
      "Xinyuan Wu",
      "Lili Wang",
      "Ruoyu Chen",
      "Bowen Liu",
      "Weiyi Zhang",
      "Xi Yang",
      "Yifan Feng",
      "Mingguang He",
      "Danli Shi"
    ],
    "abstract": "Fundus fluorescein angiography (FFA) is critical for diagnosing retinal vascular diseases, but beginners often struggle with image interpretation. This study develops FFA Sora, a text-to-video model that converts FFA reports into dynamic videos via a Wavelet-Flow Variational Autoencoder (WF-VAE) and a diffusion transformer (DiT). Trained on an anonymized dataset, FFA Sora accurately simulates disease features from the input text, as confirmed by objective metrics: Frechet Video Distance (FVD) = 329.78, Learned Perceptual Image Patch Similarity (LPIPS) = 0.48, and Visual-question-answering Score (VQAScore) = 0.61. Specific evaluations showed acceptable alignment between the generated videos and textual prompts, with BERTScore of 0.35. Additionally, the model demonstrated strong privacy-preserving performance in retrieval evaluations, achieving an average Recall@K of 0.073. Human assessments indicated satisfactory visual quality, with an average score of 1.570(scale: 1 = best, 5 = worst). This model addresses privacy concerns associated with sharing large-scale FFA data and enhances medical education.",
    "arxiv_url": "http://arxiv.org/abs/2412.17346v1",
    "pdf_url": "http://arxiv.org/pdf/2412.17346v1",
    "published_date": "2024-12-23",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers",
    "authors": [
      "Haoran You",
      "Connelly Barnes",
      "Yuqian Zhou",
      "Yan Kang",
      "Zhenbang Du",
      "Wei Zhou",
      "Lingzhi Zhang",
      "Yotam Nitzan",
      "Xiaoyang Liu",
      "Zhe Lin",
      "Eli Shechtman",
      "Sohrab Amirghodsi",
      "Yingyan Celine Lin"
    ],
    "abstract": "Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) image generation quality but suffer from high latency and memory inefficiency, making them difficult to deploy on resource-constrained devices. One key efficiency bottleneck is that existing DiTs apply equal computation across all regions of an image. However, not all image tokens are equally important, and certain localized areas require more computation, such as objects. To address this, we propose DiffRatio-MoD, a dynamic DiT inference framework with differentiable compression ratios, which automatically learns to dynamically route computation across layers and timesteps for each image token, resulting in Mixture-of-Depths (MoD) efficient DiT models. Specifically, DiffRatio-MoD integrates three features: (1) A token-level routing scheme where each DiT layer includes a router that is jointly fine-tuned with model weights to predict token importance scores. In this way, unimportant tokens bypass the entire layer's computation; (2) A layer-wise differentiable ratio mechanism where different DiT layers automatically learn varying compression ratios from a zero initialization, resulting in large compression ratios in redundant layers while others remain less compressed or even uncompressed; (3) A timestep-wise differentiable ratio mechanism where each denoising timestep learns its own compression ratio. The resulting pattern shows higher ratios for noisier timesteps and lower ratios as the image becomes clearer. Extensive experiments on both text-to-image and inpainting tasks show that DiffRatio-MoD effectively captures dynamism across token, layer, and timestep axes, achieving superior trade-offs between generation quality and efficiency compared to prior works.",
    "arxiv_url": "http://arxiv.org/abs/2412.16822v1",
    "pdf_url": "http://arxiv.org/pdf/2412.16822v1",
    "published_date": "2024-12-22",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up",
    "authors": [
      "Songhua Liu",
      "Zhenxiong Tan",
      "Xinchao Wang"
    ],
    "abstract": "Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, we aim at a linear attention mechanism in this paper that reduces the complexity of pre-trained DiTs to linear. We begin our exploration with a comprehensive summary of existing efficient attention mechanisms and identify four key factors crucial for successful linearization of pre-trained DiTs: locality, formulation consistency, high-rank attention maps, and feature integrity. Based on these insights, we introduce a convolution-like local attention strategy termed CLEAR, which limits feature interactions to a local window around each query token, and thus achieves linear complexity. Our experiments indicate that, by fine-tuning the attention layer on merely 10K self-generated samples for 10K iterations, we can effectively transfer knowledge from a pre-trained DiT to a student model with linear complexity, yielding results comparable to the teacher model. Simultaneously, it reduces attention computations by 99.5% and accelerates generation by 6.3 times for generating 8K-resolution images. Furthermore, we investigate favorable properties in the distilled attention layers, such as zero-shot generalization cross various models and plugins, and improved support for multi-GPU parallel inference. Models and codes are available here: https://github.com/Huage001/CLEAR.",
    "arxiv_url": "http://arxiv.org/abs/2412.16112v1",
    "pdf_url": "http://arxiv.org/pdf/2412.16112v1",
    "published_date": "2024-12-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Huage001/CLEAR",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Scaling of Diffusion Transformers for Text-to-Image Generation",
    "authors": [
      "Hao Li",
      "Shamit Lal",
      "Zhiheng Li",
      "Yusheng Xie",
      "Ying Wang",
      "Yang Zou",
      "Orchid Majumder",
      "R. Manmatha",
      "Zhuowen Tu",
      "Stefano Ermon",
      "Stefano Soatto",
      "Ashwin Swaminathan"
    ],
    "abstract": "We empirically study the scaling properties of various Diffusion Transformers (DiTs) for text-to-image generation by performing extensive and rigorous ablations, including training scaled DiTs ranging from 0.3B upto 8B parameters on datasets up to 600M images. We find that U-ViT, a pure self-attention based DiT model provides a simpler design and scales more effectively in comparison with cross-attention based DiT variants, which allows straightforward expansion for extra conditions and other modalities. We identify a 2.3B U-ViT model can get better performance than SDXL UNet and other DiT variants in controlled setting. On the data scaling side, we investigate how increasing dataset size and enhanced long caption improve the text-image alignment performance and the learning efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2412.12391v1",
    "pdf_url": "http://arxiv.org/pdf/2412.12391v1",
    "published_date": "2024-12-16",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image generation",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Causal Diffusion Transformers for Generative Modeling",
    "authors": [
      "Chaorui Deng",
      "Deyao Zhu",
      "Kunchang Li",
      "Shi Guang",
      "Haoqi Fan"
    ],
    "abstract": "We introduce Causal Diffusion as the autoregressive (AR) counterpart of Diffusion models. It is a next-token(s) forecasting framework that is friendly to both discrete and continuous modalities and compatible with existing next-token prediction models like LLaMA and GPT. While recent works attempt to combine diffusion with AR models, we show that introducing sequential factorization to a diffusion model can substantially improve its performance and enables a smooth transition between AR and diffusion generation modes. Hence, we propose CausalFusion - a decoder-only transformer that dual-factorizes data across sequential tokens and diffusion noise levels, leading to state-of-the-art results on the ImageNet generation benchmark while also enjoying the AR advantage of generating an arbitrary number of tokens for in-context reasoning. We further demonstrate CausalFusion's multimodal capabilities through a joint image generation and captioning model, and showcase CausalFusion's ability for zero-shot in-context image manipulations. We hope that this work could provide the community with a fresh perspective on training multimodal models over discrete and continuous data.",
    "arxiv_url": "http://arxiv.org/abs/2412.12095v2",
    "pdf_url": "http://arxiv.org/pdf/2412.12095v2",
    "published_date": "2024-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Video Diffusion Transformers are In-Context Learners",
    "authors": [
      "Zhengcong Fei",
      "Di Qiu",
      "Debang Li",
      "Changqian Yu",
      "Mingyuan Fan"
    ],
    "abstract": "This paper investigates a solution for enabling in-context capabilities of video diffusion transformers, with minimal tuning required for activation. Specifically, we propose a simple pipeline to leverage in-context generation: ($\\textbf{i}$) concatenate videos along spacial or time dimension, ($\\textbf{ii}$) jointly caption multi-scene video clips from one source, and ($\\textbf{iii}$) apply task-specific fine-tuning using carefully curated small datasets. Through a series of diverse controllable tasks, we demonstrate qualitatively that existing advanced text-to-video models can effectively perform in-context generation. Notably, it allows for the creation of consistent multi-scene videos exceeding 30 seconds in duration, without additional computational overhead. Importantly, this method requires no modifications to the original models, results in high-fidelity video outputs that better align with prompt specifications and maintain role consistency. Our framework presents a valuable tool for the research community and offers critical insights for advancing product-level controllable video generation systems. The data, code, and model weights are publicly available at: https://github.com/feizc/Video-In-Context.",
    "arxiv_url": "http://arxiv.org/abs/2412.10783v3",
    "pdf_url": "http://arxiv.org/pdf/2412.10783v3",
    "published_date": "2024-12-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/feizc/Video-In-Context",
    "keywords": [
      "Controllable",
      "Control",
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity",
    "authors": [
      "Hongjie Wang",
      "Chih-Yao Ma",
      "Yen-Cheng Liu",
      "Ji Hou",
      "Tao Xu",
      "Jialiang Wang",
      "Felix Juefei-Xu",
      "Yaqiao Luo",
      "Peizhao Zhang",
      "Tingbo Hou",
      "Peter Vajda",
      "Niraj K. Jha",
      "Xiaoliang Dai"
    ],
    "abstract": "Text-to-video generation enhances content creation but is highly computationally intensive: The computational cost of Diffusion Transformers (DiTs) scales quadratically in the number of pixels. This makes minute-length video generation extremely expensive, limiting most existing models to generating videos of only 10-20 seconds length. We propose a Linear-complexity text-to-video Generation (LinGen) framework whose cost scales linearly in the number of pixels. For the first time, LinGen enables high-resolution minute-length video generation on a single GPU without compromising quality. It replaces the computationally-dominant and quadratic-complexity block, self-attention, with a linear-complexity block called MATE, which consists of an MA-branch and a TE-branch. The MA-branch targets short-to-long-range correlations, combining a bidirectional Mamba2 block with our token rearrangement method, Rotary Major Scan, and our review tokens developed for long video generation. The TE-branch is a novel TEmporal Swin Attention block that focuses on temporal correlations between adjacent tokens and medium-range tokens. The MATE block addresses the adjacency preservation issue of Mamba and improves the consistency of generated videos significantly. Experimental results show that LinGen outperforms DiT (with a 75.6% win rate) in video quality with up to 15$\\times$ (11.5$\\times$) FLOPs (latency) reduction. Furthermore, both automatic metrics and human evaluation demonstrate our LinGen-4B yields comparable video quality to state-of-the-art models (with a 50.5%, 52.1%, 49.1% win rate with respect to Gen-3, LumaLabs, and Kling, respectively). This paves the way to hour-length movie generation and real-time interactive video generation. We provide 68s video generation results and more examples in our project website: https://lineargen.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2412.09856v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09856v1",
    "published_date": "2024-12-13",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MSC: Multi-Scale Spatio-Temporal Causal Attention for Autoregressive Video Diffusion",
    "authors": [
      "Xunnong Xu",
      "Mengying Cao"
    ],
    "abstract": "Diffusion transformers enable flexible generative modeling for video. However, it is still technically challenging and computationally expensive to generate high-resolution videos with rich semantics and complex motion. Similar to languages, video data are also auto-regressive by nature, so it is counter-intuitive to use attention mechanism with bi-directional dependency in the model. Here we propose a Multi-Scale Causal (MSC) framework to address these problems. Specifically, we introduce multiple resolutions in the spatial dimension and high-low frequencies in the temporal dimension to realize efficient attention calculation. Furthermore, attention blocks on multiple scales are combined in a controlled way to allow causal conditioning on noisy image frames for diffusion training, based on the idea that noise destroys information at different rates on different resolutions. We theoretically show that our approach can greatly reduce the computational complexity and enhance the efficiency of training. The causal attention diffusion framework can also be used for auto-regressive long video generation, without violating the natural order of frame sequences.",
    "arxiv_url": "http://arxiv.org/abs/2412.09828v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09828v1",
    "published_date": "2024-12-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG",
    "authors": [
      "Kavana Venkatesh",
      "Yusuf Dalva",
      "Ismini Lourentzou",
      "Pinar Yanardag"
    ],
    "abstract": "We introduce a novel approach to enhance the capabilities of text-to-image models by incorporating a graph-based RAG. Our system dynamically retrieves detailed character information and relational data from the knowledge graph, enabling the generation of visually accurate and contextually rich images. This capability significantly improves upon the limitations of existing T2I models, which often struggle with the accurate depiction of complex or culturally specific subjects due to dataset constraints. Furthermore, we propose a novel self-correcting mechanism for text-to-image models to ensure consistency and fidelity in visual outputs, leveraging the rich context from the graph to guide corrections. Our qualitative and quantitative experiments demonstrate that Context Canvas significantly enhances the capabilities of popular models such as Flux, Stable Diffusion, and DALL-E, and improves the functionality of ControlNet for fine-grained image editing tasks. To our knowledge, Context Canvas represents the first application of graph-based RAG in enhancing T2I models, representing a significant advancement for producing high-fidelity, context-aware multi-faceted images.",
    "arxiv_url": "http://arxiv.org/abs/2412.09614v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09614v1",
    "published_date": "2024-12-12",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "image editing",
      "Control",
      "FLUX",
      "text-to-image"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers",
    "authors": [
      "Yusuf Dalva",
      "Kavana Venkatesh",
      "Pinar Yanardag"
    ],
    "abstract": "Rectified flow models have emerged as a dominant approach in image generation, showcasing impressive capabilities in high-quality image synthesis. However, despite their effectiveness in visual generation, rectified flow models often struggle with disentangled editing of images. This limitation prevents the ability to perform precise, attribute-specific modifications without affecting unrelated aspects of the image. In this paper, we introduce FluxSpace, a domain-agnostic image editing method leveraging a representation space with the ability to control the semantics of images generated by rectified flow transformers, such as Flux. By leveraging the representations learned by the transformer blocks within the rectified flow models, we propose a set of semantically interpretable representations that enable a wide range of image editing tasks, from fine-grained image editing to artistic creation. This work offers a scalable and effective image editing approach, along with its disentanglement capabilities.",
    "arxiv_url": "http://arxiv.org/abs/2412.09611v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09611v1",
    "published_date": "2024-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "image editing",
      "Control",
      "rectified flow",
      "FLUX"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multimodal Latent Language Modeling with Next-Token Diffusion",
    "authors": [
      "Yutao Sun",
      "Hangbo Bao",
      "Wenhui Wang",
      "Zhiliang Peng",
      "Li Dong",
      "Shaohan Huang",
      "Jianyong Wang",
      "Furu Wei"
    ],
    "abstract": "Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop $\\sigma$-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models.",
    "arxiv_url": "http://arxiv.org/abs/2412.08635v1",
    "pdf_url": "http://arxiv.org/pdf/2412.08635v1",
    "published_date": "2024-12-11",
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
    "authors": [
      "Tianwei Yin",
      "Qiang Zhang",
      "Richard Zhang",
      "William T. Freeman",
      "Fredo Durand",
      "Eli Shechtman",
      "Xun Huang"
    ],
    "abstract": "Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to an autoregressive transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher's ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all previous video generation models. It enables fast streaming generation of high-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner. We will release the code based on an open-source model in the future.",
    "arxiv_url": "http://arxiv.org/abs/2412.07772v2",
    "pdf_url": "http://arxiv.org/pdf/2412.07772v2",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STIV: Scalable Text and Image Conditioned Video Generation",
    "authors": [
      "Zongyu Lin",
      "Wei Liu",
      "Chen Chen",
      "Jiasen Lu",
      "Wenze Hu",
      "Tsu-Jui Fu",
      "Jesse Allardice",
      "Zhengfeng Lai",
      "Liangchen Song",
      "Bowen Zhang",
      "Cha Chen",
      "Yiran Fei",
      "Yifan Jiang",
      "Lezhi Li",
      "Yizhou Sun",
      "Kai-Wei Chang",
      "Yinfei Yang"
    ],
    "abstract": "The field of video generation has made remarkable advancements, yet there remains a pressing need for a clear, systematic recipe that can guide the development of robust and scalable models. In this work, we present a comprehensive study that systematically explores the interplay of model architectures, training recipes, and data curation strategies, culminating in a simple and scalable text-image-conditioned video generation method, named STIV. Our framework integrates image condition into a Diffusion Transformer (DiT) through frame replacement, while incorporating text conditioning via a joint image-text conditional classifier-free guidance. This design enables STIV to perform both text-to-video (T2V) and text-image-to-video (TI2V) tasks simultaneously. Additionally, STIV can be easily extended to various applications, such as video prediction, frame interpolation, multi-view generation, and long video generation, etc. With comprehensive ablation studies on T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simple design. An 8.7B model with 512 resolution achieves 83.1 on VBench T2V, surpassing both leading open and closed-source models like CogVideoX-5B, Pika, Kling, and Gen-3. The same-sized model also achieves a state-of-the-art result of 90.1 on VBench I2V task at 512 resolution. By providing a transparent and extensible recipe for building cutting-edge video generation models, we aim to empower future research and accelerate progress toward more versatile and reliable video generation solutions.",
    "arxiv_url": "http://arxiv.org/abs/2412.07730v1",
    "pdf_url": "http://arxiv.org/pdf/2412.07730v1",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer",
    "authors": [
      "Jinyi Hu",
      "Shengding Hu",
      "Yuxuan Song",
      "Yufei Huang",
      "Mingxuan Wang",
      "Hao Zhou",
      "Zhiyuan Liu",
      "Wei-Ying Ma",
      "Maosong Sun"
    ],
    "abstract": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion Transformer, that innovatively combines autoregressive and diffusion paradigms for modeling continuous visual information. By introducing a block-wise autoregressive unit, ACDiT offers a flexible interpolation between token-wise autoregression and full-sequence diffusion, bypassing the limitations of discrete tokenization. The generation of each block is formulated as a conditional diffusion process, conditioned on prior blocks. ACDiT is easy to implement, as simple as creating a Skip-Causal Attention Mask (SCAM) on standard diffusion transformer during training. During inference, the process iterates between diffusion denoising and autoregressive decoding that can make full use of KV-Cache. We show that ACDiT performs best among all autoregressive baselines under similar model scales on image and video generation tasks. We also demonstrate that benefiting from autoregressive modeling, pretrained ACDiT can be transferred in visual understanding tasks despite being trained with the diffusion objective. The analysis of the trade-off between autoregressive modeling and diffusion demonstrates the potential of ACDiT to be used in long-horizon visual generation tasks. We hope that ACDiT offers a novel perspective on visual autoregressive generation and unlocks new avenues for unified models.",
    "arxiv_url": "http://arxiv.org/abs/2412.07720v2",
    "pdf_url": "http://arxiv.org/pdf/2412.07720v2",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlexDiT: Dynamic Token Density Control for Diffusion Transformer",
    "authors": [
      "Shuning Chang",
      "Pichao Wang",
      "Jiasheng Tang",
      "Yi Yang"
    ],
    "abstract": "Diffusion Transformers (DiT) deliver impressive generative performance but face prohibitive computational demands due to both the quadratic complexity of token-based self-attention and the need for extensive sampling steps. While recent research has focused on accelerating sampling, the structural inefficiencies of DiT remain underexplored. We propose FlexDiT, a framework that dynamically adapts token density across both spatial and temporal dimensions to achieve computational efficiency without compromising generation quality. Spatially, FlexDiT employs a three-segment architecture that allocates token density based on feature requirements at each layer: Poolingformer in the bottom layers for efficient global feature extraction, Sparse-Dense Token Modules (SDTM) in the middle layers to balance global context with local detail, and dense tokens in the top layers to refine high-frequency details. Temporally, FlexDiT dynamically modulates token density across denoising stages, progressively increasing token count as finer details emerge in later timesteps. This synergy between FlexDiT's spatially adaptive architecture and its temporal pruning strategy enables a unified framework that balances efficiency and fidelity throughout the generation process. Our experiments demonstrate FlexDiT's effectiveness, achieving a 55% reduction in FLOPs and a 175% improvement in inference speed on DiT-XL with only a 0.09 increase in FID score on 512$\\times$512 ImageNet images, a 56% reduction in FLOPs across video generation datasets including FaceForensics, SkyTimelapse, UCF101, and Taichi-HD, and a 69% improvement in inference speed on PixArt-$\\alpha$ on text-to-image generation task with a 0.24 FID score decrease. FlexDiT provides a scalable solution for high-quality diffusion-based generation compatible with further sampling optimization techniques.",
    "arxiv_url": "http://arxiv.org/abs/2412.06028v1",
    "pdf_url": "http://arxiv.org/pdf/2412.06028v1",
    "published_date": "2024-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "video generation",
      "Control",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MotionStone: Decoupled Motion Intensity Modulation with Diffusion Transformer for Image-to-Video Generation",
    "authors": [
      "Shuwei Shi",
      "Biao Gong",
      "Xi Chen",
      "Dandan Zheng",
      "Shuai Tan",
      "Zizheng Yang",
      "Yuyuan Li",
      "Jingwen He",
      "Kecheng Zheng",
      "Jingdong Chen",
      "Ming Yang",
      "Yinqiang Zheng"
    ],
    "abstract": "The image-to-video (I2V) generation is conditioned on the static image, which has been enhanced recently by the motion intensity as an additional control signal. These motion-aware models are appealing to generate diverse motion patterns, yet there lacks a reliable motion estimator for training such models on large-scale video set in the wild. Traditional metrics, e.g., SSIM or optical flow, are hard to generalize to arbitrary videos, while, it is very tough for human annotators to label the abstract motion intensity neither. Furthermore, the motion intensity shall reveal both local object motion and global camera movement, which has not been studied before. This paper addresses the challenge with a new motion estimator, capable of measuring the decoupled motion intensities of objects and cameras in video. We leverage the contrastive learning on randomly paired videos and distinguish the video with greater motion intensity. Such a paradigm is friendly for annotation and easy to scale up to achieve stable performance on motion estimation. We then present a new I2V model, named MotionStone, developed with the decoupled motion estimator. Experimental results demonstrate the stability of the proposed motion estimator and the state-of-the-art performance of MotionStone on I2V generation. These advantages warrant the decoupled motion estimator to serve as a general plug-in enhancer for both data processing and video generation training.",
    "arxiv_url": "http://arxiv.org/abs/2412.05848v1",
    "pdf_url": "http://arxiv.org/pdf/2412.05848v1",
    "published_date": "2024-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Self-Guidance: Boosting Flow and Diffusion Generation on Their Own",
    "authors": [
      "Tiancheng Li",
      "Weijian Luo",
      "Zhiyang Chen",
      "Liyuan Ma",
      "Guo-Jun Qi"
    ],
    "abstract": "Proper guidance strategies are essential to achieve high-quality generation results without retraining diffusion and flow-based text-to-image models. Existing guidance either requires specific training or strong inductive biases of diffusion model networks, potentially limiting their applications. Motivated by the observation that artifact outliers can be detected by a significant decline in the density from a noisier to a cleaner noise level, we propose Self-Guidance (SG), which improves the image quality by suppressing the generation of low-quality samples. SG only relies on the sampling probabilities of its own diffusion model at different noise levels with no need of any guidance-specific training. This makes it flexible to be used in a plug-and-play manner with other sampling algorithms, maximizing its potential to achieve competitive performances in many generative tasks. We conduct experiments on text-to-image and text-to-video generation with different architectures, including UNet and transformer models. With open-sourced diffusion models such as Stable Diffusion 3.5 and FLUX, Self-Guidance surpasses existing algorithms on multiple metrics, including both FID and Human Preference Score. Moreover, we find that SG has a surprisingly positive effect on the generation of physiologically correct human body structures such as hands, faces, and arms, showing its ability of eliminating human body artifacts with minimal efforts. We will release our code along with this paper.",
    "arxiv_url": "http://arxiv.org/abs/2412.05827v2",
    "pdf_url": "http://arxiv.org/pdf/2412.05827v2",
    "published_date": "2024-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "FLUX",
      "text-to-image"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Language-Guided Image Tokenization for Generation",
    "authors": [
      "Kaiwen Zha",
      "Lijun Yu",
      "Alireza Fathi",
      "David A. Ross",
      "Cordelia Schmid",
      "Dina Katabi",
      "Xiuye Gu"
    ],
    "abstract": "Image tokenization, the process of transforming raw image pixels into a compact low-dimensional latent representation, has proven crucial for scalable and efficient image generation. However, mainstream image tokenization methods generally have limited compression rates, making high-resolution image generation computationally expensive. To address this challenge, we propose to leverage language for efficient image tokenization, and we call our method Text-Conditioned Image Tokenization (TexTok). TexTok is a simple yet effective tokenization framework that leverages language to provide high-level semantics. By conditioning the tokenization process on descriptive text captions, TexTok allows the tokenization process to focus on encoding fine-grained visual details into latent tokens, leading to enhanced reconstruction quality and higher compression rates. Compared to the conventional tokenizer without text conditioning, TexTok achieves average reconstruction FID improvements of 29.2% and 48.1% on ImageNet-256 and -512 benchmarks respectively, across varying numbers of tokens. These tokenization improvements consistently translate to 16.3% and 34.3% average improvements in generation FID. By simply replacing the tokenizer in Diffusion Transformer (DiT) with TexTok, our system can achieve a 93.5x inference speedup while still outperforming the original DiT using only 32 tokens on ImageNet-512. TexTok with a vanilla DiT generator achieves state-of-the-art FID scores of 1.46 and 1.62 on ImageNet-256 and -512 respectively. Furthermore, we demonstrate TexTok's superiority on the text-to-image generation task, effectively utilizing the off-the-shelf text captions in tokenization.",
    "arxiv_url": "http://arxiv.org/abs/2412.05796v1",
    "pdf_url": "http://arxiv.org/pdf/2412.05796v1",
    "published_date": "2024-12-08",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "image generation",
      "text-to-image",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mind the Time: Temporally-Controlled Multi-Event Video Generation",
    "authors": [
      "Ziyi Wu",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Ivan Skorokhodov",
      "Yuwei Fang",
      "Varnith Chordia",
      "Igor Gilitschenski",
      "Sergey Tulyakov"
    ],
    "abstract": "Real-world videos consist of sequences of events. Generating such sequences with precise temporal control is infeasible with existing video generators that rely on a single paragraph of text as input. When tasked with generating multiple events described using a single prompt, such methods often ignore some of the events or fail to arrange them in the correct order. To address this limitation, we present MinT, a multi-event video generator with temporal control. Our key insight is to bind each event to a specific period in the generated video, which allows the model to focus on one event at a time. To enable time-aware interactions between event captions and video tokens, we design a time-based positional encoding method, dubbed ReRoPE. This encoding helps to guide the cross-attention operation. By fine-tuning a pre-trained video diffusion transformer on temporally grounded data, our approach produces coherent videos with smoothly connected events. For the first time in the literature, our model offers control over the timing of events in generated videos. Extensive experiments demonstrate that MinT outperforms existing commercial and open-source models by a large margin.",
    "arxiv_url": "http://arxiv.org/abs/2412.05263v2",
    "pdf_url": "http://arxiv.org/pdf/2412.05263v2",
    "published_date": "2024-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative Layout-to-Image Generation",
    "authors": [
      "Hui Zhang",
      "Dexiang Hong",
      "Yitong Wang",
      "Jie Shao",
      "Xinglong Wu",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "abstract": "Diffusion models have been recognized for their ability to generate images that are not only visually appealing but also of high artistic quality. As a result, Layout-to-Image (L2I) generation has been proposed to leverage region-specific positions and descriptions to enable more precise and controllable generation. However, previous methods primarily focus on UNet-based models (e.g., SD1.5 and SDXL), and limited effort has explored Multimodal Diffusion Transformers (MM-DiTs), which have demonstrated powerful image generation capabilities. Enabling MM-DiT for layout-to-image generation seems straightforward but is challenging due to the complexity of how layout is introduced, integrated, and balanced among multiple modalities. To this end, we explore various network variants to efficiently incorporate layout guidance into MM-DiT, and ultimately present SiamLayout. To Inherit the advantages of MM-DiT, we use a separate set of network weights to process the layout, treating it as equally important as the image and text modalities. Meanwhile, to alleviate the competition among modalities, we decouple the image-layout interaction into a siamese branch alongside the image-text one and fuse them in the later stage. Moreover, we contribute a large-scale layout dataset, named LayoutSAM, which includes 2.7 million image-text pairs and 10.7 million entities. Each entity is annotated with a bounding box and a detailed description. We further construct the LayoutSAM-Eval benchmark as a comprehensive tool for evaluating the L2I generation quality. Finally, we introduce the Layout Designer, which taps into the potential of large language models in layout planning, transforming them into experts in layout generation and optimization. Our code, model, and dataset will be available at https://creatilayout.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2412.03859v2",
    "pdf_url": "http://arxiv.org/pdf/2412.03859v2",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Controllable",
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Navigation World Models",
    "authors": [
      "Amir Bar",
      "Gaoyue Zhou",
      "Danny Tran",
      "Trevor Darrell",
      "Yann LeCun"
    ],
    "abstract": "Navigation is a fundamental skill of agents with visual-motor capabilities. We introduce a Navigation World Model (NWM), a controllable video generation model that predicts future visual observations based on past observations and navigation actions. To capture complex environment dynamics, NWM employs a Conditional Diffusion Transformer (CDiT), trained on a diverse collection of egocentric videos of both human and robotic agents, and scaled up to 1 billion parameters. In familiar environments, NWM can plan navigation trajectories by simulating them and evaluating whether they achieve the desired goal. Unlike supervised navigation policies with fixed behavior, NWM can dynamically incorporate constraints during planning. Experiments demonstrate its effectiveness in planning trajectories from scratch or by ranking trajectories sampled from an external policy. Furthermore, NWM leverages its learned visual priors to imagine trajectories in unfamiliar environments from a single input image, making it a flexible and powerful tool for next-generation navigation systems.",
    "arxiv_url": "http://arxiv.org/abs/2412.03572v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03572v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "Controllable",
      "Control",
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seeing Beyond Views: Multi-View Driving Scene Video Generation with Holistic Attention",
    "authors": [
      "Hannan Lu",
      "Xiaohe Wu",
      "Shudong Wang",
      "Xiameng Qin",
      "Xinyu Zhang",
      "Junyu Han",
      "Wangmeng Zuo",
      "Ji Tao"
    ],
    "abstract": "Generating multi-view videos for autonomous driving training has recently gained much attention, with the challenge of addressing both cross-view and cross-frame consistency. Existing methods typically apply decoupled attention mechanisms for spatial, temporal, and view dimensions. However, these approaches often struggle to maintain consistency across dimensions, particularly when handling fast-moving objects that appear at different times and viewpoints. In this paper, we present CogDriving, a novel network designed for synthesizing high-quality multi-view driving videos. CogDriving leverages a Diffusion Transformer architecture with holistic-4D attention modules, enabling simultaneous associations across the spatial, temporal, and viewpoint dimensions. We also propose a lightweight controller tailored for CogDriving, i.e., Micro-Controller, which uses only 1.1% of the parameters of the standard ControlNet, enabling precise control over Bird's-Eye-View layouts. To enhance the generation of object instances crucial for autonomous driving, we propose a re-weighted learning objective, dynamically adjusting the learning weights for object instances during training. CogDriving demonstrates strong performance on the nuScenes validation set, achieving an FVD score of 37.8, highlighting its ability to generate realistic driving videos. The project can be found at https://luhannan.github.io/CogDrivingPage/.",
    "arxiv_url": "http://arxiv.org/abs/2412.03520v2",
    "pdf_url": "http://arxiv.org/pdf/2412.03520v2",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "video generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MaterialPicker: Multi-Modal Material Generation with Diffusion Transformers",
    "authors": [
      "Xiaohe Ma",
      "Valentin Deschaintre",
      "Milo Haan",
      "Fujun Luan",
      "Kun Zhou",
      "Hongzhi Wu",
      "Yiwei Hu"
    ],
    "abstract": "High-quality material generation is key for virtual environment authoring and inverse rendering. We propose MaterialPicker, a multi-modal material generator leveraging a Diffusion Transformer (DiT) architecture, improving and simplifying the creation of high-quality materials from text prompts and/or photographs. Our method can generate a material based on an image crop of a material sample, even if the captured surface is distorted, viewed at an angle or partially occluded, as is often the case in photographs of natural scenes. We further allow the user to specify a text prompt to provide additional guidance for the generation. We finetune a pre-trained DiT-based video generator into a material generator, where each material map is treated as a frame in a video sequence. We evaluate our approach both quantitatively and qualitatively and show that it enables more diverse material generation and better distortion correction than previous work.",
    "arxiv_url": "http://arxiv.org/abs/2412.03225v2",
    "pdf_url": "http://arxiv.org/pdf/2412.03225v2",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Panoptic Diffusion Models: co-generation of images and segmentation maps",
    "authors": [
      "Yinghan Long",
      "Kaushik Roy"
    ],
    "abstract": "Recently, diffusion models have demonstrated impressive capabilities in text-guided and image-conditioned image generation. However, existing diffusion models cannot simultaneously generate an image and a panoptic segmentation of objects and stuff from the prompt. Incorporating an inherent understanding of shapes and scene layouts can improve the creativity and realism of diffusion models. To address this limitation, we present Panoptic Diffusion Model (PDM), the first model designed to generate both images and panoptic segmentation maps concurrently. PDM bridges the gap between image and text by constructing segmentation layouts that provide detailed, built-in guidance throughout the generation process. This ensures the inclusion of categories mentioned in text prompts and enriches the diversity of segments within the background. We demonstrate the effectiveness of PDM across two architectures: a unified diffusion transformer and a two-stream transformer with a pretrained backbone. We propose a Multi-Scale Patching mechanism to generate high-resolution segmentation maps. Additionally, when ground-truth maps are available, PDM can function as a text-guided image-to-image generation model. Finally, we propose a novel metric for evaluating the quality of generated maps and show that PDM achieves state-of-the-art results in image generation with implicit scene control.",
    "arxiv_url": "http://arxiv.org/abs/2412.02929v2",
    "pdf_url": "http://arxiv.org/pdf/2412.02929v2",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SyncFlow: Toward Temporally Aligned Joint Audio-Video Generation from Text",
    "authors": [
      "Haohe Liu",
      "Gael Le Lan",
      "Xinhao Mei",
      "Zhaoheng Ni",
      "Anurag Kumar",
      "Varun Nagaraja",
      "Wenwu Wang",
      "Mark D. Plumbley",
      "Yangyang Shi",
      "Vikas Chandra"
    ],
    "abstract": "Video and audio are closely correlated modalities that humans naturally perceive together. While recent advancements have enabled the generation of audio or video from text, producing both modalities simultaneously still typically relies on either a cascaded process or multi-modal contrastive encoders. These approaches, however, often lead to suboptimal results due to inherent information losses during inference and conditioning. In this paper, we introduce SyncFlow, a system that is capable of simultaneously generating temporally synchronized audio and video from text. The core of SyncFlow is the proposed dual-diffusion-transformer (d-DiT) architecture, which enables joint video and audio modelling with proper information fusion. To efficiently manage the computational cost of joint audio and video modelling, SyncFlow utilizes a multi-stage training strategy that separates video and audio learning before joint fine-tuning. Our empirical evaluations demonstrate that SyncFlow produces audio and video outputs that are more correlated than baseline methods with significantly enhanced audio quality and audio-visual correspondence. Moreover, we demonstrate strong zero-shot capabilities of SyncFlow, including zero-shot video-to-audio generation and adaptation to novel video resolutions without further training.",
    "arxiv_url": "http://arxiv.org/abs/2412.15220v1",
    "pdf_url": "http://arxiv.org/pdf/2412.15220v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "video generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis",
    "authors": [
      "Yu Yuan",
      "Xijun Wang",
      "Yichen Sheng",
      "Prateek Chennuri",
      "Xingguang Zhang",
      "Stanley Chan"
    ],
    "abstract": "Image generation today can produce somewhat realistic images from text prompts. However, if one asks the generator to synthesize a particular camera setting such as creating different fields of view using a 24mm lens versus a 70mm lens, the generator will not be able to interpret and generate scene-consistent images. This limitation not only hinders the adoption of generative tools in photography applications but also exemplifies a broader issue of bridging the gap between the data-driven models and the physical world. In this paper, we introduce the concept of Generative Photography, a framework designed to control camera intrinsic settings during content generation. The core innovation of this work are the concepts of Dimensionality Lifting and Contrastive Camera Learning, which achieve continuous and consistent transitions for different camera settings. Experimental results show that our method produces significantly more scene-consistent photorealistic images than state-of-the-art models such as Stable Diffusion 3 and FLUX.",
    "arxiv_url": "http://arxiv.org/abs/2412.02168v2",
    "pdf_url": "http://arxiv.org/pdf/2412.02168v2",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "FLUX",
      "text-to-image",
      "image generation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "World-consistent Video Diffusion with Explicit 3D Modeling",
    "authors": [
      "Qihang Zhang",
      "Shuangfei Zhai",
      "Miguel Angel Bautista",
      "Kevin Miao",
      "Alexander Toshev",
      "Joshua Susskind",
      "Jiatao Gu"
    ],
    "abstract": "Recent advancements in diffusion models have set new benchmarks in image and video generation, enabling realistic visual synthesis across single- and multi-frame contexts. However, these models still struggle with efficiently and explicitly generating 3D-consistent content. To address this, we propose World-consistent Video Diffusion (WVD), a novel framework that incorporates explicit 3D supervision using XYZ images, which encode global 3D coordinates for each image pixel. More specifically, we train a diffusion transformer to learn the joint distribution of RGB and XYZ frames. This approach supports multi-task adaptability via a flexible inpainting strategy. For example, WVD can estimate XYZ frames from ground-truth RGB or generate novel RGB frames using XYZ projections along a specified camera trajectory. In doing so, WVD unifies tasks like single-image-to-3D generation, multi-view stereo, and camera-controlled video generation. Our approach demonstrates competitive performance across multiple benchmarks, providing a scalable solution for 3D-consistent video and image generation with a single pretrained model.",
    "arxiv_url": "http://arxiv.org/abs/2412.01821v1",
    "pdf_url": "http://arxiv.org/pdf/2412.01821v1",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "Control",
      "video generation",
      "image generation",
      "diffusion transformer"
    ],
    "citations": 0,
    "semantic_url": ""
  }
]